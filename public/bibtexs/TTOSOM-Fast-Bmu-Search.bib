% This file was created with JabRef 2.7b.
% Encoding: UTF-8

@INPROCEEDINGS{Martin2004,
  author = {Mart\'{\i}n-Herrero,,J. and Ferreiro-Arm{\'a}n,,M. and Alba-Castro
	, J. L.},
  title = {Grading Textured Surfaces with Automated Soft Clustering in a Supervised
	SOM},
  booktitle = {ICIAR (2)},
  year = {2004},
  pages = {323-330},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  crossref = {DBLP:conf/iciar/2004-2},
  ee = {http://springerlink.metapress.com/openurl.asp?genre=article{\&}issn=0302-9743{\&}volume=3212{\&}spage=323},
  file = {Martin2004.pdf:fast-BMU/Martin2004.pdf:PDF}
}

@INPROCEEDINGS{McClurkin2001,
  author = {McClurkin,,D. J. and Georgakopoulos,,G. F.},
  title = {Sphendamnoe: A Proof that k-Splay Fails to Achieve log$_{\mbox{k}}$N
	Behaviour},
  booktitle = {Panhellenic Conference on Informatics},
  year = {2001},
  pages = {480-496},
  abstract = {The splay technique was introduced to cope with biasedness or changeability
	in data access frequencies, achieving optimality within a small constant
	factor w.r.t. static trees, without sacrificing log N worst-case
	behaviour, in an amortized sense. It achieves this through a series
	of local transformations, starting from the search node and propagating
	upwards along the search path to the root. It seems plausible that
	a similar policy, suitably adapted to multi-way trees, could achieve
	logk N amortized performance (nodes visited), where k is the degree
	of the tree. Sherk’s k-splay, a generalization of Sleator and Tarjan’s
	splay technique to multi-way trees, proved to exhibit amortized log2
	N behaviour, could be considered the most likely candidate for a
	multi-way splay-like self-adjusting policy with logk N amortized
	complexity. We construct a family of k-ary trees having depth k and
	containing 2k nodes and we show that Sherk’s k-splay applied to the
	deepest node of any tree in this family always produces another tree
	in the family. A fractal-like process of replacing leaf-nodes of
	trees with copies of themselves then allows us to create such trees
	of arbitrary size for any given k. This provides us with a family
	of counterexamples for which Sherk’s k-splay always visits log2 N
	nodes when applied to the deepest node in the tree.},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  crossref = {DBLP:conf/pci/2001},
  ee = {http://springerlink.metapress.com/openurl.asp?genre=article{\&}issn=0302-9743{\&}volume=2563{\&}spage=480},
  file = {paper:Trees\\McClurkin2001.pdf:PDF}
}

@INPROCEEDINGS{Torsello2008,
  author = {Torsello,,Andrea and Dowe,,David L.},
  title = {Learning a Generative Model for Structural Representations},
  booktitle = {Australasian Conference on Artificial Intelligence},
  year = {2008},
  pages = {573-583},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  crossref = {DBLP:conf/ausai/2008},
  ee = {http://dx.doi.org/10.1007/978-3-540-89378-3_58}
}

@INPROCEEDINGS{Venna2001,
  author = {Venna,,J. and Kaski,,S.},
  title = {Neighborhood Preservation in Nonlinear Projection Methods: An Experimental
	Study},
  booktitle = {ICANN},
  year = {2001},
  pages = {485-491},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  crossref = {DBLP:conf/icann/2001},
  ee = {http://link.springer.de/link/service/series/0558/bibs/2130/21300485.htm},
  file = {paper:References/SOM/som-related/Venna2001.pdf:PDF},
  owner = {castudil},
  timestamp = {2009.08.27},
  url = {http://www.springerlink.com/content/rxtf5rpfxadqhypl/}
}

@ARTICLE{Adelson1962,
  author = {Adelson-Velskii,,M. and Landis,M.,E.},
  title = {An algorithm for the organization of information},
  journal = {Sov. Math. DokL},
  year = {1962},
  volume = {3},
  pages = {1259-1262},
  owner = {castudil},
  timestamp = {2009.03.17}
}

@ARTICLE{Aha1991,
  author = {Aha,, D. W. and Kibler,, D. and Albert,, M. K.},
  title = {Instance-Based Learning Algorithms},
  journal = {Machine Learning},
  year = {1991},
  volume = {6},
  pages = {37-66},
  abstract = {Storing and using specific instances improves the performance of several
	supervised learning algorithms. These include algorithms that learn
	decision trees, classification rules, and distributed networks. However,
	no investigation has analyzed algorithms that use only specific instances
	to solve incremental learning tasks. In this paper, we describe a
	framework and methodology, called instance-based learning, that generates
	classification predictions using only specific instances. Instance-based
	learning algorithms do not maintain a set of abstractions derived
	from specific instances. This approach extends the nearest neighbor
	algorithm, which has large storage requirements. We describe how
	storage requirements can be significantly reduced with, at most,
	minor sacrifices in learning rate and classification accuracy. While
	the storage-reducing algorithm performs well on several real-world
	databases, its performance degrades rapidly with the level of attribute
	noise in training instances. Therefore, we extended it with a significance
	test to distinguish noisy instances. This extended algorithm's performance
	degrades gracefully with increasing noise levels and compares favorably
	with a noise-tolerant decision tree algorithm.},
  doi = {10.1023/A:1022689900470},
  file = {Aha1991.pdf:Pattern Recognition/Aha1991.pdf:PDF},
  issn = {0885-6125},
  issue = {1},
  keyword = {Computer Science},
  publisher = {Springer Netherlands}
}

@ARTICLE{Ahmadi2004,
  author = {Ahmadi,,A. and Omatu,,S. and Fujinaka,,T. and Kosaka,,T.},
  title = {Improvement of reliability in banknote classification using reject
	option and local PCA},
  journal = {Information Sciences},
  year = {2004},
  volume = {168},
  pages = {277 - 293},
  number = {1-4},
  abstract = {The improvement of reliability in banknote neuro-classifier is investigated
	and a reject option is proposed based on the probability density
	function of the input data. The classification reliability is evaluated
	through two reliability parameters, which are associated with the
	winning class probability and the second maximal probability. Then
	a threshold value is set up to reject the unreliable classifications.
	As for modeling the non-linear correlation among the data variables
	and extracting the features, a local principal components analysis
	(PCA) is applied. The method is tested with a learning vector quantization
	(LVQ) classifier using 1440 data samples of various US dollar bills.
	The results show that by taking a suitable reject threshold value
	and also a proper number of regions for the local PCA, the reliability
	of system can be improved significantly.},
  doi = {10.1016/j.ins.2004.02.018},
  file = {Ahmadi2004.pdf:SOM\\applications\\Ahmadi2004.pdf:PDF},
  issn = {0020-0255},
  keywords = {Banknote recognition},
  url = {http://www.sciencedirect.com/science/article/B6V0C-4BVC8XW-5/2/005bcd1b6c3910842a47fd68d3bb85c8}
}

@INPROCEEDINGS{vonAhn2004,
  author = {von Ahn,, L. and Dabbish,, L.},
  title = {Labeling images with a computer game},
  booktitle = {CHI '04: Proceedings of the SIGCHI conference on Human factors in
	computing systems},
  year = {2004},
  pages = {319--326},
  address = {New York, NY, USA},
  publisher = {ACM},
  doi = {http://doi.acm.org/10.1145/985692.985733},
  isbn = {1-58113-702-8},
  location = {Vienna, Austria}
}

@BOOK{Aho1974,
  title = {The Design and Analysis of Computer Algorithms},
  publisher = {Addison-Wesley Longman Publishing Co., Inc.},
  year = {1974},
  author = {Aho, A. V. and Hopcroft, J. E.},
  address = {Boston, MA, USA},
  edition = {1st},
  isbn = {0201000296}
}

@ARTICLE{Aiolli2009,
  author = {Aiolli,, F. and Da San Martino,, G. and Hagenbuchner,, M. and Sperduti,,
	A.},
  title = {Learning Nonsparse Kernels by Self-Organizing Maps for Structured
	Data},
  journal = {Neural Networks, IEEE Transactions on},
  year = {2009},
  volume = {20},
  pages = {1938-1949},
  number = {12},
  month = {Dec. },
  abstract = {The development of neural network (NN) models able to encode structured
	input, and the more recent definition of kernels for structures,
	makes it possible to directly apply machine learning approaches to
	generic structured data. However, the effectiveness of a kernel can
	depend on its sparsity with respect to a specific data set. In fact,
	the accuracy of a kernel method typically reduces as the kernel sparsity
	increases. The sparsity problem is particularly common in structured
	domains involving discrete variables which may take on many different
	values. In this paper, we explore this issue on two well-known kernels
	for trees, and propose to face it by recurring to self-organizing
	maps (SOMs) for structures. Specifically, we show that a suitable
	combination of the two approaches, obtained by defining a new class
	of kernels based on the activation map of a SOM for structures, can
	be effective in avoiding the sparsity problem and results in a system
	that can be significantly more accurate for categorization tasks
	on structured data. The effectiveness of the proposed approach is
	demonstrated experimentally on two relatively large corpora of XML
	formatted data and a data set of user sessions extracted from Website
	logs.},
  doi = {10.1109/TNN.2009.2033473},
  file = {paper:C\:\\Users\\castudil\\Documents\\Research\\Cesar\\References\\to-read\\Aiolli2009.pdf:PDF},
  issn = {1045-9227},
  keywords = {Web sites, XML, learning (artificial intelligence), self-organising
	feature mapsWeb site logs, XML formatted data, categorization tasks,
	generic structured data, learning nonsparse kernels, machine learning,
	neural network models, self-organizing maps},
  owner = {READ},
  timestamp = {2010.01.18}
}

@ARTICLE{Alahakoon2000,
  author = {Alahakoon,,D. and Halgamuge,,S. K. and Srinivasan,,B.},
  title = {Dynamic self-organizing maps with controlled growth for knowledge
	discovery},
  journal = {IEEE Transactions on Neural Networks},
  year = {2000},
  volume = {11},
  pages = {601-614},
  number = {3},
  abstract = {The growing self-organizing map (GSOM) algorithm is presented in detail
	and the effect of a spread factor, which can be used to measure and
	control the spread of the GSOM, is investigated. The spread factor
	is independent of the dimensionality of the data and as such can
	be used as a controlling measure for generating maps with different
	dimensionality, which can then be compared and analyzed with better
	accuracy. The spread factor is also presented as a method of achieving
	hierarchical clustering of a data set with the GSOM. Such hierarchical
	clustering allows the data analyst to identify significant and interesting
	clusters at a higher level of the hierarchy, and continue with finer
	clustering of the interesting clusters only. Therefore, only a small
	map is created in the beginning with a low spread factor, which can
	be generated for even a very large data set. Further analysis is
	conducted on selected sections of the data and of smaller volume.
	Therefore, this method facilitates the analysis of even very large
	data sets},
  doi = {10.1109/72.846732},
  file = {paper:References\\SOM\\SOM-variants\\non-tree-based\\Alahakoon2000.PDF:PDF},
  issn = {1045-9227},
  keywords = {data mining, self-organising feature maps, unsupervised learningdata
	mining, growing self-organizing map, hierarchical clustering, knowledge
	discovery, neural networks, spread factor, unsupervised learning}
}

@ARTICLE{Allen1978,
  author = {Allen,, B. and Munro,, I.},
  title = {Self-Organizing Binary Search Trees},
  journal = {J. ACM},
  year = {1978},
  volume = {25},
  pages = {526--535},
  number = {4},
  address = {New York, NY, USA},
  doi = {http://doi.acm.org/10.1145/322092.322094},
  issn = {0004-5411},
  publisher = {ACM}
}

@TECHREPORT{Anderson1979,
  author = {Anderson,, A. C. and Fu,, K. S.},
  title = {Design and Development of a Linear Binary Tree Classifier for Leukocytes},
  institution = {Purdue University},
  year = {1979},
  address = {West Lafayette, IN},
  owner = {castudillo},
  timestamp = {2010.12.01}
}

@INPROCEEDINGS{Arnow1982,
  author = {Arnow,,D. M. and Tenenbaum,,A. M.},
  title = {An investigation of the move-ahead-k rules},
  booktitle = {Congressus Numerantium, Proc. Thirteenth Southeastern Conf. Combinatorics,
	Graph Theory and Computing},
  year = {1982},
  pages = {47-65},
  month = {February},
  owner = {castudil},
  timestamp = {2009.02.17}
}

@ARTICLE{Arsuaga2005,
  author = {Arsuaga Uriarte,,E. and D\'{i}az Mart\'{i}n,,F.},
  title = {Topology Preservation in {SOM}},
  journal = {International Journal of Applied Mathematics and Computer Sciences},
  year = {2005},
  volume = {1},
  pages = {19-22},
  number = {1},
  abstract = {The SOM has several beneficial features which make it a useful method
	for data mining. One of the most important features is the ability
	to preserve the topology in the projection. There are several measures
	that can be used to quantify the goodness of the map in order to
	obtain the optimal projection, including the average quantization
	error and many topological errors. Many researches have studied how
	the topology preservation should be measured. One option consists
	of using the topographic error which considers the ratio of data
	vectors for which the first and second best BMUs are not adjacent.
	In this work we present a study of the behaviour of the topographic
	error in different kinds of maps. We have found that this error devaluates
	the rectangular maps and we have studied the reasons why this happens.
	Finally, we suggest a new topological error to improve the deficiency
	of the topographic error.},
  file = {PAPER:References/SOM/som-related/Aursuaga2005.pdf:PDF},
  keywords = {Map lattice, Self-Organizing Map, topographic
	
	error, topology preservation.},
  owner = {castudil},
  timestamp = {2009.08.19},
  url = {http://www.waset.org/journals/ijamcs/v1/v1-1-4.pdf}
}

@INPROCEEDINGS{Arthur2006,
  author = {Arthur,,D. and Vassilvitskii,,S.},
  title = {How slow is the k-means method?},
  booktitle = {SCG '06: Proceedings of the twenty-second annual symposium on Computational
	geometry},
  year = {2006},
  pages = {144--153},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  doi = {http://doi.acm.org/10.1145/1137856.1137880},
  isbn = {1-59593-340-9},
  location = {Sedona, Arizona, USA}
}

@INPROCEEDINGS{Arya1993,
  author = {Arya,,S. and Mount,,D. M.},
  title = {Algorithms for Fast Vector Quantization},
  booktitle = {Proc. of the Data Compression Conference (DCC'93)},
  year = {1993},
  editor = {J. A. Storer and M. Cohn},
  pages = {381--390},
  publisher = {IEEE Press},
  file = {Arya1993.pdf:fast-BMU\\Arya1993.pdf:PDF},
  owner = {andrea montoya},
  timestamp = {2010.12.13}
}

@PHDTHESIS{AstudilloThesis,
  author = {Astudillo,,C. A.},
  title = {Self Organizing Maps Constrained by Data Structures},
  school = {School of Computer Science},
  year = {2011},
  owner = {castudil},
  timestamp = {2010.05.06}
}

@INPROCEEDINGS{Astudillo2009a,
  author = {Astudillo,,C. A. and Oommen,,B. J.},
  title = {On Using Adaptive Binary Search Trees to Enhance Self Organizing
	Maps},
  booktitle = {22nd Australasian Joint Conference on Artificial Intelligence (AI
	2009)},
  year = {2009},
  editor = {A. Nicholson and X. Li},
  pages = {199-209},
  abstract = {We present a strategy by which a Self-OrganizingMap (SOM) with an
	underlying Binary Search Tree (BST) structure can be adaptively re-structured
	using conditional rotations. These rotations on the nodes of the
	tree are local and are performed in constant time, guaranteeing a
	decrease in the Weighted Path Length (WPL) of the entire tree. As
	a result, the algorithm, referred to as the Tree-based Topology-Oriented
	SOM with Conditional Rotations (TTO-CONROT), converges in such a
	manner that the neurons are ultimately placed in the input space
	so as to represent its stochastic distribution, and additionally,
	the neighborhood properties of the neurons suit the best BST that
	represents the data.},
  file = {Astudillo2009a.pdf:personal/Astudillo2009a.pdf:PDF},
  owner = {castudil},
  timestamp = {2009.11.03}
}

@INCOLLECTION{Astudillo2011a,
  author = {Astudillo, C. A. and Oommen, B. J.},
  title = {Semi-Supervised Classification Using Tree-Based Self-Organizing Maps},
  booktitle = {AI 2011: Advances in Artificial Intelligence},
  publisher = {Springer Berlin Heidelberg},
  year = {2011},
  editor = {Wang, Dianhui and Reynolds, Mark},
  volume = {7106},
  series = {Lecture Notes in Computer Science},
  pages = {21-30},
  doi = {10.1007/978-3-642-25832-9_3},
  isbn = {978-3-642-25831-2},
  keywords = {Hierarchical SOM; Topology-Based Self-Organization; Pattern Recognition;
	Semi-Supervised Learning},
  url = {http://dx.doi.org/10.1007/978-3-642-25832-9_3}
}

@ARTICLE{Astudillo2014,
  author = {C. A. Astudillo and B. J. Oommen},
  title = {Self-organizing maps whose topologies can be learned with adaptive
	binary search trees using conditional rotations },
  journal = {Pattern Recognition},
  year = {2014},
  volume = {47},
  pages = {96--113},
  number = {1},
  doi = {http://dx.doi.org/10.1016/j.patcog.2013.04.012},
  issn = {0031-3203},
  keywords = {<!-- Tag Not Handled --><keyword id=#key0010#>Adaptive data structures},
  owner = {cesar},
  timestamp = {2014.01.14},
  url = {http://www.sciencedirect.com/science/article/pii/S0031320313001829}
}

@ARTICLE{AstudilloSurvey,
  author = {Astudillo,,C. A. and Oommen,,B. J.},
  title = {Topology-Oriented Self-Organizing Maps: A Survey},
  journal = {Pattern Analysis and Applications},
  year = {2014},
  note = {\url{http://dx.doi.org/10.1007/s10044-014-0367-9}},
  doi = {10.1007/s10044-014-0367-9},
  owner = {castudil},
  timestamp = {2010.09.23},
  url = {doi.org/10.1007/s10044-014-0367-9}
}

@ARTICLE{Astudillo2013,
  author = {C. A. Astudillo and B. J. Oommen},
  title = {On achieving semi-supervised pattern recognition by utilizing tree-based
	SOMs},
  journal = {Pattern Recognition},
  year = {2013},
  volume = {46},
  pages = {293 - 304},
  number = {1},
  doi = {10.1016/j.patcog.2012.07.006},
  issn = {0031-3203},
  keywords = {SOM},
  owner = {cesar},
  timestamp = {2014.01.14},
  url = {http://www.sciencedirect.com/science/article/pii/S0031320312003226}
}

@ARTICLE{Astudillo2011,
  author = {Astudillo,,C. A. and Oommen,,B. J.},
  title = {Imposing tree-based topologies onto self organizing maps},
  journal = {Information Sciences},
  year = {2011},
  volume = {181},
  pages = {3798 - 3815},
  number = {18},
  abstract = {The beauty of the Kohonen map is that it has the property of organizing
	the codebook vectors, which represent the data points, both with
	respect to the underlying distribution and topologically. This topology
	is traditionally linear, even though the underlying lattice could
	be a grid, and this has been used in a variety of applications [23],
	[35] and [40]. The most prominent efforts to render the topology
	to be structured involves the Evolving Tree (ET) due to Pakkanen
	et al. [36], and the Self-Organizing Tree Maps (SOTM) due to Guan
	et al. [18], among others. In this paper we propose a strategy, the
	Tree-Based Topology-Oriented SOM (TTOSOM) by which we can impose
	an arbitrary, user-defined, tree-like topology onto the codebooks.
	Such an imposition enforces a neighborhood phenomenon which is based
	on the user-defined tree, and consequently renders the so-called
	bubble of activity to be drastically different from the ones defined
	in the prior literature. The map learned as a consequence of training
	with the TTOSOM is able to infer both the distribution of the data
	and its structured topology interpreted via the perspective of the
	user-defined tree. The TTOSOM also reveals multi-resolution capabilities,
	which are helpful for representing the original data set with different
	numbers of points, and this can be obtained without the necessity
	of recomputing the whole tree. The ability to extract an skeleton,
	which is a #stick-like# representation of the image in a lower dimensional
	space, is discussed as well. These properties has been confirmed
	by our experimental results on a variety of data sets.},
  doi = {DOI: 10.1016/j.ins.2011.04.038},
  issn = {0020-0255},
  keywords = {Hierarchical SOM}
}

@INPROCEEDINGS{Astudillo2009,
  author = {Astudillo,,C. A. and Oommen,,B. J.},
  title = {A Novel Self Organizing Map Which Utilizes Imposed Tree-based Topologies},
  booktitle = {6th International Conference on Computer Recognition Systems},
  year = {2009},
  volume = {57},
  pages = {169-178},
  abstract = {In this paper we propose a strategy, the Tree-based Topology-Oriented
	
	SOM (TTO-SOM) by which we can impose an arbitrary, user-defined, tree-like
	topology
	
	onto the codebooks. Such an imposition enforces a neighborhood phenomenon
	
	which is based on the user-defined tree, and consequently renders
	the so-called bubble
	
	of activity to be drastically different from the ones defined in the
	prior literature.
	
	The map learnt as a consequence of training with the TTO-SOM is able
	to infer
	
	both the distribution of the data and its structured topology interpreted
	via the
	
	perspective of the user-defined tree. The TTO-SOM also reveals multi-resolution
	
	capabilities, which are helpful for representing the original data
	set with different
	
	numbers of points, whithout the necessity of recomputing the whole
	tree. The ability
	
	to extract an skeleton, which is a “stick-like” representation of
	the image in a lower
	
	dimensional space, is discussed as well. These properties has been
	confirmed by our
	
	experimental results on a variety of data sets.},
  doi = {10.1007/978-3-540-93905-4_21},
  file = {paper:References/SOM/SOM-variants/Astudillo09.pdf:PDF},
  owner = {castudil},
  timestamp = {2009.03.18},
  url = {http://www.springerlink.com/content/v6171486433308pw/}
}

@INPROCEEDINGS{Athitsos2005,
  author = {Athitsos,, V. and Sclaroff,, S.},
  title = {Boosting Nearest Neighbor Classifiers for Multiclass Recognition},
  booktitle = {CVPR '05: Proceedings of the 2005 IEEE Computer Society Conference
	on Computer Vision and Pattern Recognition (CVPR'05) - Workshops},
  year = {2005},
  pages = {45},
  address = {Washington, DC, USA},
  publisher = {IEEE Computer Society},
  doi = {http://dx.doi.org/10.1109/CVPR.2005.424},
  file = {Athitsos2005.pdf:UCI/Athitsos2005.pdf:PDF},
  isbn = {0-7695-2372-2-3},
  review = {results for the yeast dataset}
}

@BOOK{Baeza-Yates1999,
  title = {Modern Information Retrieval},
  publisher = {Addison-Wesley Longman Publishing Co., Inc.},
  year = {1999},
  author = {Baeza-Yates, R. A. and Ribeiro-Neto, B.},
  address = {Boston, MA, USA},
  isbn = {020139829X}
}

@BOOK{Barnett1994,
  title = {Outliers in statistical data},
  publisher = {Wiley},
  year = {1994},
  author = {Barnett,,V. and Lewis,,T.},
  edition = {Third},
  owner = {castudil},
  timestamp = {2009.09.09}
}

@ARTICLE{Bartolucci1976,
  author = {Bartolucci,, L.A. and Swain,, P.H. and Chialin Wu},
  title = {Selective Radiant Temperature Mapping Using a Layered Classifier},
  journal = {Geoscience Electronics, IEEE Transactions on},
  year = {1976},
  volume = {14},
  pages = {101 -106},
  number = {2},
  month = april,
  abstract = {A method of measuring temperatures of selected ground cover types
	using remotely sensed multispectral scanner data and a layered classification
	approach is described. A brief review of radiation theory is presented
	to show that for the wavelength bands and temperature ranges involved
	in remote sensing applications, a linear calibration function can
	be satisfactorily utilized. Finally, an example of the application
	of the layered classifier for temperature mapping of water is shown.},
  doi = {10.1109/TGE.1976.294417},
  file = {Bartolucci1976.pdf:RHST/HyperplaneTree-classifiers/Bartolucci1976.pdf:PDF},
  issn = {0018-9413}
}

@ARTICLE{Bauer1999,
  author = {Bauer,,H. U. and Herrmann,,M. and Villmann,,T.},
  title = {Neural maps and topographic vector quantization},
  journal = {Neural Networks},
  year = {1999},
  volume = {12},
  pages = {659 - 676},
  number = {4-5},
  doi = {DOI: 10.1016/S0893-6080(99)00027-1},
  file = {paper:References/SOM/som-related/Bauer1999.pdf:PDF},
  issn = {0893-6080},
  keywords = {Self-organization},
  url = {http://www.sciencedirect.com/science/article/B6T08-40M518N-9/2/fba18e7f0395e19ed639ee56472ce4ad}
}

@ARTICLE{Bauer1992,
  author = {Bauer,,H. U. and Pawelzik,, K. R.},
  title = {Quantifying the neighborhood preservation of self-organizing feature
	maps},
  journal = {Neural Networks},
  year = {1992},
  volume = {3},
  pages = {570-579},
  number = {4},
  month = {July},
  abstract = {It is shown that a topographic product P, first introduced in nonlinear
	dynamics, is an appropriate measure of the preservation or violation
	of neighborhood relations. It is sensitive to large-scale violations
	of the neighborhood ordering, but does not account for neighborhood
	ordering distortions caused by varying areal magnification factors.
	A vanishing value of the topographic product indicates a perfect
	neighborhood preservation; negative (positive) values indicate a
	too small (too large) output space dimensionality. In a simple example
	of maps from a 2D input space onto 1D, 2D, and 3D output spaces,
	it is demonstrated how the topographic product picks the correct
	output space dimensionality. In a second example, 19D speech data
	are mapped onto various output spaces and it is found that a 3D output
	space (instead of 2D) seems to be optimally suited to the data. This
	is an agreement with a recent speech recognition experiment on the
	same data set},
  doi = {10.1109/72.143371},
  file = {paper:References/SOM/som-related/Bauer1992.pdf:PDF},
  issn = {1045-9227},
  keywords = {neural nets, self-adjusting systems, speech recognitiondimensionality,
	input space, neighborhood ordering distortions, neighborhood preservation,
	nonlinear dynamics, output space, self-organizing feature maps, speech
	recognition, topographic product}
}

@ARTICLE{Bauer1997,
  author = {Bauer, H.-U. and Villmann, T.},
  title = {Growing a hypercubical output space in a self-organizing feature
	map},
  journal = {Neural Networks, IEEE Transactions on},
  year = {1997},
  volume = {8},
  pages = {218-226},
  number = {2},
  month = {Mar},
  abstract = {Neural maps project data from an input space onto a neuron position
	in a (often lower dimensional) output space grid in a neighborhood
	preserving way, with neighboring neurons in the output space responding
	to neighboring data points in the input space. A map-learning algorithm
	can achieve an optimal neighborhood preservation only, if the output
	space topology roughly matches the effective structure of the data
	in the input space. We here present a growth algorithm, called the
	GSOM or growing self-organizing map, which enhances a widespread
	map self-organization process, Kohonen's self-organizing feature
	map (SOFM), by an adaptation of the output space grid during learning.
	The GSOM restricts the output space structure to the shape of a general
	hypercubical shape, with the overall dimensionality of the grid and
	its extensions along the different directions being subject of the
	adaptation. This constraint meets the demands of many larger information
	processing systems, of which the neural map can be a part. We apply
	our GSOM-algorithm to three examples, two of which involve real world
	data. Using recently developed methods for measuring the degree of
	neighborhood preservation in neural maps, we find the GSOM-algorithm
	to produce maps which preserve neighborhoods in a nearly optimal
	fashion},
  doi = {10.1109/72.557659},
  file = {paper:C\:\\Users\\castudil\\Documents\\Research\\Cesar\\References\\SOM\\SOM-variants\\non-tree-based\\Bauer1997.pdf:PDF},
  issn = {1045-9227},
  keywords = {self-organising feature mapsGSOM, Kohonen's self-organizing feature
	map, SOFM, growing self-organizing map, growth algorithm, hypercubical
	output space, learning output space grid, map-learning algorithm,
	neural maps, optimal neighborhood preservation, output space grid,
	output space topology},
  owner = {READ}
}

@INPROCEEDINGS{Beaton2009,
  author = {Beaton,, D. and Valova,, I. and MacLean,, D.},
  title = {Growing mechanisms and cluster identification with TurSOM},
  booktitle = {Neural Networks, 2009. IJCNN 2009. International Joint Conference
	on},
  year = {2009},
  pages = {280-287},
  month = {June},
  abstract = {TurSOM is a novel self-organizing map algorithm with the capability
	of connection reorganization, not just neuron reorganization. This
	behavior facilitates the ability to map distinct patterns in a given
	input space. Multiple networks exist, and operate independently.
	This work presents an application driven approach, based on the theoretical
	and empirical work of previous TurSOM experiments. TurSOM is a highly
	robust algorithm, designed to eliminate the need for post processing
	methods of cluster identification using SOM algorithms. One of the
	applications TurSOM is suitable for, but obviously not limited to,
	is image segmentation, as it is demonstrated in this work.},
  doi = {10.1109/IJCNN.2009.5178750},
  file = {paper:References\\SOM\\SOM-variants\\non-tree-based\\Beaton2009.pdf:PDF},
  issn = {1098-7576},
  keywords = {pattern clustering, self-organising feature mapsSOM algorithms, TurSOM,
	application driven approach, cluster identification, connection reorganization,
	image segmentation, neuron reorganization, robust algorithm, self-organizing
	map algorithm}
}

@BOOK{Bellman1961,
  title = {Adaptive Control Processes: A Guided Tour},
  publisher = {Princeton University Press},
  year = {1961},
  author = {Bellman, R.},
  owner = {castudil},
  timestamp = {2009.06.25}
}

@INBOOK{Ben-Gal2007,
  chapter = {Bayesian Networks},
  title = {Encyclopedia of Statistics in Quality and Reliability},
  publisher = {John Wiley \& Sons, Ltd},
  year = {2007},
  author = {Ben-Gal,, I.},
  abstract = {Bayesian networks are probabilistic models based on direct acyclic
	graphs. These models enable a direct representation of causal relations
	between variables. Their structure is ideal for combining prior knowledge,
	which often comes in causal form, and observed data. This article
	gives a short description of the concepts of this important class
	of models that have become extremely popular in recent years.},
  doi = {10.1002/9780470061572.eqr089},
  file = {Ben-Gal2007.pdf:Pattern Recognition/Ben-Gal2007.pdf:PDF},
  owner = {castudil},
  timestamp = {2010.08.24}
}

@ARTICLE{Bentley1975,
  author = {Bentley, Jon Louis},
  title = {Multidimensional binary search trees used for associative searching},
  journal = {Commun. ACM},
  year = {1975},
  volume = {18},
  pages = {509--517},
  number = {9},
  abstract = {This paper develops the multidimensional binary search tree (or k-d
	tree, where k is the dimensionality of the search space) as a data
	structure for storage of information to be retrieved by associative
	searches. The k-d tree is defined and examples are given. It is shown
	to be quite efficient in its storage requirements. A significant
	advantage of this structure is that a single data structure can handle
	many types of queries very efficiently. Various utility algorithms
	are developed; their proven average running times in an n record
	file are: insertion, O(log n); deletion of the root, O(n(k-1)/k);
	deletion of a random node, O(log n); and optimization (guarantees
	logarithmic performance of searches), O(n log n). Search algorithms
	are given for partial match queries with t keys specified [proven
	maximum running time of O(n(k-t)/k)] and for nearest neighbor queries
	[empirically observed average running time of O(log n).] These performances
	far surpass the best currently known algorithms for these tasks.
	An algorithm is presented to handle any general intersection query.
	The main focus of this paper is theoretical. It is felt, however,
	that k-d trees could be quite useful in many applications, and examples
	of potential uses are given.},
  address = {New York, NY, USA},
  doi = {http://doi.acm.org/10.1145/361002.361007},
  file = {Bentley1975.PDF:Trees/Bentley1975.PDF:PDF},
  issn = {0001-0782},
  publisher = {ACM}
}

@ARTICLE{Berglund2006,
  author = {Berglund,, E. and Sitte,, J.},
  title = {The parameterless self-organizing map algorithm},
  journal = {Neural Networks, IEEE Transactions on},
  year = {2006},
  volume = {17},
  pages = { 305 -- 316},
  number = {2},
  month = {march},
  abstract = { The parameterless self-organizing map (PLSOM) is a new neural network
	algorithm based on the self-organizing map (SOM). It eliminates the
	need for a learning rate and annealing schemes for learning rate
	and neighborhood size. We discuss the relative performance of the
	PLSOM and the SOM and demonstrate some tasks in which the SOM fails
	but the PLSOM performs satisfactory. Finally we discuss some example
	applications of the PLSOM and present a proof of ordering under certain
	limited conditions.},
  doi = {10.1109/TNN.2006.871720},
  file = {:SOM/SOM-variants/Berglund2006.pdf:PDF},
  issn = {1045-9227},
  keywords = { neural network algorithm; ordering proof; parameterless self-organizing
	map algorithm; self-organising feature maps;}
}

@BOOK{Bertsekas2008,
  title = {Introduction to Probability},
  publisher = {Athena Scientific},
  year = {2008},
  author = {Bertsekas,,D. P. and Tsitsiklis,,J. N.},
  pages = {544},
  edition = {2},
  month = {July},
  note = {hardcover},
  owner = {castudil},
  timestamp = {2010.08.26},
  url = {http://www.athenasc.com/probbook.html}
}

@BOOK{Bezdek1981,
  title = {Pattern Recognition With Fuzzy Objective Function Algorithms},
  publisher = {Plenum Press},
  year = {1981},
  author = {Bezdek,,J. C.},
  pages = {272},
  address = {New York},
  owner = {castudil},
  timestamp = {2009.09.21}
}

@PHDTHESIS{Bezdek1973,
  author = {Bezdek,,J. C.},
  title = {Fuzzy Mathematics in Pattern Classification},
  school = {Cornell University, Applied Mathematics Center},
  year = {1973},
  address = {Ithaca, New York},
  owner = {castudil},
  timestamp = {2009.09.21}
}

@PHDTHESIS{Birattari2004,
  author = {Birattari,, M.},
  title = {The Problem of Tuning Metaheuristics as Seen from a Machine Learning
	Perspective},
  school = {Université Libre de Bruxelles},
  year = {2004},
  type = {PhD Thesis},
  file = {Birattari2004.pdf:Parameter-Tunning/Birattari2004.pdf:PDF},
  owner = {castudillo},
  timestamp = {2012.01.05},
  url = {http://iridia.ulb.ac.be/~mbiro/publications.html}
}

@BOOK{Bishop1995,
  title = {Neural Networks for Pattern Recognition},
  publisher = {Oxford University Press, Inc.},
  year = {1995},
  author = {Bishop,, Christopher M.},
  address = {New York, NY, USA},
  isbn = {0198538642}
}

@ARTICLE{Bitner1979,
  author = {Bitner,,J. R.},
  title = {Heuristics that dynamically organize data structures},
  journal = {SIAM J. Comput.},
  year = {1979},
  volume = {8},
  pages = {82-110},
  owner = {castudil},
  timestamp = {2009.02.17}
}

@BOOK{Bjorck1996,
  title = {Numerical Methods for Least Squares Problems},
  publisher = {SIAM},
  year = {1996},
  author = {Bj{\"o}rck,,{\AA}.},
  address = {Philadelphia},
  kwds = {survey, sparse, iter}
}

@MISC{dads:multiway-tree,
  author = {Black,,P. E.},
  title = {``multiway tree''},
  howpublished = {in \emph{Dictionary of Algorithms and Data Structures} [online],
	Paul E. Black, ed., U.S. National Institute of Standards and Technology},
  month = {27 October},
  year = {2005},
  note = {(accessed 15 December 2010) % LAST ACCESS DATE Available from: http://xw2k.nist.gov/dads/HTML/multiwaytree.html}
}

@MASTERSTHESIS{Blackmore1995,
  author = {Blackmore,,J.},
  title = {Visualizing High-Dimensional Structure with the Incremental Grid
	Growing Neural Network},
  school = {University of Texas at Austin},
  year = {1995},
  file = {Blackmore1995.pdf:SOM/SOM-variants/non-tree-based/Blackmore1995.pdf:PDF},
  owner = {castudil},
  timestamp = {2010.01.12}
}

@INPROCEEDINGS{Blackmore1993,
  author = {Blackmore,,J. and Miikkulainen,,R.},
  title = {Incremental Grid Growing: Encoding High-Dimensional Structure into
	a Two-Dimensional Feature Map},
  booktitle = {Proc. {ICNN}'93, International Conference on Neural Networks},
  year = {1993},
  volume = {I},
  pages = {450--455},
  address = {Piscataway, NJ},
  publisher = {IEEE Service Center},
  file = {paper:C\:\\Users\\castudil\\Documents\\Research\\Cesar\\References\\SOM\\SOM-variants\\non-tree-based\\Blackmore1993.PDF:PDF}
}

@ARTICLE{Block1962,
  author = {Block, H. D.},
  title = {The Perceptron: A Model for Brain Functioning},
  journal = {Rev. Mod. Phys.},
  year = {1962},
  volume = {34},
  pages = {123--135},
  number = {1},
  month = {Jan},
  doi = {10.1103/RevModPhys.34.123},
  file = {paper:References\\ANN\\Block1962.PDF:PDF},
  numpages = {12},
  owner = {castudil},
  publisher = {American Physical Society},
  timestamp = {2009.12.07}
}

@ARTICLE{Bonabeau2002,
  author = {Bonabeau,,E.},
  title = {Graph multidimensional scaling with self-organizing maps},
  journal = {Information Sciences},
  year = {2002},
  volume = {143},
  pages = {159 - 180},
  number = {1-4},
  abstract = {Self-organizing maps (SOM) are unsupervised, competitive neural networks
	used to project high-dimensional data onto a low-dimensional space.
	In this paper it is shown that SOM can be used to perform multidimensional
	scaling (MDS) on graphs. The SOM-based approach is applied to two
	families of random graphs and three real-world networks.},
  doi = {10.1016/S0020-0255(02)00191-3},
  file = {Bonabeau2002.pdf:SOM\\applications\\Bonabeau2002.pdf:PDF},
  issn = {0020-0255},
  keywords = {Self-organizing maps},
  url = {http://www.sciencedirect.com/science/article/B6V0C-45H97WH-2/2/c4e0e41474c02b0ff21a3e0b3d852ad7}
}

@ARTICLE{Bradley1997,
  author = {Bradley,,A. P.},
  title = {The use of the area under the {ROC} curve in the evaluation of machine
	learning algorithms},
  journal = {Pattern Recognition},
  year = {1997},
  volume = {30},
  pages = {1145--1159},
  number = {7},
  abstract = {In this paper we investigate the use of the area under the receiver
	operating characteristic (ROC) curve (AUC) as a performance measure
	for machine learning algorithms. As a case study we evaluate six
	machine learning algorithms (C4.5, Multiscale Classifier, Perceptron,
	Multi-layer Perceptron, k-Nearest Neighbours, and a Quadratic Discriminant
	Function) on six ``real world'' medical diagnostics data sets. We
	compare and discuss the use of AUC to the more conventional overall
	accuracy and find that AUC exhibits a number of desirable properties
	when compared to overall accuracy: increased sensitivity in Analysis
	of Variance (ANOVA) tests; a standard error that decreased as both
	AUC and the number of test samples increased; decision threshold
	independent; and it is invariant to a priori class probabilities.
	The paper concludes with the recommendation that AUC be used in preference
	to overall accuracy for ``single number'' evaluation of machine learning
	algorithms.},
  doi = {10.1016/S0031-3203(96)00142-2},
  file = {Bradley1997.pdf:Statistics/Bradley1997.pdf:PDF},
  issn = {0031-3203},
  keywords = {The ROC curve},
  owner = {castudil},
  timestamp = {2010.08.29}
}

@INCOLLECTION{Bramer2002,
  author = {Bramer,, M.},
  title = {Pre-pruning Classification Trees to Reduce Overfitting in Noisy Domains},
  booktitle = {Intelligent Data Engineering and Automated Learning -- IDEAL 2002},
  publisher = {Springer Berlin / Heidelberg},
  year = {2002},
  editor = {Yin, Hujun and Allinson, Nigel and Freeman, Richard and Keane, John
	and Hubbard, Simon},
  volume = {2412},
  series = {Lecture Notes in Computer Science},
  pages = {247--258},
  abstract = {The automatic induction of classification rules from examples in the
	form of a classification tree is an important technique used in data
	mining. One of the problems encountered is the overfitting of rules
	to training data. In some cases this can lead to an excessively large
	number of rules, many of which have very little predictive value
	for unseen data. This paper describes a means of reducing overfitting
	known as J-pruning, based on the J-measure, an information theoretic
	means of quantifying the information content of a rule. It is demonstrated
	that using J-pruning generally leads to a substantial reduction in
	the number of rules generated and an increase in predictive accuracy.
	The advantage gained becomes more pronounced as the proportion of
	noise increases.},
  affiliation = {University of Portsmouth Faculty of Technology UK},
  doi = {10.1007/3-540-45675-9\_2},
  file = {Bramer2002.pdf:Pattern Recognition/Bramer2002.pdf:PDF}
}

@ARTICLE{Breiman2001,
  author = {Breiman, Leo},
  title = {Random Forests},
  journal = {Machine Learning},
  year = {2001},
  volume = {45},
  pages = {5--32},
  abstract = {Random forests are a combination of tree predictors such that each
	tree depends on the values of a random vector sampled independently
	and with the same distribution for all trees in the forest. The generalization
	error for forests converges a.s. to a limit as the number of trees
	in the forest becomes large. The generalization error of a forest
	of tree classifiers depends on the strength of the individual trees
	in the forest and the correlation between them. Using a random selection
	of features to split each node yields error rates that compare favorably
	to Adaboost (Y. Freund &amp; R. Schapire, Machine Learning: Proceedings
	of the Thirteenth International conference, ***, 148â156), but
	are more robust with respect to noise. Internal estimates monitor
	error, strength, and correlation and these are used to show the response
	to increasing the number of features used in the splitting. Internal
	estimates are also used to measure variable importance. These ideas
	are also applicable to regression.},
  doi = {10.1023/A:1010933404324},
  file = {Breiman2001.pdf:Trees/Breiman2001.pdf:PDF},
  issn = {0885-6125},
  issue = {1},
  keyword = {Computer Science},
  publisher = {Springer Netherlands}
}

@BOOK{Breiman1984,
  title = {Classification and Regression Trees},
  publisher = {Wadsworth International},
  year = {1984},
  author = {Breiman,,L. and Friedman,,J. H. and Olshen,, R. A. and Stone,,C.
	J.},
  address = {Belmont, CA},
  owner = {castudillo},
  review = {Classification and Regression Trees, , , .},
  timestamp = {2010.12.01}
}

@ARTICLE{Breslow1997,
  author = {Breslow,L. A. and Aha,D. W.},
  title = {Simplifying decision trees: A survey},
  journal = {The Knowledge Engineering Review},
  year = {1997},
  volume = {12},
  pages = {1-40},
  number = {01},
  abstract = { ABSTRACT Induced decision trees are an extensively-researched solution
	to classification tasks. For many practical tasks, the trees produced
	by tree-generation algorithms are not comprehensible to users due
	to their size and complexity. Although many tree induction algorithms
	have been shown to produce simpler, more comprehensible trees (or
	data structures derived from trees) with good classification accuracy,
	tree simplification has usually been of secondary concern relative
	to accuracy, and no attempt has been made to survey the literature
	from the perspective of simplification. We present a framework that
	organizes the approaches to tree simplification and summarize and
	critique the approaches within this framework. The purpose of this
	survey is to provide researchers and practitioners with a concise
	overview of tree-simplification approaches and insight into their
	relative capabilities. In our final discussion, we briefly describe
	some empirical findings and discuss the application of tree induction
	algorithms to case retrieval in case-based reasoning systems. },
  doi = {10.1017/S0269888997000015},
  eprint = {http://journals.cambridge.org/article_S0269888997000015},
  file = {Breslow1997.pdf:Pattern Recognition/Breslow1997.pdf:PDF},
  url = {http://journals.cambridge.org/action/displayAbstract?fromPage=online&aid=70953&fulltextType=RA&fileId=S0269888997000015}
}

@BOOK{Buckley1990,
  title = {Distance in Graphs},
  publisher = {Addison-Wesley},
  year = {1990},
  author = {Buckley,, F. and Harary,, F.},
  address = {Redwood City, CA},
  owner = {castudil},
  timestamp = {2009.09.25}
}

@ARTICLE{Budinich1995,
  author = {Budinich,,M.},
  title = {On the ordering conditions for self-organizing maps},
  journal = {Neural Computation},
  year = {1995},
  volume = {7},
  pages = {284--289},
  number = {2},
  file = {paper:References/SOM/som-related/Budinich1995.pdf:PDF},
  owner = {castudil},
  timestamp = {2009.10.29}
}

@ARTICLE{Buzo1980,
  author = {Buzo, A. and Gray, A., Jr. and Gray, R. and Markel, J.},
  title = {Speech coding based upon vector quantization},
  journal = {Acoustics, Speech and Signal Processing, IEEE Transactions on},
  year = {1980},
  volume = {28},
  pages = { 562 - 574},
  number = {5},
  month = oct,
  abstract = {With rare exception, all presently available narrow-band speech coding
	systems implement scalar quantization (independent quantization)
	of the transmission parameters (such as reflection coefficients or
	transformed reflection coefficients in LPC systems). This paper presents
	a new approach called vector quantization. For very low data rates,
	realistic experiments have shown that vector quantization can achieve
	a given level of average distortion with 15 to 20 fewer bits/frame
	than that required for the optimized scalar quantizing approaches
	presently in use. The vector quantizing approach is shown to be a
	mathematically and computationally tractable method which builds
	upon knowledge obtained in linear prediction analysis studies. This
	paper introduces the theory in a nonrigorous form, along with practical
	results to date and an extensive list of research topics for this
	new area of speech coding.},
  doi = {10.1109/TASSP.1980.1163445},
  file = {Buzo1980.pdf:fast-BMU/Buzo1980.pdf:PDF},
  issn = {0096-3518}
}

@BOOK{Camastra2007,
  title = {Machine Learning for Audio, Image and Video Analysis: Theory and
	Applications},
  publisher = {Springer London},
  year = {2007},
  author = {Camastra,, Francesco and Vinciarelli,, Alessandro},
  doi = {10.1007/978-1-84800-007-0},
  file = {Table of Contents:References/book/Camastra2007/back-matter.pdf:PDF;Chapter 1 \: Introduction:References/book/Camastra2007/c1-Introduction.pdf:PDF;Chapter 2 \: Audio Acquisition, Representation and Storage:References/book/Camastra2007/c2-Audio Acquisition, Representation and Storage.pdf:PDF;Chapter 6 \: Clustering Methods:References/book/Camastra2007/c6-Clustering Methods.pdf:PDF},
  isbn = {1848000065, 9781848000063},
  url = {http://www.springerlink.com/content/978-1-84800-006-3?v=editorial}
}

@ARTICLE{Campos2001,
  author = {Campos,,M. M. and Carpenter,,G. A.},
  title = {S-TREE: self-organizing trees for data clustering and online vector
	quantization},
  journal = {Neural Networks},
  year = {2001},
  volume = {14},
  pages = {505 - 525},
  number = {4-5},
  doi = {DOI: 10.1016/S0893-6080(01)00020-X},
  file = {the paper:References/SOM/SOM-variants/Campos2001.pdf:PDF},
  issn = {0893-6080},
  keywords = {Hierarchical clustering},
  url = {http://www.sciencedirect.com/science/article/B6T08-431YRR1-9/2/3d1eb0c74f850f82e7b8b8318a47f1b4}
}

@CONFERENCE{Candia2004,
  author = {Candia,,Alfredo and Astudillo,,C\'{e}sar A.},
  title = {Algoritmos para el Problema de las n-reinas},
  booktitle = {30th Latin-American Conference on Informatics},
  year = {2004},
  pages = {178-185},
  abstract = {The n-queens problem is to find the different ways to assign n non-attacking
	queens in a nxn chessboard. This work analizes the application of
	an algorithm based on Local Search for the resolution of the n-queens
	problem.
	
	Empirical results show that, large size instances of the problem are
	well solved by the Local Search algorithm in comparisson to more
	sophisticated algorithms like Genetic Algorithms.},
  keywords = {n-queens problem, Local Search},
  owner = {castudil},
  timestamp = {2009.03.19}
}

@ARTICLE{Carpenter1988,
  author = {Carpenter, G. A. and Grossberg, S.},
  title = {The ART of Adaptive Pattern Recognition by a Self-Organizing Neural
	Network},
  journal = {Computer},
  year = {1988},
  volume = {21},
  pages = {77--88},
  number = {3},
  abstract = {The adaptive resonance theory (ART) suggests a solution to the stability-plasticity
	dilemma facing designers of learning systems, namely how to design
	a learning system that will remain plastic, or adaptive, in response
	to significant events and yet remain stable in response to irrelevant
	events. ART architectures are discussed that are neural networks
	that self-organize stable recognition codes in real time in response
	to arbitrary sequences of input patterns. Within such an ART architecture,
	the process of adaptive pattern recognition is a special case of
	the more general cognitive process of hypothesis discovery, testing,
	search, classification, and learning. This property opens up the
	possibility of applying ART systems to more general problems of adaptively
	processing large abstract information sources and databases. The
	main computational properties of these ART architectures are outlined
	and contrasted with those of alternative learning and recognition
	systems.},
  address = {Los Alamitos, CA, USA},
  doi = {http://dx.doi.org/10.1109/2.33},
  file = {paper:SOM\\som-related\\Carpenter1988.pdf:PDF},
  issn = {0018-9162},
  publisher = {IEEE Computer Society Press}
}

@INPROCEEDINGS{Chan2002,
  author = {Chan,Alton,Kam-Fai and Woo,,Kam-Tim and Kok,,Chi-Wah},
  title = {Vector quantization fast search algorithm using hyperplane based
	k-dimensional multi-node search tree},
  booktitle = {Circuits and Systems, 2002. ISCAS 2002. IEEE International Symposium
	on},
  year = {2002},
  volume = {1},
  pages = { I-793-I-796 vol.1},
  doi = {10.1109/ISCAS.2002.1009960},
  issn = { },
  keywords = { backtracking, computational complexity, image classification, image
	coding, tree searching, vector quantisation PSNR, computation time,
	data compression, hyperplane based search tree, hyperplane decision,
	image coding, k-dimensional multi-node search tree, lower bound,
	misclassification problem, multi-level backtracking algorithm, performance,
	relative distance quantization rule, search distance, triangular
	inequality, vector quantization complexity}
}

@ARTICLE{Cheetham1993,
  author = {Cheetham,, R. P. and Oommen,, B. J. and Ng,, D. T. H.},
  title = {Adaptive Structuring of Binary Search Trees Using Conditional Rotations},
  journal = {IEEE Trans. on Knowl. and Data Eng.},
  year = {1993},
  volume = {5},
  pages = {695--704},
  number = {4},
  address = {Piscataway, NJ, USA},
  doi = {http://dx.doi.org/10.1109/69.234780},
  file = {paper:ADS\\Cheetham1993.PDF:PDF;Cheetham1993.PDF:References/ADS/Cheetham1993.PDF:PDF},
  issn = {1041-4347},
  publisher = {IEEE Educational Activities Department}
}

@ARTICLE{Chen2008,
  author = {Chen, S.X. and Li, F.W. and Zhu, W.L.},
  title = {Fast searching algorithm for vector quantisation based on features
	of vector and subvector},
  journal = {Image Processing, IET},
  year = {2008},
  volume = {2},
  pages = {275--285},
  number = {6},
  month = december,
  abstract = {Vector quantisation (VQ) is an efficient technique for data compression
	and retrieval. But its encoding requires expensive computation that
	greatly limits its practical use. A fast algorithm for VQ encoding
	on the basis of features of vectors and subvectors is presented.
	Making use of three characteristics of a vector: the sum, the partial
	sum and the partial variance, a four-step eliminating algorithm is
	introduced. The proposed algorithm can reject a lot of codewords,
	while holding the same quality of encoded images as the full search
	algorithm (FSA). Experimental results show that the proposed algorithm
	needs only a little computational complexity and distortion calculation
	against the FSA. Compared with the equal-average equal-variance equal-norm
	nearest neighbour search algorithm based on the ordered Hadamard
	transform, the proposed algorithm reduces the number of distortion
	calculations by 8 to 61%. The average number of operations of the
	proposed algorithm is %79% of that of Zhibin%s method for all test
	images. The proposed algorithm outperforms most of existing algorithms.},
  doi = {10.1049/iet-ipr:20070153},
  file = {Chen2008.pdf:fast-BMU/Chen2008.pdf:PDF},
  issn = {1751-9659},
  keywords = {Hadamard transform;VQ encoding;computational complexity;data compression;data
	retrieval;equal-average search algorithm;equal-variance equal-norm
	nearest neighbour search algorithm;fast searching algorithm;four-step
	eliminating algorithm;full search algorithm;image encoding;vector
	quantisation;Hadamard transforms;computational complexity;data compression;image
	coding;search problems;vector quantisation;}
}

@INPROCEEDINGS{Cheng1984,
  author = {Cheng,, D.-Y. and Gersho, A. and Ramamurthi, B. and Shoham, Y.},
  title = {Fast search algorithms for vector quantization and pattern matching},
  booktitle = {Acoustics, Speech, and Signal Processing, IEEE International Conference
	on ICASSP '84.},
  year = {1984},
  volume = {9},
  pages = {372 - 375},
  month = mar,
  abstract = {A fundamental computational task that arises in several areas of signal
	processing is pattern matching, where a given test pattern is compared
	with a large set of stored templates, to find the best match that
	minimizes a given measure of dissimilarity. Three different geometrically-oriented
	methods are proposed for substantially reducing the computational
	complexity of the search process by reducing the number of multiplies
	in exchange for additional low complexity operations and, in two
	of the methods, additional memory for storing precomputed tables.},
  doi = {10.1109/ICASSP.1984.1172352},
  file = {Cheng1984.pdf:RHST/HyperplaneTree-classifiers/Cheng1984.pdf:PDF}
}

@ARTICLE{Cho2000,
  author = {Cho,,S.-B.},
  title = {Ensemble of structure-adaptive self-organizing maps for high performance
	classification},
  journal = {Information Sciences},
  year = {2000},
  volume = {123},
  pages = {103 - 114},
  number = {1-2},
  abstract = {Combining multiple models has been recently exploited for the development
	of reliable neural networks. This paper introduces a structure-adaptive
	self-organizing map (SOM) which can adapt the structure as well as
	the weights, and presents a method to improve the performance by
	combining the multiple maps. The structure-adaptive SOM places the
	nodes of prototype vectors into the pattern space properly so as
	to make the decision boundaries as close to the class boundaries
	as possible. In order to show the performance of the proposed method,
	experiments with the unconstrained handwritten digit database of
	Concordia University in Canada have been conducted.},
  doi = {10.1016/S0020-0255(99)00112-7},
  file = {Cho2000.pdf:SOM/SOM-variants/Cho2000.pdf:PDF},
  issn = {0020-0255},
  owner = {castudil},
  timestamp = {2010.07.06},
  url = {http://www.sciencedirect.com/science/article/B6V0C-3YCV0B3-M/2/4c5b0a8822a3dc6dab4482963cdefca6}
}

@ARTICLE{Chow2009,
  author = {Chow, T. W. S. and Rahman, M. K. M.},
  title = {Multilayer {SOM} With Tree-Structured Data for Efficient Document
	Retrieval and Plagiarism Detection},
  journal = {Neural Networks, IEEE Transactions on},
  year = {2009},
  volume = {20},
  pages = {1385 --1402},
  number = {9},
  month = {sept. },
  abstract = {This paper proposes a new document retrieval (DR) and plagiarism detection
	(PD) system using multilayer self-organizing map (MLSOM). A document
	is modeled by a rich tree-structured representation, and a SOM-based
	system is used as a computationally effective solution. Instead of
	relying on keywords/lines, the proposed scheme compares a full document
	as a query for performing retrieval and PD. The tree-structured representation
	hierarchically includes document features as document, pages, and
	paragraphs. Thus, it can reflect underlying context that is difficult
	to acquire from the currently used word-frequency information. We
	show that the tree-structured data is effective for DR and PD. To
	handle tree-structured representation in an efficient way, we use
	an MLSOM algorithm, which was previously developed by the authors
	for the application of image retrieval. In this study, it serves
	as an effective clustering algorithm. Using the MLSOM, local matching
	techniques are developed for comparing text documents. Two novel
	MLSOM-based PD methods are proposed. Detailed simulations are conducted
	and the experimental results corroborate that the proposed approach
	is computationally efficient and accurate for DR and PD.},
  doi = {10.1109/TNN.2009.2023394},
  file = {paper:SOM\\SOM-variants\\Chow2009.PDF:PDF},
  issn = {1045-9227},
  keywords = {MLSOM algorithm;clustering algorithm;document retrieval system;image
	retrieval;local matching techniques;multilayer selforganizing map;plagiarism
	detection system;simulation;tree-structured representation;word-frequency
	information;image retrieval;query processing;self-organising feature
	maps;tree data structures;}
}

@ARTICLE{Clarkson1988,
  author = {Clarkson,, K. L.},
  title = {A randomized algorithm for closest-point queries},
  journal = {SIAM J. Comput.},
  year = {1988},
  volume = {17},
  pages = {830--847},
  month = {August},
  abstract = {An algorithm for closest-point queries is given. The problem is this:
	given a set $S$ of $n$ points in $d$-dimensional space, build a data
	structure so that given an arbitrary query point $p$, a closest point
	in $S$ to $p$ can be found quickly. The measure of distance is the
	Euclidean norm. This is sometimes called the post-office problem.
	The new data structure will be termed an RPO tree, from Randomized
	Post Office. The expected time required to build an RPO tree is $O(n^{\lceil
	{{d / 2}} \rceil (1 + \epsilon )} )$, for any fixed $\epsilon > 0$,
	and a query can be answered in $O(\log n)$ worst-case time. An RPO
	tree requires $O(n^{\lceil {{d / 2}} \rceil (1 + \epsilon )} )$ space
	in the worst case. The constant factors in these bounds depend on
	$d$ and $\epsilon $. The bounds are average-case due to the randomization
	employed by the algorithm, and hold for any set of input points.
	This result approaches the $\Omega (n^{\lceil {{d / 2}} \rceil }
	)$ worst-case time required for any algorithm that constructs the
	Voronoi diagram of the input points, and is a considerable improvement
	over previous bounds for $d > 3$. The main step of the construction
	algorithm is the determination of the Voronoi diagram of a random
	sample of the sites, and the triangulation of that diagram.},
  acmid = {64991},
  address = {Philadelphia, PA, USA},
  doi = {10.1137/0217052},
  issn = {0097-5397},
  issue = {4},
  numpages = {18},
  publisher = {Society for Industrial and Applied Mathematics},
  url = {http://portal.acm.org/citation.cfm?id=64978.64991}
}

@INPROCEEDINGS{Conti1991,
  author = {Conti,, P. L. and De Giovanni,, L.},
  title = {On the mathematical treatment of self organization: extension of
	some classical results},
  booktitle = {Artificial Neural Networks - ICANN 1991, International Conference},
  year = {1991},
  volume = {2},
  pages = {1089-1812},
  owner = {castudil},
  timestamp = {2009.10.29}
}

@BOOK{Cormen2001,
  title = {Introduction to Algorithms, Second Edition},
  publisher = {McGraw-Hill Science/Engineering/Math},
  year = {2001},
  author = {Cormen, T. H. and Leiserson, C. E. and Rivest, R. L. and Stein, C.
	},
  month = {July},
  howpublished = {Hardcover},
  isbn = {0070131511},
  posted-at = {2008-08-14 06:44:54},
  url = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/0070131511}
}

@ARTICLE{Cortez2009,
  author = {Cortez,,P. and Cerdeira,,A. and Almeida,,F. and Matos,,T. and Reis,,J.},
  title = {Modeling wine preferences by data mining from physicochemical properties},
  journal = {Decision Support Systems},
  year = {2009},
  volume = {47},
  pages = {547 - 553},
  number = {4},
  note = {Smart Business Networks: Concepts and Empirical Evidence},
  abstract = {We propose a data mining approach to predict human wine taste preferences
	that is based on easily available analytical tests at the certification
	step. A large dataset (when compared to other studies in this domain)
	is considered, with white and red vinho verde samples (from Portugal).
	Three regression techniques were applied, under a computationally
	efficient procedure that performs simultaneous variable and model
	selection. The support vector machine achieved promising results,
	outperforming the multiple regression and neural network methods.
	Such model is useful to support the oenologist wine tasting evaluations
	and improve wine production. Furthermore, similar techniques can
	help in target marketing by modeling consumer tastes from niche markets.},
  doi = {10.1016/j.dss.2009.05.016},
  file = {Cortez2009.pdf:Wine\\Cortez2009.pdf:PDF},
  issn = {0167-9236},
  keywords = {Sensory preferences}
}

@INPROCEEDINGS{Dara2002,
  author = {Dara,, R. and Kremer,, S.C. and Stacey,, D.A.},
  title = {Clustering unlabeled data with {SOM}s improves classification of
	labeled real-world data},
  booktitle = {Neural Networks, 2002. IJCNN '02. Proc. of the 2002 International
	Joint Conference on},
  year = {2002},
  volume = {3},
  pages = {2237 -2242},
  abstract = {We show the use of a self organizing map to cluster unlabeled data
	and to infer possible labelings from the clusters. Our inferred labels
	are presented to a multilayer perceptron along with labeled data,
	performance is improved over using only the labeled data. Results
	are presented for a number of popular real-world benchmark problems
	from domains other than text. This shows one way in which unlabeled
	data can be used to enhance supervised learning in a general-purpose
	neural network},
  doi = {10.1109/IJCNN.2002.1007489},
  file = {Dara2002.pdf:Pattern Recognition\\Dara2002.pdf:PDF},
  keywords = {SOMs;classification;general-purpose neural network;labeled real-world
	data;multilayer perceptron;self organizing map;supervised learning;unlabeled
	data;learning (artificial intelligence);multilayer perceptrons;pattern
	classification;pattern clustering;self-organising feature maps;}
}

@ARTICLE{Datta1996,
  author = {Datta,,A. and Parui,,S. M. and Chaudhuri,,B. B.},
  title = {Skeletal shape extraction from dot patterns by self-organization},
  journal = {Pattern Recognition, 1996., Proceedings of the 13th International
	Conference on},
  year = {1996},
  volume = {4},
  pages = {80-84 vol.4},
  month = {Aug},
  abstract = {Extraction of skeletal shape from a 2D dot pattern is discussed. We
	use a self-organizing neural network model to get a piecewise linear
	approximation of a skeleton of the pattern. It is found that even
	without a proper definition of a skeleton, the proposed algorithm
	is able to produce skeletons that are quite close to what we intuitively
	feel it should be. In Kohonen's self-organizing model, the set of
	processors and their neighbourhoods are fixed. We suggest here some
	modifications of it in which the set of processors and their neighbourhoods
	change adaptively},
  doi = {10.1109/ICPR.1996.547238},
  keywords = {approximation theory, character recognition, computer vision, feature
	extraction, self-organising feature maps, trees (mathematics)2D dot
	patterns, Kohonen self-organizing model, character recognition, feature
	extraction, piecewise linear approximation, self-organizing neural
	network, skeletal shape extraction, skeleton, tree patterns}
}

@ARTICLE{Demsar2006,
  author = {Dem\v{s}ar,, J.},
  title = {Statistical Comparisons of Classifiers over Multiple Data Sets},
  journal = {J. Mach. Learn. Res.},
  year = {2006},
  volume = {7},
  pages = {1--30},
  file = {Demsar2006.pdf:References/Statistics/Demsar2006.pdf:PDF},
  issn = {1532-4435},
  publisher = {JMLR.org},
  review = {Demsar’s Recommendations
	
	- Wilcoxon Signed Ranks Test to compare a pair of algorithms
	
	- Friedman’s Test to look for differences between a number of algorithms
	
	- Post-hoc tests to look a pair-wise differences}
}

@INPROCEEDINGS{Demiriz1999,
  author = {Demiriz,,A. and Bennett,,K. and Embrechts,,M.J.},
  title = {Semi-Supervised Clustering Using Genetic Algorithms},
  booktitle = {Artificial Neural Networks in Engineering (ANNIE-99)},
  year = {1999},
  pages = {809--814},
  abstract = {A semi-supervised clustering algorithm is proposed that combines the
	benefits of supervised and unsupervised learning methods. Data are
	segmented/clustered using an unsupervised learning technique that
	is biased toward producing segments or clusters as pure as possible
	in terms of class distribution. These clusters can then be used to
	predict the class of future points. For example in database marketing,
	the technique can be used to identify and characterize segments of
	the customer population likely to respond to a promotion. One benefit
	of the approach is that it allows unlabeled data with no known class
	to be used to improve classification accuracy. The objective function
	of an unsupervised technique, e.g. K-means clustering, is modified
	to minimize both the within cluster variance of the input attributes
	and a measure of cluster impurity based on the class labels. Minimizing
	the within cluster variance of the examples is a form of capacity
	control to prevent overfitting. For the the output labels, impurity
	measures from decision tree algorithms such as the Gini index can
	be used. A genetic algorithm optimizes the objective function to
	produce clusters. Non-empty clusters are labeled with the majority
	class. Experimental results show that using class information improves
	the generalization ability compared to unsupervised methods based
	only on the input attributes. The results also indicate that the
	method performs very well even when few training examples are available.
	Training using information from unlabeled data can improve classification
	accuracy on that data as well.},
  file = {Demiriz1999.pdf:Pattern Recognition\\Demiriz1999.pdf:PDF}
}

@BOOK{Descartes2001,
  title = {Discourse on Method, Optics, Geometry, and Meteorology.},
  publisher = {Hackett Publishing},
  year = {2001},
  author = {Descartes,, R.},
  note = {Oscamp, P. J. (trans)},
  owner = {castudillo},
  timestamp = {2011.01.19}
}

@ARTICLE{Deschenes1995,
  author = {Deschenes,,C. J. and Noonan,,J.},
  title = {Fuzzy kohonen network for the classification of transients using
	the wavelet transform for feature extraction},
  journal = {Information Sciences},
  year = {1995},
  volume = {87},
  pages = {247 - 266},
  number = {4},
  abstract = {A system for identifying and classifying short duration signals (transients)
	is proposed. The transients are perturbed by multiplicative noise,
	and are embedded in various noise backgrounds to simulate an undersea
	environment. The transients are generated using FM chirp sum and
	sinusoidal sum models. The system uses wavelets as linear filters
	for preprocessing, and a fuzzy Kohonen neural network for classification.
	The design of the classifier system is presented, as well as results
	from initial experiments. The system is shown to be able to classify
	signals down to -1 dB.},
  doi = {10.1016/0020-0255(95)00139-5},
  file = {Deschenes1995.pdf:SOM\\applications\\Deschenes1995.pdf:PDF},
  issn = {0020-0255},
  url = {http://www.sciencedirect.com/science/article/B6V0C-403253H-G/2/0df47bff048dd675b42bcc66f9f50c2c}
}

@ARTICLE{DeSieno1988,
  author = {DeSieno, D.},
  title = {Adding a conscience to competitive learning},
  journal = {IEEE International Conference on Neural Networks},
  year = {1988},
  volume = {1},
  pages = {117-124},
  month = {July},
  abstract = {There are a number of neural networks that self-organize on the basis
	of what has come to be known as Kohonen learning. The author introduces
	a modification of Kohonen learning that provides rapid convergence
	and improved representation of the input data. In many areas of pattern
	recognition, statistical analysis, and control, it is essential to
	form a nonparametric model of a probability density function p(x).
	The purpose of the improvement to Kohonen learning presented is to
	form a better approximation of p (x). Simulation results are presented
	to illustrate the operation of this competitive learning algorithm},
  doi = {10.1109/ICNN.1988.23839},
  file = {paper:References\\SOM\\SOM-variants\\modules\\DeSieno1998.PDF:PDF},
  keywords = {learning systems, neural nets, self-adjusting systemsKohonen learning,
	competitive learning, conscience, convergence, learning systems,
	neural nets, nonparametric model, probability density function, self
	adjusting systems, self organising systems}
}

@ARTICLE{Devroye1986,
  author = {Devroye, L.},
  title = {A note on the height of binary search trees},
  journal = {J. ACM},
  year = {1986},
  volume = {33},
  pages = {489--498},
  month = {May},
  acmid = {5930},
  address = {New York, NY, USA},
  doi = {http://doi.acm.org/10.1145/5925.5930},
  file = {Devroye1986.pdf:RHST/Devroye1986.pdf:PDF},
  issn = {0004-5411},
  issue = {3},
  numpages = {10},
  publisher = {ACM}
}

@ARTICLE{Devroye2009,
  author = {Devroye,,L. and King,,J. and McDiarmid,,C.},
  title = {Random Hyperplane Search Trees},
  journal = {SIAM J. Comput.},
  year = {2009},
  volume = {38},
  pages = {2411-2425},
  number = {6},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  doi = {http://dx.doi.org/10.1137/060678609},
  file = {paper:Trees\\Devroye2009.PDF:PDF}
}

@INPROCEEDINGS{Dittenbach2000,
  author = {Dittenbach, M. and Merkl, D. and Rauber, A.},
  title = {The growing hierarchical self-organizing map},
  booktitle = {Neural Networks, 2000. IJCNN 2000, Proceedings of the IEEE-INNS-ENNS
	International Joint Conference on},
  year = {2000},
  volume = {6},
  pages = {15-19 vol.6},
  abstract = {We present the growing hierarchical self-organizing map. This dynamically
	growing neural network model evolves into a hierarchical structure
	according to the requirements of the input data during an unsupervised
	training process. We demonstrate the benefits of this novel neural
	network model by organizing a real-world document collection according
	to their similarities},
  doi = {10.1109/IJCNN.2000.859366},
  file = {paper:C\:\\Users\\castudil\\Documents\\Research\\Cesar\\References\\SOM\\SOM-variants\\Dittenbach2000.pdf:PDF},
  keywords = {full-text databases, self-organising feature maps, unsupervised learningdynamically
	growing neural network model, growing hierarchical self-organizing
	map, hierarchical structure, real-world document collection, unsupervised
	training process}
}

@INPROCEEDINGS{Dittenbach2001,
  author = {Dittenbach,,M. and Rauber,,A. and Merkl,,D.},
  title = {Recent Advances with the {G}rowing {H}ierarchical {S}elf-{O}rganizing
	{M}ap},
  booktitle = {Proceedings of the 3rd Workshop on Self-Organizing Maps},
  year = {2001},
  editor = {Allinson, N. and Yin, H. and Allinson, L. and Slack, J.},
  series = {Advances in Self-Organizing Maps},
  pages = {140-145},
  address = {Lincoln, England},
  month = {June 13-15},
  publisher = {Springer},
  abstract = {We present our recent work on the Growing Hierarchical Self-Organizing
	Map, a dynamically growing neural network model which evolves into
	a hierarchical structure according to the necessities of the input
	data during an unsupervised training process. The benefits of this
	novel architecture are shown by organizing a real-world document
	collection according to semantic similarities.},
  file = {paper:C\:\\Users\\castudil\\Documents\\Research\\Cesar\\References\\SOM\\SOM-variants\\Dittenbach2001.pdf:PDF},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.57.6813}
}

@INBOOK{Dopazo2007,
  chapter = {Clustering -- Class Discovery in the Post-Genomic Era},
  pages = {123--148},
  title = {Fundamentals of Data Mining in Genomics and Proteomics},
  publisher = {Springer US},
  year = {2007},
  author = {Dopazo, J.},
  abstract = {From a historical perspective we can distinguish an initial period
	in the DNA microarray technology in which almost all publications
	were related to reproducibility and sensitivity issues. Thus, many
	classical microarray papers dating from the late nineties were simple
	proof-of-prineiple experiments (Eisen et al., 1998); (Perou et al.,
	1999)},
  file = {Dopazo2007.pdf:SOM\\SOM-variants\\Dopazo2007.pdf:PDF},
  owner = {castudil},
  timestamp = {2010.01.27},
  url = {http://dx.doi.org/10.1007/978-0-387-47509-7_6}
}

@ARTICLE{Dopazo1997,
  author = {Dopazo,, J. and Carazo,, J. M.},
  title = {Phylogenetic Reconstruction Using an Unsupervised Growing Neural
	Network That Adopts the Topology of a Phylogenetic Tree},
  journal = {Journal of Molecular Evolution},
  year = {1997},
  volume = {44},
  pages = {226--233},
  number = {2},
  month = feb,
  abstract = {We propose a new type of unsupervised, growing, self-organizing neural
	network that expands itself by following the taxonomic relationships
	that exist among the sequences being classified. The binary tree
	topology of this neutral network, contrary to other more classical
	neural network topologies, permits an efficient classification of
	sequences. The growing nature of this procedure allows to stop it
	at the desired taxonomic level without the necessity of waiting until
	a complete phylogenetic tree is produced. This novel approach presents
	a number of other interesting properties, such as a time for convergence
	which is, approximately, a lineal function of the number of sequences.
	Computer simulation and a real example show that the algorithm accurately
	finds the phylogenetic tree that relates the data. All this makes
	the neural network presented here an excellent tool for phylogenetic
	analysis of a large number of sequences.},
  file = {Dopazo1997.pdf:SOM\\SOM-variants\\Dopazo1997.pdf:PDF},
  owner = {castudil},
  timestamp = {2010.01.27},
  url = {http://dx.doi.org/10.1007/PL00006139}
}

@BOOK{Duda2000,
  title = {Pattern Classification (2nd Edition)},
  publisher = {Wiley-Interscience},
  year = {2000},
  author = {Duda,,R. and Hart,P. E. and Stork,,D. G.},
  isbn = {0471056693}
}

@ARTICLE{Exoo2003,
  author = {Exoo,,G.},
  title = {A Euclidean Ramsey Problem},
  journal = {Discrete {\&} Computational Geometry},
  year = {2003},
  volume = {29},
  pages = {223-227},
  number = {2},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  ee = {http://link.springer.de/link/service/journals/00454/contents/02/0780/index.html}
}

@ARTICLE{Fawcett2006,
  author = {Fawcett,, T.},
  title = {An introduction to {ROC} analysis},
  journal = {Pattern Recogn. Lett.},
  year = {2006},
  volume = {27},
  pages = {861--874},
  number = {8},
  abstract = {Receiver operating characteristics (ROC) graphs are useful for organizing
	classifiers and visualizing their performance. ROC graphs are commonly
	used in medical decision making, and in recent years have been used
	increasingly in machine learning and data mining research. Although
	ROC graphs are apparently simple, there are some common misconceptions
	and pitfalls when using them in practice. The purpose of this article
	is to serve as an introduction to ROC graphs and as a guide for using
	them in research.},
  address = {New York, NY, USA},
  doi = {10.1016/j.patrec.2005.10.010},
  file = {Fawcett2006.pdf:Pattern Recognition/Fawcett2006.pdf:PDF},
  issn = {0167-8655},
  publisher = {Elsevier Science Inc.}
}

@BOOK{Fayyad2001,
  title = {Information Visualization in Data Mining and Knowledge Discovery},
  publisher = {Morgan Kaufmann Publishers Inc.},
  year = {2001},
  author = {Fayyad,, U. and Grinstein,, G. G. and Wierse,, A.},
  address = {San Francisco, CA, USA},
  isbn = {1558606890}
}

@ARTICLE{Fisher1936,
  author = {Fisher,, R. A.},
  title = {The Use of Multiple Measurements in Taxonomic Problems},
  journal = {Annals of Eugenics},
  year = {1936},
  volume = {7},
  pages = {179--188},
  file = {Fisher1936.pdf:Pattern Recognition/Fisher1936.pdf:PDF},
  owner = {castudil},
  timestamp = {2010.08.29}
}

@BOOK{Forbes2004,
  title = {Imitation of Life: How Biology Is Inspiring Computing},
  publisher = {MIT Press},
  year = {2004},
  author = {Forbes,, N.},
  owner = {César A. Astudillo},
  timestamp = {2009.10.04}
}

@ARTICLE{Forti2006,
  author = {Forti,, A. and Foresti,, G. L.},
  title = {{G}rowing {H}ierarchical {T}ree {SOM}: An unsupervised neural network
	with dynamic topology},
  journal = {Neural Networks},
  year = {2006},
  volume = {19},
  pages = {1568--1580},
  number = {10},
  abstract = {In this paper we introduce a tree structured self-organizing network,
	called the Growing Hierarchical Tree SOM (GHTSOM), that combines
	unsupervised learning with a dynamic topology for hierarchical classification
	of unlabelled data sets. The main feature of the proposed model is
	a SOM-like self-organizing process that allows the network to adapt
	the topology of each layer of the hierarchy to the characteristics
	of the training set. In particular the self-organization is obtained
	in two steps: the first one concerns the learning phase and is finalized
	with the creation of a tree of SOMs, while the second one is in regard
	to the clustering phase and provides the formation of classes for
	each level of the tree (hence self-organization not only for training
	but also for the creation of topological connections). As a result
	the network works without the need for user-defined parameters. Experimental
	results are proposed on both synthetic and real data sets.},
  address = {Oxford, UK, UK},
  doi = {http://dx.doi.org/10.1016/j.neunet.2006.02.009},
  file = {:SOM\\SOM-variants\\Forti2006.pdf:PDF},
  issn = {0893-6080},
  publisher = {Elsevier Science Ltd.}
}

@MISC{UCI,
  author = {Frank,,A. and Asuncion,,A.},
  title = {{UCI} Machine Learning Repository. \url{http://archive.ics.uci.edu/ml}},
  year = {2010},
  institution = {University of California, Irvine, School of Information and Computer
	Sciences}
}

@INCOLLECTION{Friedman1979,
  author = {Friedman,, J.},
  title = {A tree-structured approach to nonparametric multiple regression},
  booktitle = {Smoothing Techniques for Curve Estimation},
  publisher = {Springer Berlin / Heidelberg},
  year = {1979},
  editor = {Gasser, Th. and Rosenblatt, M.},
  volume = {757},
  series = {Lecture Notes in Mathematics},
  pages = {5-22},
  affiliation = {Stanford Linear Accelerator Center 94305 Stanford California USA 94305
	Stanford California USA},
  doi = {10.1007/BFb0098488},
  file = {Friedman1979.pdf:RHST/HyperplaneTree-classifiers/Friedman1979.pdf:PDF}
}

@ARTICLE{Friedman1977,
  author = {Friedman, J. H. and Bentley, J. L. and Finkel, R. A.},
  title = {An Algorithm for Finding Best Matches in Logarithmic Expected Time},
  journal = {ACM Trans. Math. Softw.},
  year = {1977},
  volume = {3},
  pages = {209--226},
  month = {September},
  acmid = {355745},
  address = {New York, NY, USA},
  doi = {10.1145/355744.355745},
  issn = {0098-3500},
  issue = {3},
  numpages = {18},
  publisher = {ACM}
}

@BOOK{Friedman1999,
  title = {Introduction to Pattern Recognition: Statistical, Structural, Neural
	and Fuzzy Logic Approaches},
  publisher = {Imperical College Press},
  year = {1999},
  author = {Friedman,, M. and Kandel,, A.}
}

@INPROCEEDINGS{Fritzke1995,
  author = {Fritzke,,B.},
  title = {A growing neural gas network learns topologies},
  booktitle = {Advances in Neural Information Processing Systems 7},
  year = {1995},
  editor = {G. Tesauro and D. S. Touretzky and T. K. Leen},
  pages = {625--632},
  address = {Cambridge MA},
  publisher = {MIT Press},
  file = {paper:References\\SOM\\SOM-variants\\non-tree-based\\Fritzke1995.PDF:PDF},
  url = {http://citeseer.ist.psu.edu/fritzke95growing.html}
}

@MISC{Fritzke1997,
  author = {Fritzke,,B.},
  title = {Some competitive learning methods. {L}ast accessed on may 15th, 2009.
	{A}vailable on the web at http://citeseer.ist.psu.edu/fritzke97some.html},
  year = {1997},
  text = {Fritzke, B. (1997). Some competitive learning methods. },
  url = {http://citeseer.ist.psu.edu/fritzke97some.html}
}

@ARTICLE{Fritzke1995a,
  author = {Fritzke,,B.},
  title = {Growing {G}rid - a self-organizing network with constant neighborhood
	range and adaptation strength},
  journal = {Neural Processing Letters},
  year = {1995},
  volume = {2},
  pages = {9--13},
  number = {5},
  file = {Fritzke1995a.pdf:SOM/SOM-variants/non-tree-based/Fritzke1995a.pdf:PDF},
  publisher = {D Facto Publishing}
}

@ARTICLE{Fritzke1994,
  author = {Fritzke,,B.},
  title = {{G}rowing {C}ell {S}tructures -- a self-organizing network for unsupervised
	and supervised learning},
  journal = {Neural Networks},
  year = {1994},
  volume = {7},
  pages = {1441-1460},
  number = {9},
  abstract = {We present a new self-organizing neural network model that has two
	variants. The first variant performs unsupervised learning and can
	be used for data visualization, clustering, and vector quantization.
	The main advantage over existing approaches (e.g., the Kohonen feature
	map) is the ability of the model to automatically find a suitable
	network structure and size. This is achieved through a controlled
	growth process that also includes occasional removal of units. The
	second variant of the model is a supervised learning method that
	results from the combination of the above-mentioned self-organizing
	network with the radial basis function (RBF) approach. In this model
	it is possible—in contrast to earlier approaches—to perform the positioning
	of the RBF units and the supervised training of the weights in parallel.
	Therefore, the current classification error can be used to determine
	where to insert new RBF units. This leads to small networks that
	generalize very well. Results on the two-spirals benchmark and a
	vowel classification problem are presented that are better than any
	results previously published.},
  file = {paper:C\:\\Users\\castudil\\Documents\\Research\\Cesar\\References\\SOM\\SOM-variants\\non-tree-based\\Fritzke1994.PDF:PDF},
  publisher = {Pergamon Press}
}

@INPROCEEDINGS{Fritzke1991,
  author = {Fritzke, B.},
  title = {Unsupervised clustering with growing cell structures},
  booktitle = {Neural Networks, 1991., IJCNN-91-Seattle International Joint Conference
	on},
  year = {1991},
  volume = {ii},
  pages = {531-536 vol.2},
  month = {Jul},
  abstract = {A neural network model is presented which is able to detect clusters
	of similar patterns. The patterns are n-dimensional real number vectors
	according to an unknown probability distribution P(X). By evaluating
	sample vectors according to P (X) a two-dimensional cell structure
	is gradually built up which models the distribution. Through removal
	of cells corresponding to areas with low probability density the
	structure is then split into several disconnected substructures.
	Each of them identifies one cluster of similar patterns. Not only
	is the number of clusters determined but also an approximation of
	the probability distribution inside each cluster. The accuracy of
	the cluster description increases linearly with the number of evaluated
	sample vectors},
  doi = {10.1109/IJCNN.1991.155390},
  file = {paper:References\\SOM\\SOM-variants\\non-tree-based\\Fritzke1991.PDF:PDF},
  keywords = {neural nets, pattern recognition, probabilityclusters of similar patterns,
	n-dimensional real number vectors, neural network model, probability
	distribution, two-dimensional cell structure}
}

@BOOK{Fukunaga1990,
  title = {Introduction to statistical pattern recognition (2nd ed.)},
  publisher = {Academic Press Professional, Inc.},
  year = {1990},
  author = {Fukunaga,,K.},
  address = {San Diego, CA, USA},
  isbn = {0-12-269851-7},
  owner = {castudil},
  timestamp = {2008.04.13}
}

@ARTICLE{Furukawa2009,
  author = {Tetsuo Furukawa},
  title = {SOM of SOMs},
  journal = {Neural Networks},
  year = {2009},
  volume = {22},
  pages = {463 - 478},
  number = {4},
  abstract = {This paper proposes an extension of the self-organizing map (SOM),
	in which the mapping objects themselves are self-organizing maps.
	Thus a ``SOM of SOMs'' is presented, which we refer to as a SOM2.
	A SOM2 has a hierarchical structure consisting of a single parent
	SOM and a set of child SOMs. Each child SOM is trained to represent
	the distribution of a data class in a manifold, while the parent
	SOM generates a self-organizing map of the group of manifolds modeled
	by the child SOMs. Thus a SOM2 is an architecture that organizes
	a product manifold represented as (child SOM) × (parent SOM). Such
	a product manifold is called a fiber bundle in terms of the topology.
	This extension of a SOM is easily generalized to any combination
	of SOM families, including cases of neural gas (NG) in which, for
	example, `` NG2 (=NG×NG) as an NG of NGs'' and ``NG×SOM as a SOM
	of NGs'' are possible. Furthermore, a SOM2 can be extended to a SOMn,
	such as SOM3=SOM×SOM×SOM defined as a ``SOM of SOM2''. In this paper,
	the algorithms for the SOM2 and its variations are introduced, and
	some simulation results are reported.},
  doi = {DOI: 10.1016/j.neunet.2009.01.012},
  file = {:SOM\\Furukawa2009.pdf:PDF},
  issn = {0893-6080},
  keywords = {Self-organizing map},
  url = {http://www.sciencedirect.com/science/article/B6T08-4VH8B1B-1/2/d059283839faab61260f542c48ca138b}
}

@ARTICLE{Gabrys2004,
  author = {Gabrys,,B. and Petrakieva,,L.},
  title = {Combining labelled and unlabelled data in the design of pattern classification
	systems},
  journal = {International Journal of Approximate Reasoning},
  year = {2004},
  volume = {35},
  pages = {251 - 273},
  number = {3},
  note = {Integration of Methods and Hybrid Systems},
  abstract = {There has been much interest in applying techniques that incorporate
	knowledge from unlabelled data into a supervised learning system
	but less effort has been made to compare the effectiveness of different
	approaches and to analyse the behaviour of the learning system when
	using different ratios of labelled to unlabelled data. In this paper
	various methods for learning from labelled and unlabelled data are
	first discussed and categorised into one of three major groups: pre-labelling,
	post-labelling and semi-supervised approaches. Their generalised
	formal description and extensive experimental analysis is then provided.
	The experimental results show that when supported by unlabelled samples
	much less labelled data is generally required to build a classifier
	without compromising the classification performance. If only a very
	limited amount of labelled data is available the results based on
	random selection of labelled samples show high variability and the
	performance of the final classifier is more dependent on how reliable
	the labelled data samples are rather than use of additional unlabelled
	data. In response to this finding three types of static (one-step)
	selection methods guided by a clustering information and various
	options of allocating a number of samples within clusters and their
	distributions have been proposed and analysed. A significant improvement
	compared to the random selection of the labelled samples have been
	observed when using these selective sampling techniques.},
  doi = {DOI: 10.1016/j.ijar.2003.08.005},
  file = {Gabrys2004.pdf:Pattern Recognition/Gabrys2004.pdf:PDF},
  issn = {0888-613X},
  keywords = {Combined learning methods}
}

@ARTICLE{Gallant1990,
  author = {Gallant, S. I.},
  title = {Perceptron-based learning algorithms},
  journal = {Neural Networks, IEEE Transactions on},
  year = {1990},
  volume = {1},
  pages = {179-191},
  number = {2},
  month = {Jun},
  doi = {10.1109/72.80230},
  file = {paper:References\\ANN\\Gallant1990.PDF:PDF},
  issn = {1045-9227},
  keywords = {learning systems, neural netsconnectionist expert systems, learning
	algorithms, machine learning, network scaling, pattern recognition,
	perceptron, single-cell models, training data},
  owner = {castudil},
  timestamp = {2009.12.07}
}

@BOOK{Gamma1994,
  title = {Design patterns: elements of reusable object-oriented software},
  publisher = {Addison-Wesley Professional},
  year = {1994},
  author = {Gamma,,E. and Helm,,R. and Johnson,,R. and Vlissides,,J.},
  owner = {castudil},
  timestamp = {2009.06.20}
}

@INCOLLECTION{Garcia2008,
  author = {García,, S. and Herrera,, F.},
  title = {Design of Experiments in Computational Intelligence: On the Use of
	Statistical Inference},
  booktitle = {Hybrid Artificial Intelligence Systems},
  publisher = {Springer Berlin / Heidelberg},
  year = {2008},
  editor = {Corchado, Emilio and Abraham, Ajith and Pedrycz, Witold},
  volume = {5271},
  series = {Lecture Notes in Computer Science},
  pages = {4-14},
  abstract = {The experimental analysis on the performance of a proposed method
	is a crucial and necessary task to carry out in a research. In this
	contribution we focus on the use of statistical inference for analyzing
	the results obtained in a design of experiment within the field of
	computational intelligence. We present some studies involving a set
	of techniques in different topics which can be used for doing a rigorous
	comparison among the algorithms studied in an experimental comparison.
	Particularly, we study whether the sample of results from multiple
	trials obtained by the run of several algorithms checks the required
	conditions for being analyzed through parametric tests. In most of
	the cases, the results indicate that the fulfillment of these conditions
	are problem dependent and indefinite, which justifies the need of
	using nonparametric statistics in the experimental analysis. We show
	a case study which illustrates the use of nonparametric tests and
	finally we give some guidelines on the use of nonparametric statistical
	tests.},
  affiliation = {University of Granada Department of Computer Science and Artificial
	Intelligence, E.T.S.I. Informática 18071 Granada Spain},
  doi = {10.1007/978-3-540-87656-4_3},
  file = {Garcia2008.pdf:Statistics/Garcia2008.pdf:PDF}
}

@ARTICLE{Gelfand1991,
  author = {Gelfand,, S. B. and Ravishankar,, C. S. and Delp,, E. J.},
  title = {An iterative growing and pruning algorithm for classification tree
	design},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {1991},
  volume = {13},
  pages = {163 -174},
  number = {2},
  month = feb,
  abstract = {A critical issue in classification tree design-obtaining right-sized
	trees, i.e. trees which neither underfit nor overfit the data-is
	addressed. Instead of stopping rules to halt partitioning, the approach
	of growing a large tree with pure terminal nodes and selectively
	pruning it back is used. A new efficient iterative method is proposed
	to grow and prune classification trees. This method divides the data
	sample into two subsets and iteratively grows a tree with one subset
	and prunes it with the other subset, successively interchanging the
	roles of the two subsets. The convergence and other properties of
	the algorithm are established. Theoretical and practical considerations
	suggest that the iterative free growing and pruning algorithm should
	perform better and require less computation than other widely used
	tree growing and pruning algorithms. Numerical results on a waveform
	recognition problem are presented to support this view},
  doi = {10.1109/34.67645},
  file = {Gelfand1991.pdf:RHST/HyperplaneTree-classifiers/Gelfand1991.pdf:PDF},
  issn = {0162-8828},
  keywords = {Bayes methods;classification tree design;estimation theory;iterative
	method;pattern recognition;pruning algorithm;right-sized trees;stopping
	rules;terminal nodes;waveform recognition problem;Bayes methods;estimation
	theory;iterative methods;pattern recognition;trees (mathematics);}
}

@INBOOK{Gentle1998,
  chapter = {Cholesky Factorization. §3.2.2},
  pages = {93--95},
  title = {Numerical Linear Algebra for Applications in Statistics},
  publisher = {Springer-Verlag},
  year = {1998},
  author = {Gentle, J. E.},
  address = {Berlin},
  owner = {castudillo},
  timestamp = {2011.01.18}
}

@BOOK{Gersho1991,
  title = {Vector quantization and signal compression},
  publisher = {Kluwer Academic Publishers},
  year = {1991},
  author = {Gersho,, A. and Gray,, R. M.},
  address = {Norwell, MA, USA},
  isbn = {0-7923-9181-0}
}

@ARTICLE{Goldberg2009,
  author = {Goldberg, X.},
  title = {Introduction to semi-supervised learning},
  journal = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
  year = {2009},
  volume = {6},
  pages = {1-116},
  abstract = {Semi-supervised learning is a learning paradigm concerned with the
	study of how computers and natural systems such as humans learn in
	the presence of both labeled and unlabeled data. Traditionally, learning
	has been studied either in the unsupervised paradigm (e.g., clustering,
	outlier detection) where all the data are unlabeled, or in the supervised
	paradigm (e.g., classification, regression) where all the data are
	labeled. The goal of semi-supervised learning is to understand how
	combining labeled and unlabeled data may change the learning behavior,
	and design algorithms that take advantage of such a combination.
	Semi-supervised learning is of great interest in machine learning
	and data mining because it can use readily available unlabeled data
	to improve supervised learning tasks when the labeled data are scarce
	or expensive. Semi-supervised learning also shows potential as a
	quantitative tool to understand human category learning, where most
	of the input is self-evidently unlabeled. In this introductory book,
	we present some popular semi-supervised learning models, including
	self-training, mixture models, co-training and multiview learning,
	graph-based methods, and semi-supervised support vector machines.
	For each model, we discuss its basic mathematical formulation. The
	success of semi-supervised learning depends critically on some underlying
	assumptions. We emphasize the assumptions made by each model and
	give counterexamples when appropriate to demonstrate the limitations
	of the different models. In addition, we discuss semi-supervised
	learning for cognitive psychology. Finally, we give a computational
	learning theoretic perspective on semi-supervised learning, and we
	conclude the book with a brief discussion of open questions in the
	field. Copyright; 2009 by Morgan & Claypool.},
  author_keywords = {Cluster-then-label; Co-training; Expectation maximization (EM); Gaussian
	mixture model; Harmonic function; Label propagation; Manifold regularization;
	Mincut; Multiview learning; Self-training; Semi-supervised learning;
	Transductive learning},
  comment = {http://www.scopus.com/inward/record.url?eid=2-s2.0-67650272548&partnerID=40&md5=238f047fb2be7d5499421645b7c35eae},
  doi = {10.2200/S00196ED1V01Y200906AIM006},
  keywords = {Cluster-then-label; Co-training; Expectation maximization (EM); Gaussian
	mixture model; Harmonic function; Label propagation; Manifold regularization;
	Mincut; Multiview learning; Self-training; Semi-supervised learning;
	Transductive learning, Cobalt; Cobalt compounds; Communication channels
	(information theory); Computer science; Education; Fourier series;
	Harmonic analysis; Image segmentation; Labels; Maximum principle;
	Mining; Mixtures; Object recognition; SCADA systems; Supervised learning,
	Learning algorithms}
}

@ARTICLE{Gonnet1981,
  author = {Gonnet,,G. H. and Munro,,J. I. and Suwanda,,H.},
  title = {Exegesis of self-organizing linear search},
  journal = {SIAM J. Comput.},
  year = {1981},
  volume = {10},
  pages = {613-637},
  owner = {castudil},
  timestamp = {2009.02.17}
}

@ARTICLE{Gordon1978,
  author = {Gordon,, L. and Olshen,, R. A.},
  title = {Asymptotically Efficient Solutions to the Classification Problem},
  journal = {Annals of Statistics},
  year = {1978},
  volume = {6},
  pages = {515--533},
  file = {Gordon1978.pdf:RHST/HyperplaneTree-classifiers/Gordon1978.pdf:PDF},
  owner = {castudillo},
  timestamp = {2010.12.01}
}

@INBOOK{Gorgonio2010,
  chapter = {PartSOM: A Framework for Distributed Data Clustering Using SOM and
	K-Means},
  pages = {43--62},
  title = {Self-Organizing Maps},
  publisher = {In-Tech},
  year = {2010},
  editor = {Matsopoulos,, G. K.},
  author = {Gorgônio,, F. L. and Costa,, J. A. F.},
  file = {Gorgonio2010.pdf:fast-BMU/Gorgonio2010.pdf:PDF},
  owner = {castudillo},
  timestamp = {2011.02.04}
}

@ARTICLE{Gray1984,
  author = {Gray, R.},
  title = {Vector quantization},
  journal = {ASSP Magazine, IEEE},
  year = {1984},
  volume = {1},
  pages = { 4 - 29},
  number = {2},
  month = apr,
  abstract = { A vector quantizer is a system for mapping a sequence of continuous
	or discrete vectors into a digital sequence suitable for communication
	over or storage in a digital channel. The goal of such a system is
	data compression: to reduce the bit rate so as to minimize communication
	channel capacity or digital storage memory requirements while maintaining
	the necessary fidelity of the data. The mapping for each vector may
	or may not have memory in the sense of depending on past actions
	of the coder, just as in well established scalar techniques such
	as PCM, which has no memory, and predictive quantization, which does.
	Even though information theory implies that one can always obtain
	better performance by coding vectors instead of scalars, scalar quantizers
	have remained by far the most common data compression system because
	of their simplicity and good performance when the communication rate
	is sufficiently large. In addition, relatively few design techniques
	have existed for vector quantizers. During the past few years several
	design algorithms have been developed for a variety of vector quantizers
	and the performance of these codes has been studied for speech waveforms,
	speech linear predictive parameter vectors, images, and several simulated
	random processes. It is the purpose of this article to survey some
	of these design techniques and their applications.},
  doi = {10.1109/MASSP.1984.1162229},
  file = {Gray1984.pdf:fast-BMU/Gray1984.pdf:PDF},
  issn = {0740-7467}
}

@INBOOK{Greene2008,
  chapter = {Unsupervised Learning and Clustering},
  pages = {51--90},
  title = {Machine Learning Techniques for Multimedia: Case Studies on Organization
	and Retrieval (Cognitive Technologies)},
  publisher = {Springer Berlin Heidelberg},
  year = {2008},
  author = {Greene,, D. and Cunningham,,P. and Mayer,, R.},
  abstract = {Unsupervised learning is very important in the processing of multimedia
	content as clustering or partitioning of data in the absence of class
	labels is often a requirement. This chapter begins with a review
	of the classic clustering techniques of k-means clustering and hierarchical
	clustering. Modern advances in clustering are covered with an analysis
	of kernel-based clustering and spectral clustering. One of the most
	popular unsupervised learning techniques for processing multimedia
	content is the self-organizing map, so a review of self-organizing
	maps and variants is presented in this chapter. The absence of class
	labels in unsupervised learning makes the question of evaluation
	and cluster quality assessment more complicated than in supervised
	learning. So this chapter also includes a comprehensive analysis
	of cluster validity assessment techniques.},
  file = {Chapter:References\\SOM\\SOM-variants\\Cord2008.PDF:PDF},
  journal = {Machine Learning Techniques for Multimedia},
  owner = {castudil},
  timestamp = {2010.01.15},
  url = {http://dx.doi.org/10.1007/978-3-540-75171-7_3}
}

@INPROCEEDINGS{Guan2006,
  author = {Guan,,L.},
  title = {Self-Organizing Trees and Forests: A Powerful Tool in Pattern Clustering
	and Recognition},
  booktitle = {Image Analysis and Recognition, Third International Conference, ICIAR
	2006, P{\'o}voa de Varzim, Portugal, September 18-20, 2006, Proceedings,
	Part I},
  year = {2006},
  pages = {I: 1-14},
  abstract = {As the fruit of the Information Age comes to bare, the question of
	how such information, especially visual information, might be effectively
	harvested, archived and analyzed, remains a monumental challenge
	facing today’s research community. The processing of such information,
	however, is often fraught with the need for conceptual interpretation:
	a relatively simple task for humans, yet arduous for computers. In
	attempting to handle oppressive volumes of visual information becoming
	readily accessible within consumer and industrial sectors, some level
	of automation remains a highly desired goal. To achieve such a goal
	requires computational systems that exhibit some degree of intelligence
	in terms of being able to formulate their own models of the data
	in question with little or no user intervention – a process popularly
	referred to as Pattern Clustering or Unsupervised Pattern Classification.
	One powerful tool in pattern clustering is the computational technologies
	based on principles of Self-Organization. In this talk, we explore
	a new family of computing architectures that have a basis in self
	organization, yet are somewhat free from many of the constraints
	typical of other well known self-organizing architectures. The basic
	processing unit in the family is known as the Self-Organizing Tree
	Map (SOTM). We will look at how this model has evolved since its
	inception in 1995, how it has inspired new models, and how it is
	being applied to complex pattern clustering problems in image processing
	and retrieval, and three dimensional data analysis and visualization.},
  bibsource = {http://www.visionbib.com/bibliography/pattern615.html#TT44752},
  file = {paper:SOM\\SOM-variants\\Guan2006.pdf:PDF}
}

@INPROCEEDINGS{Gustafson1980,
  author = {Gustafson,, D. E. and Gelfand, S. and Mitter,, S. K.},
  title = {A nonparametric multiclass partitioning method for classification},
  booktitle = {Proceedings of the Fifth International Conference on Pattern Recognition},
  year = {1980},
  pages = {654--659},
  address = {Miami, FL},
  organization = {International Association for Pattern Recognition},
  owner = {castudillo},
  timestamp = {2010.12.01}
}

@ARTICLE{Hall2003,
  author = {Hall, M.A. and Holmes, G.},
  title = {Benchmarking attribute selection techniques for discrete class data
	mining},
  journal = {Knowledge and Data Engineering, IEEE Transactions on},
  year = {2003},
  volume = {15},
  pages = {1437--1447},
  number = {6},
  month = {nov.-dec.},
  abstract = {Data engineering is generally considered to be a central issue in
	the development of data mining applications. The success of many
	learning schemes, in their attempts to construct models of data,
	hinges on the reliable identification of a small set of highly predictive
	attributes. The inclusion of irrelevant, redundant, and noisy attributes
	in the model building process phase can result in poor predictive
	performance and increased computation. Attribute selection generally
	involves a combination of search and attribute utility estimation
	plus evaluation with respect to specific learning schemes. This leads
	to a large number of possible permutations and has led to a situation
	where very few benchmark studies have been conducted. This paper
	presents a benchmark comparison of several attribute selection methods
	for supervised classification. All the methods produce an attribute
	ranking, a useful devise for isolating the individual merit of an
	attribute. Attribute selection is achieved by cross-validating the
	attribute rankings with respect to a classification learner to find
	the best attributes. Results are reported for a selection of standard
	data sets and two diverse learning schemes C4.5 and naive Bayes.},
  doi = {10.1109/TKDE.2003.1245283},
  issn = {1041-4347},
  keywords = { C4.5 learning scheme; attribute ranking; attribute selection technique
	benchmarking; attribute utility estimation; classification learner;
	data engineering; discrete class data mining; naive Bayes learning
	scheme; predictive attribute identification; search; supervised classification;
	Bayes methods; data mining; feature extraction; learning (artificial
	intelligence); pattern classification;}
}

@MASTERSTHESIS{Handl2003,
  author = {Handl,,J.},
  title = {{Ant-based methods for tasks of clustering and topographic mapping:
	extensions, analysis and comparison with alternative methods. Masters
	thesis}},
  school = {Chair of Artificial Intelligence, University of Erlangen-Nuremberg},
  year = {2003},
  address = {Germany},
  month = {November}
}

@BOOK{Haykin2008,
  title = {Neural Networks and Learning Machines},
  publisher = {Prentice Hall},
  year = {2008},
  author = {Haykin,,S.},
  pages = {936},
  edition = {3rd Edition},
  owner = {castudil},
  timestamp = {2009.11.15}
}

@BOOK{Hebb1949,
  title = {The Organization of Behaviour},
  publisher = {John Wiley \& Sons},
  year = {1949},
  author = {Hebb,, D. O.},
  address = {New York},
  owner = {castudil},
  timestamp = {2009.12.09}
}

@ARTICLE{Hester1985,
  author = {Hester,, J. H. and Hirschberg,, D. S.},
  title = {Self-organizing linear search},
  journal = {ACM Comput. Surv.},
  year = {1985},
  volume = {17},
  pages = {295--311},
  number = {3},
  address = {New York, NY, USA},
  doi = {http://doi.acm.org/10.1145/5505.5507},
  issn = {0360-0300},
  publisher = {ACM}
}

@INPROCEEDINGS{Horton1996,
  author = {Horton,,P. and Nakai,,K.},
  title = {A probabilistic classification system for predicting the cellular
	localization sites of proteins},
  booktitle = {Proceedings of the 4th International Conference on Intelligent Systems
	for Molecular Biology (ISMB96)},
  year = {1996},
  volume = {4},
  pages = {109--115},
  abstract = {We have defined a simple model of classification which combines human
	provided
	
	 expert knowledge with probabilistic reasoning. We have developed
	software to
	
	 implement this model and have applied it to the problem of classifying
	proteins
	
	 into their various cellular localization sites based on their amino
	acid
	
	 sequences. Since our system requires no hand tuning to learn training
	data, we
	
	 can now evaluate the prediction accuracy of protein localization
	sites by a more 
	
	 objective cross-validation method than earlier studies using production
	rule type
	
	 expert systems. 336 E. coli proteins were classified into 8 classes
	with an
	
	 accuracy of 81\% while 1484 yeast proteins were classified into 10
	classes with an
	
	 accuracy of 55\%. Additionally we report empirical results using
	three different
	
	 strategies for handling continuously valued variables in our probabilistic
	
	 reasoning system.},
  file = {Horton1996.pdf:UCI/Horton1996.pdf:PDF}
}

@ARTICLE{Huang1998,
  author = {Huang,, G. and Babri,, H. A. and Li, H.},
  title = {Ordering of Self-Organizing Maps in Multi-Dimensional Cases},
  journal = {Neural Computation},
  year = {1998},
  volume = {10},
  pages = {19-24},
  file = {paper:References/SOM/som-related/Huang1998.pdf:PDF},
  owner = {castudil},
  timestamp = {2009.10.29}
}

@INPROCEEDINGS{Hussin2004,
  author = {Hussin,,Mahmoud F. and Kamel,,Mohamed S. and Nagi,,Magdy H.},
  title = {An Efficient Two-Level SOMART Document Clustering Through Dimensionality
	Reduction},
  booktitle = {11th International Conference Neural Information Processing},
  year = {2004},
  series = {Neural Information Processing},
  pages = {158-165},
  address = {Calcutta, India},
  month = {November},
  file = {paper:References/SOM/som-related/Hussin2004.pdf:PDF},
  owner = {castudil},
  timestamp = {2009.10.19}
}

@BOOK{Jain1988,
  title = {Algorithms for clustering data},
  publisher = {Prentice-Hall, Inc.},
  year = {1988},
  author = {Jain,,A. K. and Dubes,,R. C.},
  address = {Upper Saddle River, NJ, USA},
  citeulike-article-id = {165423},
  isbn = {013022278X},
  keywords = {algorithm cluster clustering data data-mining methods mining pattern-recognition
	reference search},
  owner = {castudil},
  timestamp = {2008.04.14},
  url = {http://portal.acm.org/citation.cfm?id=42779}
}

@ARTICLE{Jain2000,
  author = {Jain, A. K. and Duin, R. P. W. and Mao,,J.},
  title = {Statistical pattern recognition: a review},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2000},
  volume = {22},
  pages = {4 -37},
  number = {1},
  month = {jan},
  doi = {10.1109/34.824819},
  file = {Jain2000.pdf:Pattern Recognition\\Jain2000.pdf:PDF},
  issn = {0162-8828},
  keywords = {classifier design;cluster analysis;complex patterns;cursive handwriting
	recognition;data mining;multimedia data retrieval;neural network
	techniques;pattern classes;pattern representation;performance evaluation;sensing
	environment;statistical learning theory;statistical pattern recognition;supervised
	classification;unsupervised classification;web searching;Bayes methods;decision
	theory;learning (artificial intelligence);neural nets;parameter estimation;pattern
	recognition;}
}

@ARTICLE{Jain1999,
  author = {Jain,,A. K. and Murty,,M. N. and Flynn,,P. J.},
  title = {Data clustering: a review},
  journal = {ACM Computing Surveys},
  year = {1999},
  volume = {31},
  pages = {264--323},
  number = {3},
  file = {paper:References/SOM/som-related/Jain1999.pdf:PDF},
  url = {citeseer.ist.psu.edu/jain99data.html}
}

@ARTICLE{Jin2004,
  author = {Jin,, H. and Shum,,W.-H. and Leung K.-S.and Wong M.-L.},
  title = {Expanding Self-Organizing Map for data visualization and cluster
	analysis},
  journal = {Information Sciences},
  year = {2004},
  volume = {163},
  pages = {157 - 173},
  number = {1-3},
  note = {Soft Computing Data Mining},
  abstract = {The Self-Organizing Map (SOM) is a powerful tool in the exploratory
	phase of data mining. It is capable of projecting high-dimensional
	data onto a regular, usually 2-dimensional grid of neurons with good
	neighborhood preservation between two spaces. However, due to the
	dimensional conflict, the neighborhood preservation cannot always
	lead to perfect topology preservation. In this paper, we establish
	an Expanding SOM (ESOM) to preserve better topology between the two
	spaces. Besides the neighborhood relationship, our ESOM can detect
	and preserve an ordering relationship using an expanding mechanism.
	The computational complexity of the ESOM is comparable with that
	of the SOM. Our experiment results demonstrate that the ESOM constructs
	better mappings than the classic SOM, especially, in terms of the
	topological error. Furthermore, clustering results generated by the
	ESOM are more accurate than those obtained by the SOM.},
  doi = {10.1016/j.ins.2003.03.020},
  file = {Jin2004.pdf:SOM\\applications\\Jin2004.pdf:PDF},
  issn = {0020-0255},
  keywords = {Self-Organizing Map},
  url = {http://www.sciencedirect.com/science/article/B6V0C-4B3JHTS-2/2/284cd52ab1f675d465e514f48ffaa7eb}
}

@BOOK{Jolliffe1986,
  title = {Principal Component Analysis},
  publisher = {Springer-Verlag},
  year = {1986},
  author = {Jolliffe,,I. T.},
  owner = {castudil},
  timestamp = {2008.07.16}
}

@BOOK{Jones2007,
  title = {Artificial Intelligence: A Systems Approach},
  publisher = {Infinity Science Press},
  year = {2007},
  author = {Jones,, M. Tim},
  isbn = {0977858235},
  owner = {César A. Astudillo},
  timestamp = {2009.09.23}
}

@ARTICLE{Kaelbling1996,
  author = {Kaelbling,,Leslie Pack and Littman,,Michael L. and Moore,,Andrew
	W.},
  title = {Reinforcement Learning: A Survey},
  journal = {Journal of Artificial intelligence Research},
  year = {1996},
  volume = {4},
  pages = {237--285},
  file = {paper:References/SOM/som-related/Kaelbling1996.pdf:PDF}
}

@INBOOK{Kaplan2004,
  chapter = {31: Persistent Data Structures},
  pages = {31.1 -- 31.26},
  title = {Handbook of Data Structures and Applications},
  publisher = {Chapman and Hall/CRC},
  year = {2004},
  author = {Kaplan,,H.},
  doi = {10.1201/9781420035179.ch31},
  url = {http://www.crcnetbase.com/doi/abs/10.1201/9781420035179.ch31}
}

@INPROCEEDINGS{Kasai2009,
  author = {Kasai, W. and Tobe, Y. and Hasegawa, O.},
  title = {A Fast BMU Search for Support Vector Machine},
  booktitle = {ICANN '09: Proceedings of the 19th International Conference on Artificial
	Neural Networks},
  year = {2009},
  pages = {864--873},
  address = {Berlin, Heidelberg},
  publisher = {Springer-Verlag},
  doi = {10.1007/978-3-642-04274-4_89},
  file = {Kasai2009.pdf:fast-BMU/Kasai2009.pdf:PDF},
  isbn = {978-3-642-04273-7},
  location = {Limassol, Cyprus}
}

@ARTICLE{Kaski1998,
  author = {Samuel Kaski and Jari Kangas and Teuvo Kohonen},
  title = {Bibliography of Self-Organizing Map ({SOM}) Papers: 1981--1997},
  journal = {Neural Computing Surveys},
  year = {1998},
  volume = {1},
  pages = {102--350}
}

@ARTICLE{Katsavounidis1996,
  author = {Katsavounidis,, I. and Kuo,, C.-C.J. and Zhang,,Z.},
  title = {Fast tree-structured nearest neighbor encoding for vector quantization},
  journal = {Image Processing, IEEE Transactions on},
  year = {1996},
  volume = {5},
  pages = {398 -404},
  number = {2},
  month = feb,
  abstract = {This work examines the nearest neighbor encoding problem with an unstructured
	codebook of arbitrary size and vector dimension. We propose a new
	tree-structured nearest neighbor encoding method that significantly
	reduces the complexity of the full-search method without any performance
	degradation in terms of distortion. Our method consists of efficient
	algorithms for constructing a binary tree for the codebook and nearest
	neighbor encoding by using this tree. Numerical experiments are given
	to demonstrate the performance of the proposed method},
  doi = {10.1109/83.480778},
  file = {Katsavounidis1996.pdf:fast-BMU/Katsavounidis1996.pdf:PDF},
  issn = {1057-7149},
  keywords = {algorithms;binary tree;codebook size;complexity reduction;fast tree-structured
	nearest neighbor encoding;full-search method;numerical experiments;performance;unstructured
	codebook;vector dimension;vector quantization;computational complexity;tree
	searching;vector quantisation;}
}

@ARTICLE{Kirkpatrick1983,
  author = {Kirkpatrick,,Scott and Gelatt Jr.,,D. and Vecchi,,Mario P.},
  title = {Optimization by Simmulated Annealing},
  journal = {Science},
  year = {1983},
  volume = {220},
  pages = {671-680},
  number = {4598},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  file = {paper:References/Metaheuristics/Kirkpatrick1983.pdf:PDF}
}

@INPROCEEDINGS{Kiviluoto1996,
  author = {Kiviluoto, K.},
  title = {Topology preservation in self-organizing maps},
  booktitle = {Proceedings of International Conference on Neural Networks, ICNN'96},
  year = {1996},
  editor = {P. IEEE Neural Networks Council},
  volume = {1},
  pages = {294-299},
  address = {New Jersey, USA},
  file = {paper:/References/SOM/som-related/Kiviluoto1996.pdf:PDF},
  owner = {castudil},
  timestamp = {2009.08.19},
  url = {http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=548907}
}

@ARTICLE{Klee1972,
  author = {Klee,,V.L. and Minty,,G.J.},
  title = {How good is the simplex algorithm?},
  journal = {Inequalities},
  year = {1972},
  volume = {III},
  pages = {159--175},
  eid = {O. Shisha},
  owner = {castudil},
  timestamp = {2009.11.05}
}

@BOOK{Knuth1998,
  title = {The art of computer programming, volume 3: (2nd ed.) sorting and
	searching},
  publisher = {Addison Wesley Longman Publishing Co., Inc.},
  year = {1998},
  author = {Knuth,, D. E.},
  address = {Redwood City, CA, USA},
  isbn = {0-201-89685-0}
}

@INPROCEEDINGS{Kohavi1995,
  author = {Kohavi,,R.},
  title = {A Study of Cross-Validation and Bootstrap for Accuracy Estimation
	and Model Selection},
  booktitle = {Proceedings of the Fourteenth International Joint Conference on Artificial
	Intelligence},
  year = {1995},
  volume = {2},
  number = {12},
  pages = {1137--1143},
  publisher = {Morgan Kaufmann},
  file = {Kohavi1995.pdf:Pattern Recognition\\Kohavi1995.pdf:PDF}
}

@ARTICLE{Kohavi1998,
  author = {Kohavi,,R. and Provost,, F.},
  title = {Glossary of Terms},
  journal = {Machine Learning},
  year = {1998},
  volume = {30},
  pages = {271-274},
  doi = {10.1023/A:1017181826899},
  file = {Kohavi1998.pdf:Pattern Recognition/Kohavi1998.pdf:PDF},
  issn = {0885-6125},
  issue = {2},
  keyword = {Computer Science},
  publisher = {Springer Netherlands}
}

@BOOK{Kohonen1995,
  title = {Self-Organizing Maps},
  publisher = {Springer-Verlag New York, Inc.},
  year = {1995},
  author = {Kohonen,,T.},
  address = {Secaucus, NJ, USA},
  isbn = {3540679219}
}

@ARTICLE{Kohonen1990,
  author = {Kohonen,,T.},
  title = {The self-organizing map},
  journal = {Proceedings of the IEEE},
  year = {1990},
  volume = {78},
  pages = {1464-1480},
  number = {9},
  doi = {10.1109/5.58325},
  file = {paper:References/SOM/Kohonen1990.pdf:PDF},
  issn = {0018-9219},
  keywords = {learning systems, neural nets, self-adjusting systems, speech recognitionclustering,
	competitive learning, learning vector, neural networks, self-organizing
	map, semantic maps, speech recognition}
}

@INPROCEEDINGS{Kohonen1990a,
  author = {Kohonen,, T.},
  title = {Improved versions of learning vector quantization},
  booktitle = {Neural Networks, 1990., 1990 {IJCNN} International Joint Conference
	on},
  year = {1990},
  volume = {1},
  pages = {545--550},
  month = {June},
  abstract = {The author introduces a variant of (supervised) learning vector quantization
	(LVQ) and discusses practical problems associated with the application
	of the algorithms. The LVQ algorithms work explicitly in the input
	domain of the primary observation vectors, and their purpose is to
	approximate the theoretical Bayes decision borders using piecewise
	linear decision surfaces. This is done by purported optimal placement
	of the class codebook vectors in signal space. As the classification
	decision is based on the nearest-neighbor selection among the codebook
	vectors, its computation is very fast. It has turned out that the
	differences between the presented algorithms in regard to the remaining
	discretization error are not significant, and thus the choice of
	the algorithm may be based on secondary arguments, such as stability
	in learning, in which respect the variant introduced (LVQ2.1) seems
	to be superior to the others. A comparative study of several methods
	applied to speech recognition is included},
  doi = {10.1109/IJCNN.1990.137622},
  file = {Kohonen1990a.pdf:Pattern Recognition/Kohonen1990a.pdf:PDF},
  keywords = {LVQ2.1;class codebook vectors;classification decision;codebook vectors
	initialization;discretization error;learning stability;learning vector
	quantization;nearest-neighbor selection;pattern recognition;piecewise
	linear decision surfaces;speech recognition;supervised learning;theoretical
	Bayes decision borders;data compression;encoding;learning systems;neural
	nets;pattern recognition;speech recognition;}
}

@ARTICLE{Kohonen1982,
  author = {Kohonen,,T.},
  title = {Self-organized formation of topologically correct feature maps},
  journal = {Biological Cybernetics},
  year = {1982},
  volume = {43},
  pages = {59--69},
  keywords = {learning systems, neural nets, self-adjusting systems, speech recognitionclustering,
	competitive learning, learning vector, neural networks, self-organizing
	map, semantic maps, speech recognition}
}

@INCOLLECTION{Kohonen2009,
  author = {Kohonen,, T. and Nieminen,, I. and Honkela,, T.},
  title = {On the Quantization Error in SOM vs. VQ: A Critical and Systematic
	Study},
  booktitle = {Advances in Self-Organizing Maps},
  publisher = {Springer Berlin / Heidelberg},
  year = {2009},
  editor = {Príncipe, José and Miikkulainen, Risto},
  volume = {5629},
  series = {Lecture Notes in Computer Science},
  pages = {133-144},
  abstract = {The self-organizing map (SOM) is related to the classical vector quantization
	(VQ). Like in the VQ, the SOM represents a distribution of input
	data vectors using a finite set of models. In both methods, the quantization
	error (QE) of an input vector can be expressed, e.g., as the Euclidean
	norm of the difference of the input vector and the best-matching
	model. Since the models are usually optimized in the VQ so that the
	sum of the squared QEs is minimized for the given set of training
	vectors, a common notion is that it will be impossible to find models
	that produce a smaller rms QE. Therefore it has come as a surprise
	that in some cases the rms QE of a SOM can be smaller than that of
	a VQ with the same number of models and the same input data. This
	effect may manifest itself if the number of training vectors per
	model is on the order of small integers and the testing is made with
	an independent set of test vectors. An explanation seems to ensue
	from statistics. Each model vector in the VQ is determined as the
	average of those training vectors that are mapped into the same Voronoi
	domain as the model vector. On the contrary, each model vector of
	the SOM is determined as a weighted average of all of those training
	vectors that are mapped into the topological neighborhood around
	the corresponding model. The number of training vectors mapped into
	the neighborhood of a SOM model is generally much larger than that
	mapped into a Voronoi domain around a model in the VQ. Since the
	SOM model vectors are then determined with a significantly higher
	statistical accuracy, the Voronoi domains of the SOM are significantly
	more regular, and the resulting rms QE may then be smaller than in
	the VQ. However, the effective dimensionality of the vectors must
	also be sufficiently high.},
  affiliation = {Helsinki University of Technology, Centre of Adaptive Informatics
	P.O. Box 5400 02015 HUT Finland},
  doi = {10.1007/978-3-642-02397-2_16},
  file = {Kohonen2009.pdf:VQ/Kohonen2009.pdf:PDF}
}

@ARTICLE{Koikkalainen1990,
  author = {Koikkalainen,,P. and Oja,,E.},
  title = {Self-organizing hierarchical feature maps},
  journal = {IJCNN International Joint Conference on Neural Networks},
  year = {1990},
  volume = {2},
  pages = {279-284},
  month = {June},
  abstract = {The topological feature map (TFM) algorithm introduced by T. Kohenen
	(1982) implements two important properties: a vector quantization
	(VQ) and a topology-preserving mapping. A tree-structured TFM (TSTFM)
	is presented as a computationally inexpensive alternative to the
	TFM algorithm. The computational complexity of the TSTFM is O (log
	N) rather than O(N) for the TFM. In addition, the TSTFM has some
	new properties that prove to be useful for VQ and in the context
	of visual perception: increased performance in VQ compared to the
	tree-structured VQ of A. Buzo et al. (1980) and hierarchical mapping
	of code vectors},
  doi = {10.1109/IJCNN.1990.137727},
  file = {Koikkalainen1990.pdf:SOM\\SOM-variants\\Koikkalainen1990.pdf:PDF},
  keywords = {computational complexity, neural nets, self-adjusting systemsTFM algorithm,
	computational complexity, neural nets, self-organising hierarchical
	feature maps, topological feature map, topology-preserving mapping,
	tree-structured TFM, unsupervised learning, vector quantization,
	visual perception}
}

@ARTICLE{Kolmogorov1957,
  author = {Kolmogorov,, A. N.},
  title = {On the representation of continuous functions of several variables
	by superpositions of continuous functions of one variable and addition},
  journal = {Doklady Akademiia Nauk SSSR},
  year = {1957},
  volume = {114},
  pages = {953-956},
  number = {5},
  owner = {castudil},
  timestamp = {2009.12.09}
}

@ARTICLE{Kotsiantis2006,
  author = {Kotsiantis,, S. and Zaharakis,, I. and Pintelas,, P.},
  title = {Machine learning: a review of classification and combining techniques},
  journal = {Artificial Intelligence Review},
  year = {2006},
  volume = {26},
  pages = {159--190},
  note = {10.1007/s10462-007-9052-3},
  abstract = {Supervised classification is one of the tasks most frequently carried
	out by so-called Intelligent Systems. Thus, a large number of techniques
	have been developed based on Artificial Intelligence (Logic-based
	techniques, Perceptron-based techniques) and Statistics (Bayesian
	Networks, Instance-based techniques). The goal of supervised learning
	is to build a concise model of the distribution of class labels in
	terms of predictor features. The resulting classifier is then used
	to assign class labels to the testing instances where the values
	of the predictor features are known, but the value of the class label
	is unknown. This paper describes various classification algorithms
	and the recent attempt for improving classification accuracy—ensembles
	of classifiers.},
  affiliation = {University of Peloponnese Department of Computer Science and Technology
	Peloponnese Greece},
  doi = {10.1007/s10462-007-9052-3},
  file = {Kotsiantis2006.pdf:Pattern Recognition/Kotsiantis2006.pdf:PDF},
  issn = {0269-2821},
  issue = {3},
  keyword = {Computer Science},
  publisher = {Springer Netherlands}
}

@ARTICLE{Kotsiantis2007,
  author = {Kotsiantis,,Sotiris B.},
  title = {Supervised Machine Learning: A Review of Classification Techniques},
  journal = {Informatica (Slovenia)},
  year = {2007},
  volume = {31},
  pages = {249-268},
  number = {3},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  ee = {http://www.informatica.si/PDF/31-3/11_Kotsiantis\%20-\%20Supervised\%20Machine\%20Learning\%20-\%20A\%20Review\%20of...pdf},
  file = {paper:References/SOM/som-related/Kotsiantis2007.pdf:PDF}
}

@ARTICLE{Kubota2005,
  author = {Kubota,,N.},
  title = {Computational intelligence for structured learning of a partner robot
	based on imitation},
  journal = {Information Sciences},
  year = {2005},
  volume = {171},
  pages = {403 - 429},
  number = {4},
  note = {Intelligent Embedded Agents},
  abstract = {Imitation is a powerful tool for gestural interaction between children
	and for teaching behaviors to children by parent. Furthermore, others'
	action can be a hint for acquiring a new behavior that might not
	be the same as the original action. The importance is how to map
	or represent others' action into new one in the internal state space.
	A good instructor can teach an action to a learner by understanding
	the mapping or imitating method of the learner. This indicates a
	robot also can acquire various behaviors using interactive learning
	based on imitation. This paper proposes structured learning for a
	partner robot based on the interactive teaching mechanism. The proposed
	method is composed of a spiking neural network, self-organizing map,
	steady-state genetic algorithm, and softmax action selection. Furthermore,
	we discuss the interactive learning of a human and a partner robot
	based on the proposed method through experiment results.},
  doi = {10.1016/j.ins.2004.09.012},
  file = {Kubota2005.pdf:SOM\\applications\\Kubota2005.pdf:PDF},
  issn = {0020-0255},
  keywords = {Partner robots},
  url = {http://www.sciencedirect.com/science/article/B6V0C-4DWVR94-2/2/7a0b6de7fe5f5824534b03e94868c067}
}

@ARTICLE{Kurzynski1983,
  author = {Marek W. Kurzynski},
  title = {The optimal strategy of a tree classifier},
  journal = {Pattern Recognition},
  year = {1983},
  volume = {16},
  pages = {81--87},
  number = {1},
  abstract = {This paper deals with the decision rules of a tree classifier for
	performing the classification at each nonterminal node, under the
	assumption of complete probabilistic information. For given tree
	structure and feature subsets to be used, the optimal decision rules
	(strategy) are derived which minimize the overall probability of
	misclassification. The primary result is illustrated by an example.},
  doi = {10.1016/0031-3203(83)90011-0},
  file = {Kurzynski1983.pdf:RHST/HyperplaneTree-classifiers/Kurzynski1983.pdf:PDF},
  issn = {0031-3203},
  keywords = {Pattern recognition}
}

@ARTICLE{Lagus2004,
  author = {Lagus,,K. and Kaski,,S. and Kohonen,,T.},
  title = {Mining massive document collections by the WEBSOM method},
  journal = {Information Sciences},
  year = {2004},
  volume = {163},
  pages = {135 - 156},
  number = {1-3},
  note = {Soft Computing Data Mining},
  abstract = {A viable alternative to the traditional text-mining methods is the
	WEBSOM, a software system based on the Self-Organizing Map (SOM)
	principle. Prior to the searching or browsing operations, this method
	orders a collection of textual items, say, documents according to
	their contents, and maps them onto a regular two-dimensional array
	of map units. Documents that are similar on the basis of their whole
	contents will be mapped to the same or neighboring map units, and
	at each unit there exist links to the document database. Thus, while
	the searching can be started by locating those documents that match
	best with the search expression, further relevant search results
	can be found on the basis of the pointers stored at the same or neighboring
	map units, even if they did not match the search criterion exactly.
	This work contains an overview to the WEBSOM method and its performance,
	and as a special application, the WEBSOM map of the texts of Encyclopaedia
	Britannica is described.},
  doi = {DOI: 10.1016/j.ins.2003.03.017},
  file = {Lagus2004.pdf:SOM\\applications\\Lagus2004.pdf:PDF},
  issn = {0020-0255},
  keywords = {Information retrieval},
  url = {http://www.sciencedirect.com/science/article/B6V0C-4B22RK2-7/2/1d521a7c4c6cdcf2ba2214c1a21cafaa}
}

@ARTICLE{Laha2008,
  author = {Laha,, A. and Chanda,, B. and Pal,, N.},
  title = {Fast codebook searching in a SOM-based vector quantizer for image
	compression},
  journal = {Signal, Image and Video Processing},
  year = {2008},
  volume = {2},
  pages = {39-49},
  note = {10.1007/s11760-007-0034-3},
  abstract = {We propose a novel method for fast codebook searching in self-organizing
	map (SOM)-generated codebooks. This method performs a non-exhaustive
	search of the codebook to find a good match for an input vector.
	While performing an exhaustive search in a large codebook with high
	dimensional vectors, the encoder faces a significant computational
	barrier. Due to its topology preservation property, SOM holds a good
	promise of being utilized for fast codebook searching. This aspect
	of SOM remained largely unexploited till date. In this paper we first
	develop two separate strategies for fast codebook searching by exploiting
	the properties of SOM and then combine these strategies to develop
	the proposed method for improved overall performance. Though the
	method is general enough to be applied for any kind of signal domain,
	in the present paper we demonstrate its efficacy with spatial vector
	quantization of gray-scale images.},
  affiliation = {Infosys Technologies Limited SET Labs Lingampally Hyderabad 500 019
	India},
  file = {Laha2008.pdf:fast-BMU/Laha2008.pdf:PDF},
  issn = {1863-1703},
  issue = {1},
  keyword = {Computer Science},
  publisher = {Springer London},
  url = {http://dx.doi.org/10.1007/s11760-007-0034-3}
}

@ARTICLE{Lai2004,
  author = {Lai,, J. Z. C. and Liaw,,Y.-C.},
  title = {Fast-searching algorithm for vector quantization using projection
	and triangular inequality},
  journal = {Image Processing, IEEE Transactions on},
  year = {2004},
  volume = {13},
  pages = {1554 -1558},
  number = {12},
  month = {dec.},
  abstract = {In this paper, a new and fast-searching algorithm for vector quantization
	is presented. Two inequalities, one used for terminating the searching
	process and the other used to delete impossible codewords, are presented
	to reduce the distortion computations. Our algorithm makes use of
	a vector's features (mean value, edge strength, and texture strength)
	to reject many unlikely codewords that cannot be rejected by other
	available approaches. Experimental results show that our algorithm
	is superior to other algorithms in terms of computing time and the
	number of distortion calculations. Compared with available approaches,
	our method can reduce the computing time and the number of distortion
	computations significantly. Compared with the best method of reducing
	distortion computation, our algorithm can further reduce the number
	of distortion calculations by 29% to 58.4%. Compared with the best
	encoding algorithm for vector quantization, our approach also further
	reduces the computing time by 8% to 47.7%.},
  doi = {10.1109/TIP.2004.837559},
  file = {Lai2004.pdf:fast-BMU/Lai2004.pdf:PDF},
  issn = {1057-7149},
  keywords = {codeword;edge strength;fast searching algorithm;projection inequality;texture
	strength;triangular inequality;vector quantization;codes;vector quantisation;Algorithms;Artificial
	Intelligence;Computer Graphics;Computer Simulation;Data Compression;Image
	Enhancement;Image Interpretation, Computer-Assisted;Numerical Analysis,
	Computer-Assisted;Pattern Recognition, Automated;Reproducibility
	of Results;Sensitivity and Specificity;Signal Processing, Computer-Assisted;}
}

@PHDTHESIS{Lai1990,
  author = {Lai,, T. W. H.},
  title = {Efficient maintenance of binary search trees},
  school = {University of Waterloo},
  year = {1990},
  address = {Waterloo, Ont., Canada},
  owner = {castudil},
  timestamp = {2009.05.13}
}

@INPROCEEDINGS{Lampinen1989,
  author = {Lampinen,, J. and Oja,, E.},
  title = {Fast self-organization by the probing algorithm},
  booktitle = {Neural Networks, 1989. IJCNN., International Joint Conference on},
  year = {1989},
  pages = {503 -507 vol.2},
  month = jun,
  doi = {10.1109/IJCNN.1989.118290},
  file = {Lampinen1989.pdf:fast-BMU/Lampinen1989.pdf:PDF},
  keywords = {Kohonen's self-organization procedure;computational algorithm;exhaustive
	search;high-dimensional vectors;matching unit;probing algorithm;self-organization;adaptive
	systems;algorithm theory;content-addressable storage;neural nets;}
}

@ARTICLE{Lawrence1997,
  author = {Lawrence,, S. and Giles,, C.L. and Ah,, Chung Tsoi and Back, A.D.},
  title = {Face recognition: a convolutional neural-network approach},
  journal = {Neural Networks, IEEE Transactions on},
  year = {1997},
  volume = {8},
  pages = {98-113},
  number = {1},
  month = {Jan},
  abstract = {We present a hybrid neural-network for human face recognition which
	compares favourably with other methods. The system combines local
	image sampling, a self-organizing map (SOM) neural network, and a
	convolutional neural network. The SOM provides a quantization of
	the image samples into a topological space where inputs that are
	nearby in the original space are also nearby in the output space,
	thereby providing dimensionality reduction and invariance to minor
	changes in the image sample, and the convolutional neural network
	provides partial invariance to translation, rotation, scale, and
	deformation. The convolutional network extracts successively larger
	features in a hierarchical set of layers. We present results using
	the Karhunen-Loeve transform in place of the SOM, and a multilayer
	perceptron (MLP) in place of the convolutional network for comparison.
	We use a database of 400 images of 40 individuals which contains
	quite a high degree of variability in expression, pose, and facial
	details. We analyze the computational complexity and discuss how
	new classes could be added to the trained recognizer},
  doi = {10.1109/72.554195},
  file = {paper:References\\ANN\\Lawrence1997.PDF:PDF},
  issn = {1045-9227},
  keywords = {computational complexity, convolution, face recognition, feature extraction,
	image matching, quantisation (signal), self-organising feature maps,
	topologycomputational complexity, convolutional neural-network, dimensionality
	reduction, face recognition, feature extraction, invariance, local
	image sampling, quantization, self-organizing map, template matching,
	topological space}
}

@ARTICLE{Lazebnik2009,
  author = {Lazebnik,, S. and Raginsky,, M.},
  title = {Supervised Learning of Quantizer Codebooks by Information Loss Minimization},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2009},
  volume = {31},
  pages = {1294 -1309},
  number = {7},
  month = {July},
  abstract = {This paper proposes a technique for jointly quantizing continuous
	features and the posterior distributions of their class labels based
	on minimizing empirical information loss such that the quantizer
	index of a given feature vector approximates a sufficient statistic
	for its class label. Informally, the quantized representation retains
	as much information as possible for classifying the feature vector
	correctly. We derive an alternating minimization procedure for simultaneously
	learning codebooks in the Euclidean feature space and in the simplex
	of posterior class distributions. The resulting quantizer can be
	used to encode unlabeled points outside the training set and to predict
	their posterior class distributions, and has an elegant interpretation
	in terms of lossless source coding. The proposed method is validated
	on synthetic and real data sets and is applied to two diverse problems:
	learning discriminative visual vocabularies for bag-of-features image
	classification and image segmentation.},
  doi = {10.1109/TPAMI.2008.138},
  file = {Lazebnik2009.pdf:Pattern Recognition/Lazebnik2009.pdf:PDF},
  issn = {0162-8828},
  keywords = {Euclidean feature space;bag-of-features image classification;feature
	vector;image segmentation;information loss minimization;quantizer
	codebooks;supervised learning;image classification;image segmentation;learning
	(artificial intelligence);Algorithms;Artificial Intelligence;Data
	Compression;Numerical Analysis, Computer-Assisted;Pattern Recognition,
	Automated;Reproducibility of Results;Sensitivity and Specificity;Signal
	Processing, Computer-Assisted;}
}

@ARTICLE{Li1997,
  author = {Li,,R. Y. and Lebby,,G. L. and Si,,H.},
  title = {A modified approach for constructing the self-organized layer in
	a multilayer feedforward neural network},
  journal = {Information Sciences},
  year = {1997},
  volume = {98},
  pages = {69 - 81},
  number = {1-4},
  abstract = {One of the most important models in neural network research is the
	backpropagation algorithm for the multilayer feedforward network.
	A major disadvantage of using a conventional backpropagation algorithm
	is that its training process can be very slow, since the network
	is assigned with random initial weights and an arbitrarily assigned
	hidden layer structure. We are using self-organizing back propagation
	and modified self-organizing back propagation algorithms to greatly
	reduce its training time. These algorithms are used to solve the
	exclusive-or problem and to classify multispectral image data. The
	results demonstrate that the new approaches can greatly reduce the
	number of interations required in the training process.},
  doi = {10.1016/S0020-0255(96)00161-2},
  file = {Li1997.pdf:SOM\\applications\\Li1997.pdf:PDF},
  issn = {0020-0255},
  url = {http://www.sciencedirect.com/science/article/B6V0C-3SNV3NV-4/2/2f079bb057a928f9c4f1e32360832704}
}

@ARTICLE{Lin1983,
  author = {Y. K. Lin and K. S. Fu},
  title = {Automatic classification of cervical cells using a binary tree classifier},
  journal = {Pattern Recognition},
  year = {1983},
  volume = {16},
  pages = {69--80},
  number = {1},
  abstract = {A k-means clustering algorithm for designing binary tree classifiers
	is introduced for the classification of cervical cells. At each nonterminal
	node of the designed binary tree classifier, two sets of effective
	feature are selected: one is based on the Bhattacharyya distance,
	a measure of separability between two classes; the other is based
	on the merits of classification accuracy. The classification result
	has shown the effectiveness of the features and the binary tree classifier
	used.},
  doi = {10.1016/0031-3203(83)90010-9},
  file = {Lin1983.pdf:RHST/HyperplaneTree-classifiers/Lin1983.pdf:PDF},
  issn = {0031-3203},
  keywords = {Bayes classifier},
  owner = {castudillo},
  timestamp = {2010.12.01}
}

@ARTICLE{Linde1980,
  author = {Linde,, Y. and Buzo,, A. and Gray,, R.},
  title = {An Algorithm for Vector Quantizer Design},
  journal = {Communications, IEEE Transactions on},
  year = {1980},
  volume = {28},
  pages = {84-95},
  number = {1},
  month = {Jan},
  abstract = { An efficient and intuitive algorithm is presented for the design
	of vector quantizers based either on a known probabilistic model
	or on a long training sequence of data. The basic properties of the
	algorithm are discussed and demonstrated by examples. Quite general
	distortion measures and long blocklengths are allowed, as exemplified
	by the design of parameter vector quantizers of ten-dimensional vectors
	arising in Linear Predictive Coded (LPC) speech compression with
	a complicated distortion measure arising in LPC analysis that does
	not depend only on the error vector.},
  file = {paper:References/SOM/som-related/Linde1980.pdf:PDF},
  issn = {0090-6778},
  keywords = {Quantization (signal), Signal quantization, Speech coding}
}

@ARTICLE{Lingras2005,
  author = {Lingras,, P. and Hogo,,M. and Snorek,,M. and West,,C.},
  title = {Temporal analysis of clusters of supermarket customers: conventional
	versus interval set approach},
  journal = {Information Sciences},
  year = {2005},
  volume = {172},
  pages = {215 - 240},
  number = {1-2},
  abstract = {Temporal data mining is the application of data mining techniques
	to data that takes the time dimension into account. This paper studies
	changes in cluster characteristics of supermarket customers over
	a 24 week period. Such an analysis can be useful for formulating
	marketing strategies. Marketing managers may want to focus on specific
	groups of customers. Therefore they may need to understand the migrations
	of the customers from one group to another group. The marketing strategies
	may depend on the desirability of these cluster migrations. The temporal
	analysis presented here is based on conventional and modified Kohonen
	self organizing maps (SOM). The modified Kohonen SOM creates interval
	set representations of clusters using properties of rough sets. A
	description of an experimental design for temporal cluster migration
	studies including, data cleaning, data abstraction, data segmentation,
	and data sorting, is provided. The paper compares conventional and
	non-conventional (interval set) clustering techniques, as well as
	temporal and non-temporal analysis of customer loyalty. The interval
	set clustering is shown to provide an interesting dimension to such
	a temporal analysis.},
  doi = {10.1016/j.ins.2004.12.007},
  file = {Lingras2005.pdf:SOM\\applications\\Lingras2005.pdf:PDF},
  issn = {0020-0255},
  keywords = {Temporal data mining},
  url = {http://www.sciencedirect.com/science/article/B6V0C-4F6K738-1/2/1af54736c4793a4a2e367659d1460dba}
}

@ARTICLE{Lu2005,
  author = {Lu,, Z. M. and Chu,, S. C. and Huang,, K. C.},
  title = {Equal-average equalvariance equal-norm nearest neighbor codeword
	search algorithm based on ordered Hadamard transform},
  journal = {International Journal of Innovative Computing, Information and Control},
  year = {2005},
  volume = {1},
  pages = {35–-41},
  number = {1},
  month = {March},
  file = {Lu2005.pdf:fast-BMU/Lu2005.pdf:PDF},
  owner = {castudillo},
  timestamp = {2011.01.19}
}

@BOOK{Mackay2003,
  title = {Information Theory, Inference \& Learning Algorithms},
  publisher = {{Cambridge University Press}},
  year = {2003},
  author = {Mackay,,D. J. C.},
  month = {June},
  file = {book:References/SOM/som-related/Mackay2003.pdf:PDF},
  howpublished = {Hardcover},
  isbn = {0521642981},
  keywords = {information-theory},
  url = {http://www.inference.phy.cam.ac.uk/mackay/itila/book.html}
}

@INPROCEEDINGS{Macqueen1967,
  author = {Macqueen,,J. B.},
  title = {Some methods of classification and analysis of multivariate observations},
  booktitle = {Proceedings of the Fifth Berkeley Symposium on Mathemtical Statistics
	and Probability},
  year = {1967},
  pages = {281--297},
  citeulike-article-id = {903715},
  keywords = {clustering},
  priority = {2}
}

@ARTICLE{Mahalanobis1936,
  author = {Mahalanobis,, P. C.},
  title = {On the generalised distance in statistics},
  journal = {Proceedings of the National Institute of Sciences of India},
  year = {1936},
  volume = {2},
  pages = {49–55},
  number = {1},
  file = {paper:/References/SOM/som-related/Mahalanobis1936.pdf:PDF},
  owner = {castudil},
  timestamp = {2009.09.23}
}

@ARTICLE{Mangasarian1995,
  author = {Mangasarian,,O. L. and Street,,W. N. and Wolberg,,W. H.},
  title = {Breast Cancer Diagnosis and Prognosis via Linear Programming},
  journal = {Operations Research},
  year = {1995},
  volume = {43},
  pages = {570--577},
  month = {Aug}
}

@BOOK{Manning2009,
  title = {An Introduction to Information Retrieval},
  publisher = {Cambridge University Press},
  year = {2009},
  author = {Manning,,C. D. and Raghavan,,P. and Sch\"{u}tze,,H.},
  address = {Cambridge, England},
  file = {Manning2009.pdf:book/Introduction to Information Retrieval/Manning2009.pdf:PDF},
  owner = {castudil},
  timestamp = {2010.08.28},
  url = {http://nlp.stanford.edu/IR-book/information-retrieval-book.html}
}

@BOOK{Marcus1988,
  title = {Introduction to Linear Algebra},
  publisher = {Dover Publications},
  year = {1988},
  author = {Marcus,, M. and Minc,, H},
  pages = {288},
  address = {New York},
  owner = {castudil},
  timestamp = {2009.10.09},
  url = {http://www.amazon.com/exec/obidos/ASIN/0486656950/ref=nosim/weisstein-20}
}

@ARTICLE{Martin2008,
  author = {Martin,,C. and Diaz,,N. N. and Ontrup,,J. and Nattkemper,,T. W.},
  title = {Hyperbolic {SOM}-based clustering of {DNA} fragment features for
	taxonomic visualization and classification},
  journal = {Bioinformatics},
  year = {2008},
  volume = {24},
  pages = {1568-1574},
  number = {14},
  abstract = {Motivation: Modern high-throughput sequencing technologies enable
	the simultaneous analysis of organisms in an environment. The analysis
	of species diversity and the binning of DNA fragments of non-sequenced
	species for assembly are two major challenges in sequence analysis.
	To achieve reasonable binnings and classifications, DNA fragment
	structure has to be represented appropriately, so it can be processed
	by machine learning algorithms.
	
	
	Results: Hierarchically growing hyperbolic Self-Organizing maps (H
	2SOMs) are trained to cluster small variable-length DNA fragments
	(0.2–50 kb) of 350 prokaryotic organisms at six taxonomic ranks Superkingdom,
	Phylum, Class, Order, Genus and Species in the Tree of Life. DNA
	fragments are mapped to three different types of feature vectors
	based on the genomic signature: basic features, features considering
	the importance of oligonucleotide patterns as well as contrast enhanced
	features. The H 2SOM classifier achieves high classification rates
	while at the same time its visualization allows further insights
	into the projected data and has the potential to support binning
	of short sequence reads, because DNA fragments can be grouped into
	phylogenetic groups.
	
	
	Availability: An implementation of the H 2HSOM classifier in Matlab
	is provided at www.techfak.uni-bielefeld.de/ags/ani/projects/HHSOMSeqData},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  ee = {http://dx.doi.org/10.1093/bioinformatics/btn257},
  file = {paper:SOM\\SOM-variants\\Martin2008.PDF:PDF}
}

@ARTICLE{Martin1993,
  author = {Martin,, L. and Leblanc,, R. and Toan,, N. K.},
  title = {Tables for the Friedman Rank Test},
  journal = {The Canadian Journal of Statistics / La Revue Canadienne de Statistique},
  year = {1993},
  volume = {21},
  pages = {pp. 39--43},
  number = {1},
  abstract = {This note presents tables for Friedman's test for two-way analysis
	of variance by ranks. These tables are more accurate than those that
	are presented in the literature. After intensive simulations, we
	have found for particular critical values some discrepancies with
	tables published earlier. The tables are also more extensive than
	those previously available. /// Cet article présente des tables du
	test des rangs de Friedman pour l'analyse de variance à deux facteurs.
	Ces tables sont plus précises que celles présentées dans la littérature.
	À l'aide d'un grand nombre de simulations, nous avons remarqué pour
	certaines valeurs critiques quelques contradictions avec les tables
	publiées par d'autres auteurs. Les tables présentées dans cet article
	sont également plus exhaustives que celles disponibles actuellement.},
  copyright = {Copyright © 1993 Statistical Society of Canada},
  issn = {03195724},
  jstor_articletype = {research-article},
  jstor_formatteddate = {Mar., 1993},
  language = {English},
  publisher = {Statistical Society of Canada},
  url = {http://www.jstor.org/stable/3315656}
}

@INPROCEEDINGS{Martinetz1991,
  author = {Martinetz,,M. and Schulten,,K. J.},
  title = {A ``neural-gas'' network learns topologies},
  booktitle = {in Proceedings of International Conference on Articial Neural Networks},
  year = {1991},
  volume = {I},
  pages = {397--402},
  address = {North-Holland, Amsterdam},
  file = {paper:References\\SOM\\SOM-variants\\non-tree-based\\Martinetz1991.PDF:PDF}
}

@ARTICLE{Martinetz1994,
  author = {Martinetz,, T. and Schulten,, K.},
  title = {Topology representing networks},
  journal = {Neural Netw.},
  year = {1994},
  volume = {7},
  pages = {507--522},
  month = {March},
  acmid = {187741},
  address = {Oxford, UK, UK},
  doi = {10.1016/0893-6080(94)90109-0},
  issn = {0893-6080},
  issue = {3},
  keywords = {Delaunay triangulation, Hebb rule, Voronoi polyhedron, path planning,
	path preservation, proximity problems, topology preserving feature
	map, topology representation, winner-take-all competition},
  numpages = {16},
  publisher = {Elsevier Science Ltd.}
}

@ARTICLE{Martinetz1993,
  author = {Martinetz,, T. M. and Berkovich,, S. G. and Schulten,, K. J.},
  title = {`{N}eural-gas' network for vector quantization and its application
	to time-series prediction},
  journal = {Neural Networks, IEEE Transactions on},
  year = {1993},
  volume = {4},
  pages = {558-569},
  number = {4},
  month = {Jul},
  abstract = {A neural network algorithm based on a soft-max adaptation rule is
	presented. This algorithm exhibits good performance in reaching the
	optimum minimization of a cost function for vector quantization data
	compression. The soft-max rule employed is an extension of the standard
	K-means clustering procedure and takes into account a neighborhood
	ranking of the reference (weight) vectors. It is shown that the dynamics
	of the reference (weight) vectors during the input-driven adaptation
	procedure are determined by the gradient of an energy function whose
	shape can be modulated through a neighborhood determining parameter
	and resemble the dynamics of Brownian particles moving in a potential
	determined by the data point density. The network is used to represent
	the attractor of the Mackey-Glass equation and to predict the Mackey-Glass
	time series, with additional local linear mappings for generating
	output values. The results obtained for the time-series prediction
	compare favorably with the results achieved by backpropagation and
	radial basis function networks},
  doi = {10.1109/72.238311},
  file = {paper:References\\SOM\\SOM-variants\\non-tree-based\\Martinetz1993.PDF:PDF},
  issn = {1045-9227},
  keywords = {data compression, minimisation, neural nets, time series, vector quantisationBrownian
	motion, K-means clustering, Mackey-Glass equation, cost function,
	data compression, data point density, local linear mappings, minimization,
	neighborhood ranking, neural network, soft-max adaptation rule, time-series
	prediction, vector quantization}
}

@BOOK{Matsopoulos2010,
  title = {Self-Organizing Maps},
  publisher = {In-Tech},
  year = {2010},
  editor = {Matsopoulos,, G. K.},
  author = {Matsopoulos,, G. K. (Ed.)},
  file = {Matsopoulos2010.pdf:book/Matsopoulos2010.pdf:PDF},
  owner = {castudillo},
  timestamp = {2011.02.04}
}

@ARTICLE{McCabe1965,
  author = {McCabe,,J.},
  title = {On serial files with relocatable records},
  journal = {Operations Research},
  year = {1965},
  volume = {12},
  pages = {609-618},
  owner = {castudil},
  timestamp = {2009.02.17}
}

@ARTICLE{McCulloch1990,
  author = {McCulloch,, Warren and Pitts,, Walter},
  title = {A logical calculus of the ideas immanent in nervous activity},
  journal = {Bulletin of Mathematical Biology},
  year = {1990},
  volume = {52},
  pages = {99--115},
  number = {1},
  month = jan,
  abstract = {Because of the ``all-or-none'' character of nervous activity, neural
	events and the relations among them can be treated by means of propositional
	logic. It is found that the behavior of every net can be described
	in these terms, with the addition of more complicated logical means
	for nets containing circles; and that for any logical expression
	satisfying certain conditions, one can find a net behaving in the
	fashion it describes. It is shown that many particular choices among
	possible neurophysiological assumptions are equivalent, in the sense
	that for every net behaving under one assumption, there exists another
	net which behaves under the other and gives the same results, although
	perhaps not in the same time. Various applications of the calculus
	are discussed.},
  file = {paper:home/castudil/Documents/research/Cesar/References/ANN/McCulloch1990.pdf:PDF},
  owner = {castudil},
  timestamp = {2009.12.08},
  url = {http://dx.doi.org/10.1007/BF02459570}
}

@ARTICLE{McCulloch1943,
  author = {McCulloch,, Warren and Pitts,, Walter},
  title = {A logical calculus of the ideas immanent in nervous activity},
  journal = {Bulletin of Mathematical Biophysics},
  year = {1943},
  volume = {7},
  pages = {115 - 133},
  abstract = {Because of the ``all-or-none'' character of nervous activity, neural
	events and the relations among them can be treated by means of propositional
	logic. It is found that the behavior of every net can be described
	in these terms, with the addition of more complicated logical means
	for nets containing circles; and that for any logical expression
	satisfying certain conditions, one can find a net behaving in the
	fashion it describes. It is shown that many particular choices among
	possible neurophysiological assumptions are equivalent, in the sense
	that for every net behaving under one assumption, there exists another
	net which behaves under the other and gives the same results, although
	perhaps not in the same time. Various applications of the calculus
	are discussed.},
  file = {paper:home/castudil/Documents/research/Cesar/References/ANN/McCulloch1990.pdf:PDF},
  owner = {castudil},
  timestamp = {2009.12.08},
  url = {http://dx.doi.org/10.1007/BF02459570}
}

@ARTICLE{Mehlhorn1979,
  author = {Mehlhorn,,K.},
  title = {Dynamic Binary Search},
  journal = {SIAM Journal on Computing},
  year = {1979},
  volume = {8},
  pages = {175--198},
  number = {2},
  publisher = {SIAM}
}

@ARTICLE{Meisel1973,
  author = {Meisel, W.S. and Michalopoulos, D.A.},
  title = {A Partitioning Algorithm with Application in Pattern Classification
	and the Optimization of Decision Trees},
  journal = {Computers, IEEE Transactions on},
  year = {1973},
  volume = {C-22},
  pages = {93--103},
  number = {1},
  month = jan.,
  abstract = {The efficient partitioning of a finite-dimensional space by a decision
	tree, each node of which corresponds to a comparison involving a
	single variable, is a problem occurring in pattern classification,
	piecewise-constant approximation, and in the efficient programming
	of decision trees. A two-stage algorithm is proposed. The first stage
	obtains a sufficient partition suboptimally, either by methods suggested
	in the paper or developed elsewhere; the second stage optimizes the
	results of the first stage through a dynamic programming approach.
	In pattern classification, the resulting decision rule yields the
	minimum average number of calculations to reach a decision. In approximation,
	arbitrary accuracy for a finite number of unique samples is possible.
	In programming decision trees, the expected number of computations
	to reach a decision is minimized.},
  doi = {10.1109/T-C.1973.223603},
  file = {Meisel1973.pdf:RHST/HyperplaneTree-classifiers/Meisel1973.pdf:PDF},
  issn = {0018-9340},
  keywords = { Decision rules, decision trees, dynamic programming, invariant imbedding,
	pattern classification, piecewise-constant approximation.;}
}

@INPROCEEDINGS{Merkl2003,
  author = {Merkl,,D. and Hui-He,,S. and Dittenbach,,M. and Rauber,,A.},
  title = {Adaptive Hierarchical Incremental Grid Growing: An architecture for
	high-dimensional data visualization},
  booktitle = {In Proceedings of the 4th Workshop on Self-Organizing Maps, Advances
	in Self-Organizing Maps},
  year = {2003},
  pages = {293--298}
}

@ARTICLE{Miikkulainen1990,
  author = {Miikkulainen,,R.},
  title = {Script Recognition with Hierarchical Feature Maps},
  journal = {Connection Science},
  year = {1990},
  volume = {2},
  pages = {83--101},
  number = {1\&2},
  abstract = {The hierarchical feature map system recognizes an input story as an
	instance of a particular script by classifying it at three levels:
	scripts, tracks and role bindings. The recognition taxonomy, i.e.
	the breakdown of each script into the tracks and roles, is extracted
	automatically and independently for each script from examples of
	script instantiations in an unsupervised self-organizing process.
	The process resembles human learning in that the differentiation
	of the most frequently encountered scripts become gradually the most
	detailed. The resulting structure is a hierachical pyramid of feature
	maps. The hierarchy visualizes the taxonomy and the maps lay out
	the topology of each level. The number of input lines and the self-organization
	time are considerably reduced compared to the ordinary single-level
	feature mapping. The system can recognize incomplete stories and
	recover the missing events. The taxonomy also serves as memory organization
	for script-based episodic memory. The maps assign a unique memory
	location for each script instantiation. The most salient parts of
	the input data are separated and most resources are concentrated
	on representing them accurately.},
  file = {Miikkulainen1990.pdf:SOM\\SOM-variants\\Miikkulainen1990.pdf:PDF},
  url = {http://eprints.kfupm.edu.sa/63813/}
}

@ARTICLE{Milligan1981,
  author = {Milligan,,Glenn},
  title = {A monte carlo study of thirty internal criterion measures for cluster
	analysis},
  journal = {Psychometrika},
  year = {1981},
  volume = {46},
  pages = {187-199},
  number = {2},
  month = {June},
  file = {paper:References/SOM/som-related/Milligan1981.pdf:PDF},
  url = {http://ideas.repec.org/a/spr/psycho/v46y1981i2p187-199.html}
}

@BOOK{Minsky1969,
  title = {Perceptrons: An Introduction to Computational Geometry},
  publisher = {The MIT Press},
  year = {1969},
  author = {Minsky,, M. and Papert,, S.},
  owner = {castudil},
  timestamp = {2009.12.06}
}

@BOOK{Mitchell1997,
  title = {Machine Learning},
  publisher = {McGraw Hill},
  year = {1997},
  author = {Mitchell,,Tom},
  owner = {castudil},
  timestamp = {2009.09.05},
  url = {http://www.cs.cmu.edu/~tom/mlbook.html}
}

@ARTICLE{Mizoguchi1977,
  author = {Mizoguchi, R. and Kizawa, M. and Shimura, M.},
  title = {Piecewise linear discriminant functions in pattern recognition},
  journal = {Systems-Comput.-Controls},
  year = {1977},
  volume = {8},
  pages = {62--68 (1978)},
  number = {1},
  file = {Mizoguchi1977.pdf:RHST/HyperplaneTree-classifiers/Mizoguchi1977.pdf:PDF},
  mrclass = {68A45},
  mrnumber = {0495290 (58 \#14007)}
}

@INPROCEEDINGS{Moschou2006,
  author = {Moschou, Vassiliki and Ververidis, Dimitrios and Kotropoulos, Constantine},
  title = {On the Variants of the Self-Organizing Map That Are Based on Order
	Statistics},
  booktitle = {Artificial Neural Networks, ICANN 2006},
  year = {2006},
  pages = {425--434},
  abstract = {Two well-known variants of the self-organizing map (SOM) that are
	based on order statistics are the marginal median SOM and the vector
	median SOM. In the past, their efficiency was demonstrated for color
	image quantization. In this paper, we employ the well-known IRIS
	data set and we assess their performance with respect to the accuracy,
	the average over all neurons mean squared error between the patterns
	that were assigned to a neuron and the neuron's weight vector, and
	the Rand index. All figures of merit favor the marginal median SOM
	and the vector median SOM against the standard SOM. Based on the
	aforementioned findings, the marginal median SOM and the vector median
	SOM are used to re-distribute emotional speech patterns from the
	Danish Emotional Speech database that were originally classified
	as being neutral to four emotional states such as hot anger, happiness,
	sadness, and surprise.},
  file = {paper:References\\SOM\\SOM-variants\\non-tree-based\\Moschou2006.pdf:PDF},
  url = {http://dx.doi.org/10.1007/11840817_45}
}

@ARTICLE{Murashima1999,
  author = {Murashima,,S. and Kashima,,M. and Fuchida,,T.},
  title = {New method for measuring the topology preservation of self-organizing
	feature maps},
  journal = {ICONIP '99, 6th International Conference on Neural Information Processing},
  year = {1999},
  volume = {1},
  pages = {273-278 vol.1},
  doi = {10.1109/ICONIP.1999.843999},
  keywords = {mesh generation, self-organising feature maps, topologydata manifolds,
	masked Delaunay triangulation, output network shape, self-organizing
	feature maps, simulations, topology preservation measure, topology-representing
	network}
}

@ARTICLE{Murthy1998,
  author = {Murthy,, S. K.},
  title = {Automatic Construction of Decision Trees from Data: A Multi-Disciplinary
	Survey},
  journal = {Data Mining and Knowledge Discovery},
  year = {1998},
  volume = {2},
  pages = {345-389},
  abstract = {Decision trees have proved to be valuable tools for the description,
	classification and generalization of data. Work on constructing decision
	trees from data exists in multiple disciplines such as statistics,
	pattern recognition, decision theory, signal processing, machine
	learning and artificial neural networks. Researchers in these disciplines,
	sometimes working on quite different problems, identified similar
	issues and heuristics for decision tree construction. This paper
	surveys existing work on decision tree construction, attempting to
	identify the important issues involved, directions the work has taken
	and the current state of the art.},
  doi = {10.1023/A:1009744630224},
  file = {Murthy1998.pdf:Pattern Recognition/Murthy1998.pdf:PDF},
  issn = {1384-5810},
  issue = {4},
  keyword = {Computer Science},
  publisher = {Springer Netherlands}
}

@ARTICLE{Nievergelt1994,
  author = {Nievergelt, Yves},
  title = {Total Least Squares: State-of-the-Art Regression in Numerical Analysis},
  journal = {SIAM Review},
  year = {1994},
  volume = {36},
  pages = {pp. 258-264},
  number = {2},
  abstract = {Total least squares regression (TLS) fits a line to data where errors
	may occur in both the dependent and independent variables. In higher
	dimensions, TLS fits a hyperplane to such data. The elementary algorithm
	presented here fits readily in a first course in numerical linear
	algebra.},
  copyright = {Copyright © 1994 Society for Industrial and Applied Mathematics},
  file = {Nievergelt1994.pdf:RHST/Nievergelt1994.pdf:PDF},
  issn = {00361445},
  jstor_articletype = {research-article},
  jstor_formatteddate = {Jun., 1994},
  language = {English},
  publisher = {Society for Industrial and Applied Mathematics},
  url = {http://www.jstor.org/stable/2132463}
}

@INPROCEEDINGS{Novikoff1962,
  author = {Novikoff,, Albert B.},
  title = {On convergence proofs for perceptrons},
  booktitle = {Proceedings of the Symposium on the Mathematical Theory of Automata},
  year = {1962},
  volume = {12},
  pages = {615--622},
  citeulike-article-id = {2073209},
  citeulike-linkout-0 = {http://citeseer.comp.nus.edu.sg/context/494822/0},
  keywords = {perceptron-learning-rule},
  owner = {castudil},
  posted-at = {2007-12-07 15:56:34},
  priority = {2},
  timestamp = {2009.12.07},
  url = {http://citeseer.comp.nus.edu.sg/context/494822/0}
}

@ARTICLE{Obaidat1998,
  author = {Obaidat,,M. S. and Khalid,,H. and Sadoun,, B.},
  title = {Ultrasonic transducer characterization by neural networks},
  journal = {Information Sciences},
  year = {1998},
  volume = {107},
  pages = {195 - 215},
  number = {1-4},
  abstract = {This paper presents neural network-based system for the characterization
	of ultrasonic transducers. An automated system for characterizing
	ultrasonic transducers was designed and built. Different characterizing
	algorithms were applied and their performance was investigated and
	compared. It was found that artificial neural network (ANN) techniques,
	in general, provide better classification as compared to the pattern
	recognition techniques we applied earlier (M.S. Obaidat, J.W. Ekis,
	IEEE Transactions on Instrumentation and Measurements, 40 (5) (1991)
	847-850). The Moody-Darken Radial Basis Function network (MD-RBFN),
	Learning Vector Quantization (LVQ) with 52 kohonen neurons, and Fuzzy
	ARTMAP classification network are the neural networks (NNs) that
	provided us with a classification accuracy of 100%. Several variants
	of Backpropagation neural network (BPNN) were tested for this application,
	and the classification results were seen to vary in the range 6.55%-98.35%.
	The best performing paradigm among several variants of the Modular
	Neural Network (MNN), Reinforcement Neural Network (RNN), Probabilistic
	Neural Network (PNN), and Counterpropagation Neural Network (CPNN)
	produced a classification accuracy of 78.85%, 31.42%, 45.21%, and
	79.28%, respectively. The competitive learning (CL) technique provided
	poor results as compared to the Self-Organizing-Map (SOM) for preclustering.},
  doi = {10.1016/S0020-0255(97)10048-2},
  file = {Obaidat1998.pdf:SOM\\applications\\Obaidat1998.pdf:PDF},
  issn = {0020-0255},
  keywords = {Ultrasonic transducers},
  url = {http://www.sciencedirect.com/science/article/B6V0C-3TKS65B-1S/2/8325ab722c88e4ee4525b21ccfcd2226}
}

@ARTICLE{Ogniewicz1995,
  author = {Ogniewicz,,R. L. and K{\"u}bler,,O.},
  title = {Hierarchic Voronoi skeletons},
  journal = {Pattern Recognition},
  year = {1995},
  volume = {28},
  pages = {343-359},
  number = {3},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  ee = {http://dx.doi.org/10.1016/0031-3203(94)00105-U}
}

@ARTICLE{Oja2003,
  author = {Oja,,M. and Kaski,,S. and Kohonen,,T.},
  title = {Bibliography of Self-Organizing Map ({SOM}) Papers: 1998-2001 Addendum},
  journal = {Neural Computing Surveys},
  year = {2003},
  volume = {3},
  pages = {1--156}
}

@ARTICLE{Oommen1998,
  author = {Oommen, J. and Altinel, I. K. and Aras, N.},
  title = {Discrete vector quantization for arbitrary distance function estimation},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B},
  year = {1998},
  volume = {28},
  pages = {496--510},
  number = {4},
  month = aug,
  abstract = {There are currently many vastly different areas of research involving
	adaptive learning. Among them are the two areas that concern neural
	networks and learning automata. This paper develops a method by which
	the general philosophies of vector quantization (VQ) and discretized
	automata learning can be incorporated for the computation of arbitrary
	distance functions. The latter is a problem which has important applications
	in logistics and location analysis. The input to our problem is the
	set of coordinates of a large number of nodes whose internode arbitrary
	“distances” have to be estimated. To render the problem interesting,
	nontrivial, and realistic, we assume that the explicit form of this
	distance function is both unknown and uncomputable. Unlike traditional
	operations research methods, which use optimized parametric functional
	estimators, we have utilized discretized VQ principles to first adaptively
	polarize the nodes into subregions. Subsequently, the parameters
	characterizing the subregions are learned by using a variety of methods
	(including, for academic purposes, a VQ strategy in the meta-domain).
	After an initial training phase, a system which achieves distance
	estimation attempts to yield an estimate of any node-pair distance
	without actually deriving an explicit form for the unknown function.
	The algorithms have been rigorously tested for the actual road-travel
	distances involving cities in Turkey and the results obtained are
	conclusive. Indeed, these present results are the best currently
	available from any single or hybrid strategy},
  doi = {10.1109/3477.704289},
  file = {paper:VQ\\Oommen1998.pdf:PDF}
}

@INPROCEEDINGS{Polzlbauer2004,
  author = {P\"olzlbauer,,G.},
  title = {Survey and Comparison of Quality Measures for Self-Organizing Maps},
  booktitle = {Proceedings of the Fifth Workshop on Data Analysis (WDA'04)},
  year = {2004},
  editor = {J\'an Parali\v{c} and Georg P\"olzlbauer and Andreas Rauber},
  pages = {67--82},
  address = {Sliezsky dom, Vysok\'e Tatry, Slovakia},
  month = {June 24--27},
  publisher = {Elfa Academic Press},
  abstract = {Self-Organizing Maps have a wide range of beneficial properties for
	data mining, like vector quantization and projection. Several measures
	exist that quantify the quality of either of these properties. The
	scope of this work is to describe and compare some of the most well-known
	measures. This is done by conducting a series of experiments for
	different map topologies with several well-known data sets. The measures
	are examined whether they are suited to determine hyperparameters
	like the optimal map size, how well the measure itself is suited
	to compare different maps, and if they allow comparison to other
	algorithms similar to the SOM (e.g. Sammons Mapping).},
  file = {paper:References/SOM/som-related/Polzlbauer2004.pdf:PDF},
  isbn = {80-89066-87-9},
  thanks = {none}
}

@ARTICLE{Polla2007,
  author = {P\"{o}ll\"{a},, M. and Honkela,, T. and Kohonen,, T.},
  title = {Bibliography of Self-Organizing Map ({SOM}) Papers: 2002-2005 Addendum},
  journal = {Neural Computing Surveys},
  year = {2007},
  volume = {forthcoming},
  owner = {castudil},
  timestamp = {2009.08.29}
}

@INPROCEEDINGS{Pakkanen2005,
  author = {Pakkanen,,J.},
  title = {Examining the Behaviour of the {E}volving {T}ree},
  booktitle = {Proceegins of the 5th Workshop on Self-Organizing Maps},
  year = {2005},
  pages = {163--170},
  address = {Paris, France},
  month = {September}
}

@INPROCEEDINGS{Pakkanen2003,
  author = {Pakkanen,,J.},
  title = {The {E}volving {T}ree, a new kind of self-organizing neural network},
  booktitle = {Proceedings of the Workshop on Self-Organizing Maps '03},
  year = {2003},
  pages = {311--316},
  address = {Kitakyushu, Japan},
  month = {September 11--14}
}

@INPROCEEDINGS{Pakkanen2005a,
  author = {Pakkanen,,J. and J. Iivarinen},
  title = {Analyzing Large Databases with the {E}volving {T}ree},
  booktitle = {Proceedings of the Third International Conference on Advances in
	Pattern Recognition},
  year = {2005},
  editor = {Sameer Singh and Maneesha Singh and Chid Apte and Petra Perner},
  number = {3686},
  series = {Lecture Notes in Computer Science},
  pages = {192--198},
  address = {Bath, UK},
  month = {August},
  publisher = {Springer}
}

@INPROCEEDINGS{Pakkanen2004a,
  author = {Pakkanen,,J. and Iivarinen,,J.},
  title = {A Novel Self-Organizing Neural Network for Defect Image Classification},
  booktitle = {Proceedings of IJCNN 2004},
  year = {2004},
  pages = {2553--2556},
  address = {Budapest, Hungary},
  month = {July 25--29}
}

@INPROCEEDINGS{Pakkanen2005b,
  author = {Pakkanen,,J. and Iivarinen,,J. and Oja,,E.},
  title = {The {E}volving {T}ree, a Hierarchical Tool for Unsupervised Data
	Analysis},
  booktitle = {Proceedings of International Joint Conference on Neural Networks},
  year = {2005},
  pages = {1395--1399},
  address = {Montr{\'e}al, Canada},
  month = {August}
}

@ARTICLE{Pakkanen2004,
  author = {Pakkanen,,J. and Iivarinen,,J. and Oja,,E.},
  title = {The {E}volving {T}ree --- A Novel Self-Organizing Network for Data
	Analysis},
  journal = {Neural Processing Letters},
  year = {2004},
  volume = {20},
  pages = {199-211},
  number = {3},
  month = {December},
  file = {Pakkanen2004.pdf:SOM\\SOM-variants\\Pakkanen2004.pdf:PDF}
}

@ARTICLE{Pal1994,
  author = {Pal,,S. K. and Mitra,,S.},
  title = {Fuzzy versions of Kohonen's net and MLP-based classification: Performance
	evaluation for certain nonconvex decision regions},
  journal = {Information Sciences},
  year = {1994},
  volume = {76},
  pages = {297 - 337},
  number = {3-4},
  abstract = {Classification of certain linearly nonseparable pattern classes with
	nonconvex decision regions is a problem that cannot be efficiently
	handled by the Bayes' classifier for normal distributions or other
	metric-based methods. An attempt is made here to demonstrate the
	ability of fuzzy versions of Kohonen's net and the multilayer perceptron
	for classification of such patterns. In these models, the uncertainties
	involved in the input description and output decision have been taken
	care of by the concept of fuzzy sets whereas the neural net theory
	helps to generate the required concave and/or disconnected decision
	regions. Superiority of these fuzzy models (over the respective conventional
	versions, the Bayes' classifier and seven other existing neural algorithms)
	has been adequately established when they are implemented on different
	sets of linearly nonseparable pattern classes. The effect of fuzzification
	at the input has been investigated for both models. The contribution
	of the a priori probabilities of the pattern classes in the back-propagation
	procedure for weight updating has also been studied.},
  doi = {10.1016/0020-0255(94)90014-0},
  file = {Pal1994.pdf:SOM\\applications\\Pal1994.pdf:PDF},
  issn = {0020-0255},
  url = {http://www.sciencedirect.com/science/article/B6V0C-48MXWGH-VT/2/13df924de768c42cb6849e81b9e16963}
}

@ARTICLE{Pampalk2004,
  author = {Pampalk,,E. and Widmer,,G. and Chan,,A.},
  title = {A new approach to hierarchical clustering and structuring of data
	with Self-Organizing Maps},
  journal = {Intell. Data Anal.},
  year = {2004},
  volume = {8},
  pages = {131--149},
  number = {2},
  abstract = {The Self-Organizing Map (SOM) is a powerful tool for exploratory data
	analysis which has been employed in a wide range of data mining applications.
	We present a novel approach to reveal the inherent hierarchical structure
	of data using multiple SOMs together with heuristics which optimize
	the stability. In particular, we address shortcomings of the Growing
	Hierarchical Self-Organizing Map (GHSOM) regarding the decision which
	areas in the hierarchical structure need to be represented by a finer
	granularity and which areas do not. We introduce the Tension and
	Mapping Ratio extension to exploit specific characteristics of the
	SOM based on the topology preservation. As a main result, in contrast
	to the GHSOM, the inherent hierarchical structure of the data is
	revealed without requiring the user to define a threshold parameter
	which controls the map sizes of the individual SOMs. We evaluate
	our approach using data from real-world data mining projects in the
	music domain.},
  address = {Amsterdam, The Netherlands, The Netherlands},
  file = {paper:C\:\\Users\\castudil\\Documents\\Research\\Cesar\\References\\SOM\\SOM-variants\\Pampalk2004.pdf:PDF},
  issn = {1088-467X},
  publisher = {IOS Press},
  url = {http://web.ebscohost.com.proxy.library.carleton.ca/ehost/detail?vid=1&hid=105&sid=9bf3dc16-2c11-404b-a227-0889091ffbed%40sessionmgr104&bdata=JnNpdGU9ZWhvc3QtbGl2ZQ%3d%3d#db=bth&AN=12968611#db=bth&AN=12968611}
}

@ARTICLE{Pan2003,
  author = {Pan,, J.-S. and Lu,, Z.-M. and Sun,, S.-H.},
  title = {An efficient encoding algorithm for vector quantization based on
	subvector technique},
  journal = {Image Processing, IEEE Transactions on},
  year = {2003},
  volume = {12},
  pages = {265--270},
  number = {3},
  month = march,
  abstract = {In this paper, a new and fast encoding algorithm for vector quantization
	is presented. This algorithm makes full use of two characteristics
	of a vector: the sum and the variance. A vector is separated into
	two subvectors: one is composed of the first half of vector components
	and the other consists of the remaining vector components. Three
	inequalities based on the sums and variances of a vector and its
	two subvectors components are introduced to reject those codewords
	that are impossible to be the nearest codeword, thereby saving a
	great deal of computational time, while introducing no extra distortion
	compared to the conventional full search algorithm. The simulation
	results show that the proposed algorithm is faster than the equal-average
	nearest neighbor search (ENNS), the improved ENNS, the equal-average
	equal-variance nearest neighbor search (EENNS) and the improved EENNS
	algorithms. Comparing with the improved EENNS algorithm, the proposed
	algorithm reduces the computational time and the number of distortion
	calculations by 2.4% to 6% and 20.5% to 26.8%, respectively. The
	average improvements of the computational time and the number of
	distortion calculations are 4% and 24.6% for the codebook sizes of
	128 to 1024, respectively.},
  doi = {10.1109/TIP.2003.810587},
  file = {Pan2003.pdf:fast-BMU/Pan2003.pdf:PDF},
  issn = {1057-7149},
  keywords = { codewords; computational time; distortion calculations; efficient
	encoding algorithm; inequalities; subvector technique; vector components;
	vector quantization; image coding; vector quantisation;}
}

@ARTICLE{Patane2002,
  author = {Patané,, G. and Russo,,M.},
  title = {Distributed unsupervised learning using the multisoft machine},
  journal = {Information Sciences},
  year = {2002},
  volume = {143},
  pages = {181 - 196},
  number = {1-4},
  abstract = {Unsupervised learning using K-means techniques is successfully employed
	in several application fields. When the training set and the number
	of reference vectors increases, the computational effort can become
	prohibitive for mono-processor computers. This paper illustrates
	the parallelization of two clustering techniques using the MULTISOFT
	machine, a commodity supercomputer, built at the University of Messina.
	The particular management policy of the MULTISOFT machine and the
	implementation techniques have shown very interesting results: the
	speedup increases together with the complexity of the problem to
	be solved.},
  doi = {10.1016/S0020-0255(02)00198-6},
  file = {Patane2002.pdf:SOM\\applications\\Patane2002.pdf:PDF},
  issn = {0020-0255},
  keywords = {Unsupervised learning},
  url = {http://www.sciencedirect.com/science/article/B6V0C-45H97WH-3/2/a9095ca3daf5c3ce32dd7d5ac5849979}
}

@ARTICLE{Peano1890,
  author = {Peano,,G.},
  title = {Sur une courbe, qui remplit toute une aire plane},
  journal = {Mathematische Annalen},
  year = {1890},
  volume = {36},
  pages = {157-160},
  number = {1},
  doi = {10.1007/BF01199438},
  file = {paper:References/SOM/som-related/peano1890.pdf:PDF},
  owner = {castudil},
  timestamp = {2009.07.28},
  url = {http://www.springerlink.com/content/w232301n53960133}
}

@ARTICLE{Peura1998,
  author = {Peura, Markus},
  title = {The Self-Organizing Map of Trees},
  journal = {Neural Processing Letters},
  year = {1998},
  volume = {8},
  pages = {155--162},
  number = {2},
  month = oct,
  abstract = {In the standard version of the Self-Organizing Map, each neuron is
	associated with a vector. An extension using trees instead of vectors
	is presented. Compared to vectors, trees provide remarkably more
	degrees of freedom. The essential points of self-organization, the
	distance function and the learning rule, are adapted to trees by
	means of graph matching. In order to avoid exhaustive searching in
	tree matching an efficient heuristic is introduced. The results of
	the experiments are promising: the proposed methods apply elegantly
	in the process of self-organization.},
  file = {paper:C\:\\Users\\castudil\\Documents\\Research\\Cesar\\References\\to-read\\Peura1998.pdf:PDF},
  owner = {READ},
  timestamp = {2010.01.18},
  url = {http://dx.doi.org/10.1023/A:1009648713183}
}

@INPROCEEDINGS{Provost1998,
  author = {Provost, Foster J. and Fawcett, Tom and Kohavi, Ron},
  title = {The Case against Accuracy Estimation for Comparing Induction Algorithms},
  booktitle = {ICML '98: Proceedings of the Fifteenth International Conference on
	Machine Learning},
  year = {1998},
  pages = {445--453},
  address = {San Francisco, CA, USA},
  publisher = {Morgan Kaufmann Publishers Inc.},
  abstract = {We analyze critically the use of classification accuracy to compare
	classifiers on natural data sets, providing a thorough investigation
	using ROC analysis, standard machine learning algorithms, and standard
	benchmark data sets. The results raise serious concerns about the
	use of accuracy for comparing classifiers and drawinto question the
	conclusions that can be drawn from such studies. In the course of
	the presentation, we describe and demonstrate what we believe to
	be the proper use of ROC analysis for comparative studies in machine
	learning research. We argue that this methodology is preferable both
	for making practical choices and for drawing scientific conclusions.},
  file = {Provost1998.pdf:Statistics/Provost1998.pdf:PDF},
  isbn = {1-55860-556-8}
}

@BOOK{Quinlan1993,
  title = {C4.5: Programs for machine learning},
  publisher = {Morgan Kaufmann},
  year = {1993},
  author = {Quinlan,, J. R.},
  address = {San Francisco},
  owner = {castudil},
  timestamp = {2010.08.26}
}

@INPROCEEDINGS{Rahmel1996,
  author = {Rahmel,, J.},
  title = {{S}plit{N}et: learning of tree structured {K}ohonen chains},
  booktitle = {Neural Networks, 1996., IEEE International Conference on},
  year = {1996},
  volume = {2},
  pages = {1221 -1226 vol.2},
  month = jun,
  abstract = {This work introduces a tree structured neural network model for topology
	preserving vector quantization with one-dimensional Kohonen chains.
	The leaves of the tree are the chains, each of which quantizes a
	subspace of the input space. Topological defects can effectively
	be detected and splitting of the chain at that location results in
	a growing of the tree structure and increase of topology preservation.
	Additionally, the chains are able to grow and shrink in order to
	approximate user defined criteria. Advantages over existing dynamic
	network models are the flexible tree structure, the total lack of
	global parameters or calculations as well as the simulation and retrieval
	speed due to the network structure. Different levels of generalization
	and prototypicality are naturally observed},
  doi = {10.1109/ICNN.1996.549072},
  file = {Rahmel1996.pdf:fast-BMU/Rahmel1996.pdf:PDF},
  keywords = {SplitNet;flexible tree structure;generalization;one-dimensional Kohonen
	chains;prototypicality;topology preserving vector quantization;tree
	structured Kohonen chains;tree structured neural network model;user
	defined criteria;generalisation (artificial intelligence);learning
	(artificial intelligence);self-organising feature maps;vector quantisation;}
}

@INPROCEEDINGS{Rahmel1997,
  author = {Rahmel,, J. and Blum,, C. and Hahn,, P.},
  title = {On the role of hierarchy for neural network interpretation},
  booktitle = {IJCAI'97: Proceedings of the Fifteenth international joint conference
	on Artifical intelligence},
  year = {1997},
  pages = {1072--1077},
  address = {San Francisco, CA, USA},
  publisher = {Morgan Kaufmann Publishers Inc.},
  file = {Rahmel1997.pdf:fast-BMU/Rahmel1997.pdf:PDF},
  isbn = {1-555860-480-4},
  location = {Nagoya, Japan}
}

@BOOK{RamonyCajal1911,
  title = {Histologie du Syst\'{e}ms Nerveux de l'homme et des vert\'{e}br\'{e}s},
  publisher = {Maloine},
  year = {1911},
  author = {Ram\'{o}n y Cajal,, S.},
  address = {Paros},
  owner = {castudil},
  timestamp = {2009.12.08}
}

@ARTICLE{Ramasubramanian1992,
  author = {Ramasubramanian, V. and Paliwal, K.K.},
  title = {Fast K-dimensional tree algorithms for nearest neighbor search with
	application to vector quantization encoding},
  journal = {Signal Processing, IEEE Transactions on},
  year = {1992},
  volume = {40},
  pages = {518 -531},
  number = {3},
  month = mar,
  abstract = {Fast search algorithms are proposed and studied for vector quantization
	encoding using the K-dimensional (K-d) tree structure. Here, the
	emphasis is on the optimal design of the K -d tree for efficient
	nearest neighbor search in multidimensional space under a bucket-Voronoi
	intersection search framework. Efficient optimization criteria and
	procedures are proposed for designing the K-d tree, for the case
	when the test data distribution is available (as in vector quantization
	application in the form of training data) as well as for the case
	when the test data distribution is not available and only the Voronoi
	intersection information is to be used. The criteria and bucket-Voronoi
	intersection search procedure are studied in the context of vector
	quantization encoding of speech waveform. They are empirically observed
	to achieve constant search complexity for O(log N) tree depths and
	are found to be more efficient in reducing the search complexity.
	A geometric interpretation is given for the maximum product criterion,
	explaining reasons for its inefficiency with respect to the optimization
	criteria},
  doi = {10.1109/78.120795},
  file = {Ramasubramanian1992.pdf:fast-BMU/Ramasubramanian1992.pdf:PDF},
  issn = {1053-587X},
  keywords = {Voronoi intersection information;bucket-Voronoi intersection search
	procedure;fast k-dimensional tree algorithms;geometric interpretation;maximum
	product criterion;multidimensional space;nearest neighbor search;optimization;search
	complexity;training data;vector quantization encoding;data compression;encoding;optimisation;search
	problems;trees (mathematics);}
}

@ARTICLE{Rauber2002,
  author = {Rauber,,A. and Merkl,,D. and Dittenbach,,M.},
  title = {The {G}rowing {H}ierarchical {S}elf-{O}rganizing {M}ap: exploratory
	analysis of high-dimensional data},
  journal = {IEEE Transactions on Neural Networks},
  year = {2002},
  volume = {13},
  pages = { 1331-1341},
  number = {6},
  abstract = {The self-organizing map (SOM) is a very popular unsupervised neural-network
	model for the analysis of high-dimensional input data as in data
	mining applications. However, at least two limitations have to be
	noted, which are related to the static architecture of this model
	as well as to the limited capabilities for the representation of
	hierarchical relations of the data. With our novel growing hierarchical
	SOM (GHSOM) we address both limitations. The GHSOM is an artificial
	neural-network model with hierarchical architecture composed of independent
	growing SOMs. The motivation was to provide a model that adapts its
	architecture during its unsupervised training process according to
	the particular requirements of the input data. Furthermore, by providing
	a global orientation of the independently growing maps in the individual
	layers of the hierarchy, navigation across branches is facilitated.
	The benefits of this novel neural network are a problem-dependent
	architecture and the intuitive representation of hierarchical relations
	in the data. This is especially appealing in explorative data mining
	applications, allowing the inherent structure of the data to unfold
	in a highly intuitive fashion.},
  doi = {10.1109/TNN.2002.804221},
  file = {paper:C\:\\Users\\castudil\\Documents\\Research\\Cesar\\References\\SOM\\SOM-variants\\Rauber2002.pdf:PDF},
  issn = {1045-9227},
  keywords = {data analysis, data mining, neural net architecture, pattern recognition,
	self-organising feature maps, unsupervised learningGHSOM, data mining,
	growing hierarchical self-organizing map, hierarchical architecture,
	hierarchical relations, high-dimensional data analysis, pattern recognition,
	unsupervised learning, unsupervised neural-network}
}

@ARTICLE{Reed2003,
  author = {Reed, B.},
  title = {The height of a random binary search tree},
  journal = {J. ACM},
  year = {2003},
  volume = {50},
  pages = {306--332},
  month = {May},
  acmid = {765571},
  address = {New York, NY, USA},
  doi = {10.1145/765568.765571},
  file = {Reed2003.pdf:RHST/Reed2003.pdf:PDF},
  issn = {0004-5411},
  issue = {3},
  keywords = {Binary search tree, asymptotics, height, probabilistic analysis, random
	tree, second moment method},
  numpages = {27},
  publisher = {ACM}
}

@ARTICLE{Ritter1986,
  author = {Ritter,, H. and Schulten,, K.},
  title = {On the stationary state of Kohonen’s selforganizing sensory mapping},
  journal = {Biological Cybernetics},
  year = {1986},
  volume = {54},
  pages = {99–106},
  file = {paper:References/SOM/som-related/Ritter1986.pdf:PDF},
  owner = {castudil},
  timestamp = {2009.10.29}
}

@BOOK{Rojas1996,
  title = {Neural networks: a systematic introduction},
  publisher = {Springer-Verlag New York, Inc.},
  year = {1996},
  author = {Rojas,,R.},
  address = {New York, NY, USA},
  isbn = {3-540-60505-3}
}

@BOOK{Rosenblatt1962,
  title = {Principles of Neurodynamics},
  publisher = {Spartan},
  year = {1962},
  author = {Rosenblatt,, Frank},
  address = {New York},
  owner = {castudil},
  timestamp = {2009.12.06}
}

@ARTICLE{Rosenblatt1958,
  author = {Rosenblatt,, Frank},
  title = {The Perceptron: A Probabilistic Model for Information Storage and
	Organization in the Brain},
  journal = {Psychological Review},
  year = {1958},
  volume = {65},
  pages = {386-408},
  number = {6},
  file = {paper:References\\ANN\\Rosenblatt1958.PDF:PDF},
  owner = {castudil},
  timestamp = {2009.12.06}
}

@ARTICLE{Roverso2000,
  author = {Roverso,,D.},
  title = {Soft computing tools for transient classification},
  journal = {Information Sciences},
  year = {2000},
  volume = {127},
  pages = {137 - 156},
  number = {3-4},
  note = {Intelligent Manufacturing and Fault Diagnosis. (II). Soft computing
	approaches to fault diagnosis},
  abstract = {Any action taken on a process, for example in response to an abnormal
	situation or in reaction to unsafe conditions, relies on the ability
	to identify the state of operation or the events that are occurring.
	Although there might be hundreds or even thousands of measurements
	in a process, there are generally few events occurring. The data
	from these measurements must then be mapped into appropriate descriptions
	of the occurring event(s), which in most cases is a difficult task.
	A systematic study was carried out with the aim of comparing alternative
	neural network designs and models for performing this mapping task.
	Four main approaches have been investigated. Radial basis function
	(RBF) neural networks and cascade-RBF neural networks combined with
	fuzzy clustering, self-organizing map neural networks, and recurrent
	neural networks. The main evaluation criteria adopted were identification
	accuracy, reliability (i.e., correct recognition of an unknown event
	as such), robustness (to noise and to changing initial conditions),
	and real-time performance. Additionally, in this paper we describe
	how ensembles of recurrent neural networks can overcome some of the
	limitations encountered in these early prototypes, and give an example
	involving the identification of anomalous events in a PWR 900 MW
	nuclear power plant.},
  doi = {10.1016/S0020-0255(00)00035-9},
  file = {Roverso2000.pdf:SOM\\applications\\Roverso2000.pdf:PDF},
  issn = {0020-0255},
  keywords = {Event identification},
  url = {http://www.sciencedirect.com/science/article/B6V0C-40WDSXC-5/2/031b6519bb5d1fbfa3f0fe41bf5a0938}
}

@ARTICLE{Russell2008,
  author = {Russell,, B. and Torralba,, A. and Murphy,, K. and Freeman,, W.},
  title = {LabelMe: A Database and Web-Based Tool for Image Annotation},
  journal = {International Journal of Computer Vision},
  year = {2008},
  volume = {77},
  pages = {157--173},
  number = {1},
  abstract = {We seek to build a large collection of images with ground truth labels
	to be used for object detection and recognition research.
	
	 Such data is useful for supervised learning and quantitative evaluation.
	To achieve this, we developed a web-based tool that
	
	 allows easy image annotation and instant sharing of such annotations.
	Using this annotation tool, we have collected a large
	
	 dataset that spans many object categories, often containing multiple
	instances over a wide variety of images. We quantify
	
	 the contents of the dataset and compare against existing state of
	the art datasets used for object recognition and detection.
	
	 Also, we show how to extend the dataset to automatically enhance
	object labels with WordNet, discover object parts, recover
	
	 a depth ordering of objects in a scene, and increase the number of
	labels using minimal user supervision and images from the
	
	 web.},
  doi = {10.1007/s11263-007-0090-8}
}

@ARTICLE{Samet1984,
  author = {Samet,, H.},
  title = {The Quadtree and Related Hierarchical Data Structures},
  journal = {ACM Comput. Surv.},
  year = {1984},
  volume = {16},
  pages = {187--260},
  month = {June},
  acmid = {356930},
  address = {New York, NY, USA},
  doi = {10.1145/356924.356930},
  file = {Samet1984.pdf:fast-BMU/Samet1984.pdf:PDF},
  issn = {0360-0300},
  issue = {2},
  numpages = {74},
  publisher = {ACM}
}

@ARTICLE{Samsonova2006,
  author = {Samsonova,,E. V. and Kok,,J. N. and {IJ}zerman,,A. P.},
  title = {TreeSOM: Cluster analysis in the self-organizing map},
  journal = {Neural Networks},
  year = {2006},
  volume = {19},
  pages = {935 - 949},
  number = {6-7},
  note = {Advances in Self Organising Maps - WSOM'05},
  abstract = {Clustering problems arise in various domains of science and engineering.
	A large number of methods have been developed to date. The Kohonen
	self-organizing map (SOM) is a popular tool that maps a high-dimensional
	space onto a small number of dimensions by placing similar elements
	close together, forming clusters. Cluster analysis is often left
	to the user. In this paper we present the method TreeSOM and a set
	of tools to perform unsupervised SOM cluster analysis, determine
	cluster confidence and visualize the result as a tree facilitating
	comparison with existing hierarchical classifiers. We also introduce
	a distance measure for cluster trees that allows one to select a
	SOM with the most confident clusters.},
  doi = {DOI: 10.1016/j.neunet.2006.05.003},
  file = {paper:SOM\\SOM-variants\\Samsonova2006.pdf:PDF},
  issn = {0893-6080},
  keywords = {Self-organizing map},
  url = {http://www.sciencedirect.com/science/article/B6T08-4K66F09-1/2/dc3d3b4fde4ba9dee0ffedc42a8fd75b}
}

@ARTICLE{Sanger1991,
  author = {Sanger,, T.D.},
  title = {A tree-structured adaptive network for function approximation in
	high-dimensional spaces},
  journal = {Neural Networks, IEEE Transactions on},
  year = {1991},
  volume = {2},
  pages = {285 -293},
  number = {2},
  month = {mar},
  abstract = {Nonlinear function approximation is often solved by finding a set
	of coefficients for a finite number of fixed nonlinear basis functions.
	However, if the input data are drawn from a high-dimensional space,
	the number of required basis functions grows exponentially with dimension,
	leading many to suggest the use of adaptive nonlinear basis functions
	whose parameters can be determined by iterative methods. The author
	proposes a technique based on the idea that for most of the data,
	only a few dimensions of the input may be necessary to compute the
	desired output function. Additional input dimensions are incorporated
	only where needed. The learning procedure grows a tree whose structure
	depends upon the input data and the function to be approximated.
	This technique has a fast learning algorithm with no local minima
	once the network shape is fixed, and it can be used to reduce the
	number of required measurements in situations where there is a cost
	associated with sensing. Three examples are given: controlling the
	dynamics of a simulated planar two-joint robot arm, predicting the
	dynamics of the chaotic Mackey-Glass equation, and predicting pixel
	values in real images from pixel values above and to the left},
  doi = {10.1109/72.80339},
  file = {Sanger1991.pdf:SOM/SOM-variants/Sanger1991.pdf:PDF},
  issn = {1045-9227},
  keywords = {chaotic Mackey-Glass equation;function approximation;high dimensional
	spaces;image processing;learning systems;neural nets;pixel values;robot
	control;tree-structured adaptive network;adaptive systems;function
	approximation;learning systems;neural nets;trees (mathematics);}
}

@ARTICLE{Seiffert1997,
  author = {Seiffer,,U. and Michaelis,,B.},
  title = {Estimating motion parameters with three-dimensional self-organizing
	maps},
  journal = {Information Sciences},
  year = {1997},
  volume = {101},
  pages = {187 - 201},
  number = {3-4},
  note = {Advanced Neuro-Fuzzy Techniques and Their Applications},
  abstract = {The importance of analyzing moving scenes within the wide area of
	digital image processing is increasingly high. Although a simple
	detection of object velocity by neural networks has been considered
	in previously published papers, an implementation of artificial neural
	networks using a priori information for motion analysis is still
	quite rare. This paper shows the benefits from artificial neural
	networks, and from using a priori information about the contents
	of the history in the image sequence to improve the accuracy and
	speed of estimating motion parameters in the cases of distorted or
	overlapped objects. In the first place, it introduces three-dimensional
	Self-Organizing Maps (SOM) with two-dimensional input layers.},
  doi = {10.1016/S0020-0255(97)00009-1},
  file = {Seiffert1997.pdf:SOM\\applications\\Seiffert1997.pdf:PDF},
  issn = {0020-0255},
  url = {http://www.sciencedirect.com/science/article/B6V0C-3SP2D7M-C/2/120804ac85a6548a6ff861ac96adb5fb}
}

@ARTICLE{Shanmukh1999,
  author = {Shanmukh,, K. and Ganesh Murthy,,C.N.S. and Venkatesh,,Y.V.},
  title = {Applications of self-organization networks spatially isomorphic to
	patterns},
  journal = {Information Sciences},
  year = {1999},
  volume = {114},
  pages = {23--39},
  number = {1-4},
  abstract = {A new technique based on self-organization is proposed for classifying
	patterns (which include characters, and two- and three-dimensional
	objects). A neuronal network, created to be a physical replica of
	each exemplar, is mapped onto the given test pattern by self-organization,
	during which the network undergoes deformation in an attempt to match
	the given test pattern. The extent of deformation is inversely proportional
	to the correctness of the match: smaller the deformation, better
	is the match. A deformation measure is proposed, leading to the classification
	of the test pattern. Also presented are some algorithmic improvements
	(including the choice of other deformation measures) to speed up
	computation. Examples illustrate the versatility of the technique.},
  doi = {10.1016/S0020-0255(98)10067-1},
  file = {Shanmukh1999.pdf:SOM\\applications\\Shanmukh1999.pdf:PDF},
  issn = {0020-0255},
  keywords = {Deformation of patterns},
  url = {http://www.sciencedirect.com/science/article/B6V0C-3W3FPDN-2/2/2833030189a10cbe4ea40b7e69a82281}
}

@ARTICLE{Sherk1995,
  author = {Sherk,,M.},
  title = {Self-Adjusting k-ary Search Trees},
  journal = {Journal of Algorithms},
  year = {1995},
  volume = {19},
  pages = {25 - 44},
  number = {1},
  doi = {DOI: 10.1006/jagm.1995.1026},
  issn = {0196-6774},
  keywords = {m-ary trees},
  url = {http://www.sciencedirect.com/science/article/B6WH3-45PTHKN-R/2/3acb5f8aa6035c13b860592a5c169f62}
}

@ARTICLE{Singh2000,
  author = {Singh,,R. and Cherkassky,,V. and Papanikolopoulos,,N.},
  title = {Self-{O}rganizing {M}aps for the skeletonization of sparse shapes},
  journal = {Neural Networks, IEEE Transactions on},
  year = {2000},
  volume = {11},
  pages = {241-248},
  number = {1},
  month = {Jan},
  abstract = {This paper presents a method for computing the skeleton of planar
	shapes and objects which exhibit sparseness (lack of connectivity),
	within their image regions. Such sparseness in images may occur due
	to poor lighting conditions, incorrect thresholding or image sub-sampling.
	Furthermore, in document image analysis, sparse shapes are characteristic
	of texts faded due to aging and/or poor ink quality. Given the pixel
	distribution for a shape, the proposed method involves an iterative
	evolution of a piecewise-linear approximation of the shape skeleton
	by using a minimum spanning tree-based self-organizing map (SOM).
	By constraining the SOM to lie on the edges of the Delaunay triangulation
	of the shape distribution, the adjacency relationships between regions
	in the shape are detected and used in the evolution of the skeleton.
	The SOM, on convergence, gives the final skeletal shape. The skeletonization
	is invariant to Euclidean transformations. The potential of the method
	is demonstrated on a variety of sparse shapes from different application
	domains},
  doi = {10.1109/72.822527},
  issn = {1045-9227},
  keywords = {approximation theory, image thinning, iterative methods, mesh generation,
	self-organising feature maps, trees (mathematics)Delaunay triangulation,
	Euclidean transformation, image degradation, iterative evolution,
	piecewise-linear approximation, principal curves, self-organizing
	maps, shape skeleton, skeletonization, spanning tree, sparse shapes}
}

@ARTICLE{Sleator1985,
  author = {Sleator,, D. D. and Tarjan,, R. E.},
  title = {Self-adjusting binary search trees},
  journal = {J. ACM},
  year = {1985},
  volume = {32},
  pages = {652--686},
  number = {3},
  address = {New York, NY, USA},
  doi = {http://doi.acm.org/10.1145/3828.3835},
  issn = {0004-5411},
  publisher = {ACM}
}

@ARTICLE{Sproull1991,
  author = {Sproull,,R. F.},
  title = {Refinements to Nearest-Neighbor Searching in k-Dimensional Trees},
  journal = {Algorithmica},
  year = {1991},
  volume = {6},
  pages = {579-589},
  number = {4},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  file = {Sproull1991.pdf:fast-BMU/Sproull1991.pdf:PDF}
}

@ARTICLE{Tai1996,
  author = {Tai,, S.C. and Lai,, C.C. and Lin,, Y.C.},
  title = {Two fast nearest neighbor searching algorithms for image vector quantization},
  journal = {Communications, IEEE Transactions on},
  year = {1996},
  volume = {44},
  pages = {1623--1628},
  number = {12},
  month = dec,
  abstract = {In this paper, two efficient codebook searching algorithms for vector
	quantization (VQ) are presented. The first fast search algorithm
	utilizes the compactness property of signal energy on transform domain
	and the geometrical relations between the input vector and every
	codevector to eliminate those codevectors that have no chance to
	be the closest codeword of the input vector. It achieves a full search
	equivalent performance. As compared with other fast methods of the
	same kind, this algorithm requires the fewest multiplications and
	the least total times of distortion measurements. Then, a suboptimal
	searching method, which sacrifices the reconstructed signal quality
	to speed up the search of nearest neighbor, is presented. This algorithm
	performs the search process on predefined small subcodebooks instead
	of the whole codebook for the closest codevector. Experimental results
	show that this method not only needs less CPU time to encode an image
	but also encounters less loss of reconstructed signal quality than
	tree-structured VQ does},
  doi = {10.1109/26.545888},
  file = {Tai1996.pdf:fast-BMU/Tai1996.pdf:PDF},
  issn = {0090-6778},
  keywords = {VQ;codebook searching algorithms;codevector;compactness property;fast
	nearest neighbor searching algorithms;full search equivalent performance;geometrical
	relations;image coding;input vector;reconstructed signal quality;signal
	energy;subcodebooks;suboptimal searching method;transform domain;vector
	quantization;image coding;search problems;vector quantisation;}
}

@ARTICLE{Ultsch2002,
  author = {Ultsch,, A. and Röske,,F.},
  title = {Self-organizing feature maps predicting sea levels},
  journal = {Information Sciences},
  year = {2002},
  volume = {144},
  pages = {91 - 125},
  number = {1-4},
  doi = {10.1016/S0020-0255(02)00203-7},
  file = {Ultsch2002.pdf:SOM\\applications\\Ultsch2002.pdf:PDF},
  issn = {0020-0255},
  url = {http://www.sciencedirect.com/science/article/B6V0C-45NY5F8-5/2/712dd7c896826b8879c09df393f97d2e}
}

@ARTICLE{Vandeginste1990,
  author = {Vandeginste, B.},
  title = {{PARVUS}: {A}n extendable package of programs for data exploration,
	classification and correlation, {M}. {F}orina, {R}. {L}eardi, {C}.
	{A}rmanino and {S}. {L}anteri, {E}lsevier, {A}msterdam, 1988, {P}rice:
	{US} \$645 {ISBN} 0-444-43012-1},
  journal = {Journal of Chemometrics},
  year = {1990},
  volume = {4},
  pages = {191--193},
  number = {2},
  doi = {10.1002/cem.1180040210}
}

@BOOK{Venables2002,
  title = {Modern Applied Statistics with S},
  publisher = {Springer},
  year = {2002},
  author = {Venables, W.N., Ripley, B.D.},
  series = {Statistics and Computing},
  edition = {4th},
  owner = {castudillo},
  timestamp = {2012.01.05}
}

@INPROCEEDINGS{Verleysen2005,
  author = {Verleysen,, Michel and Fran\c{c}ois,, Damien},
  title = {The Curse of Dimensionality in Data Mining and Time Series Prediction},
  booktitle = {8th International Workshop on Artificial Neural Networks, IWANN 2005},
  year = {2005},
  pages = {758--770},
  abstract = {Modern data analysis tools have to work on high-dimensional data,
	whose components are not independently distributed. High-dimensional
	spaces show surprising, counter-intuitive geometrical properties
	that have a large influence on the performances of data analysis
	tools. Among these properties, the concentration of the norm phenomenon
	results in the fact that Euclidean norms and Gaussian kernels, both
	commonly used in models, become inappropriate in high-dimensional
	spaces. This papers presents alternative distance measures and kernels,
	together with geometrical methods to decrease the dimension of the
	space. The methodology is applied to a typical time series prediction
	example.},
  file = {paper:References/SOM/som-related/Verleysen2005.pdf:PDF},
  url = {http://dx.doi.org/10.1007/11494669_93}
}

@ARTICLE{Vidaurre2007,
  author = {Vidaurre, D. and Muruzabal, J.},
  title = {A Quick Assessment of Topology Preservation for SOM Structures},
  journal = {Neural Networks, IEEE Transactions on},
  year = {2007},
  volume = {18},
  pages = {1524 -1528},
  number = {5},
  month = {sep.},
  abstract = {Several topology preservation measures and monitoring schemes have
	been proposed to help ascertain the correct organization of the self-organizing
	map (SOM) structure. Here, we consider a novel idea that performs
	faster than previous alternatives while showing interesting behavior
	in practice. Our proposal aims to facilitate inexpensive, online
	monitoring of topographic map formation algorithms.},
  doi = {10.1109/TNN.2007.895820},
  file = {Vidaurre2007.pdf:Topology/Vidaurre2007.pdf:PDF},
  issn = {1045-9227},
  keywords = {SOM structures;monitoring schemes;online monitoring;self-organizing
	map structure;topographic map formation algorithms;topology preservation;self-organising
	feature maps;topology;Algorithms;Computer Simulation;Models, Statistical;Neural
	Networks (Computer);Pattern Recognition, Automated;}
}

@ARTICLE{Villmann1997,
  author = {Villmann,,T. and Der,,R. and Herrmann,,M. and Martinetz,,T. M.},
  title = {Topology preservation in self-organizing feature maps: exact definition
	and measurement},
  journal = {IEEE Transactions on Neural Networks},
  year = {1997},
  volume = {8},
  pages = {256-266},
  number = {2},
  month = {Mar},
  abstract = {The neighborhood preservation of self-organizing feature maps like
	the Kohonen map is an important property which is exploited in many
	applications. However, if a dimensional conflict arises this property
	is lost. Various qualitative and quantitative approaches are known
	for measuring the degree of topology preservation. They are based
	on using the locations of the synaptic weight vectors. These approaches,
	however, may fail in case of nonlinear data manifolds. To overcome
	this problem, in this paper we present an approach which uses what
	we call the induced receptive fields for determining the degree of
	topology preservation. We first introduce a precise definition of
	topology preservation and then propose a tool for measuring it, the
	topographic function. The topographic function vanishes if and only
	if the map is topology preserving. We demonstrate the power of this
	tool for various examples of data manifolds},
  doi = {10.1109/72.557663},
  file = {paper:References\\SOM\\Topology\\Villmann1997.PDF:PDF},
  issn = {1045-9227},
  keywords = {self-organising feature maps, topologyKohonen map, dimensional conflict,
	induced receptive fields, neighborhood preservation, nonlinear data
	manifolds, qualitative approaches, quantitative approaches, self-organizing
	feature maps, synaptic weight vectors, topographic function, topology
	preservation}
}

@ARTICLE{Voegtlin2002,
  author = {Voegtlin,,T.},
  title = {Recursive self-organizing maps},
  journal = {Neural Networks},
  year = {2002},
  volume = {15},
  abstract = {A temporal extension of the Self-Organizing Map (SOM) is presented.
	The learning algorithm is based on self-reference, and is called
	Recursive SOM. This network learns local representations of the temporal
	context associated with a time series, and extends classical properties
	of SOM to time.},
  owner = {castudil},
  timestamp = {2008.04.24}
}

@INBOOK{Walker1972,
  chapter = {A top down algorithm for constructing nearly-optimal lexicographic
	trees},
  title = {Graph Theory and Computing},
  publisher = {New York: Academic Press},
  year = {1972},
  author = {Walker,, W. A. and Gotlieb,, C. C.},
  owner = {castudil},
  timestamp = {2009.02.17}
}

@ARTICLE{Wang1998,
  author = {Wang,,H. and Dopazo,,J. and De La Fraga,,L. G. and Zhu,,Y. and Carazo,,J.
	M.},
  title = {Self-organizing tree-growing network for the classification of protein
	sequences},
  journal = {Protein Science},
  year = {1998},
  volume = {7},
  pages = {2613-2622},
  number = {12},
  abstract = {The self-organizing tree algorithm (SOTA) was recently introduced
	to construct phylogenetic trees from biological sequences, based
	on the principles of Kohonen's self-organizing maps and on Fritzke's
	growing cell structures. SOTA is designed in such a way that the
	generation of new nodes can be stopped when the sequences assigned
	to a node are already above a certain similarity threshold. In this
	way a phylogenetic tree resolved at a high taxonomic level can be
	obtained. This capability is especially useful to classify sets of
	diversified sequences. SOTA was originally designed to analyze pre-aligned
	sequences. It is now adapted to be able to analyze patterns associated
	to the frequency of residues along a sequence, such as protein dipeptide
	composition and other n-gram compositions. In this work we show that
	the algorithm applied to these data is able to not only successfully
	construct phylogenetic trees of protein families, such as cytochrome
	c, triosephophate isomerase, and hemoglobin alpha chains, but also
	classify very diversified sequence data sets, such as a mixture of
	interleukins and their receptors.},
  doi = {10.1002/pro.5560071215},
  file = {Wang1998.pdf:SOM\\SOM-variants\\Wang1998.pdf:PDF},
  owner = {castudil},
  timestamp = {2010.01.27}
}

@ARTICLE{Ward1963,
  author = {Ward, Joe H., Jr.},
  title = {Hierarchical Grouping to Optimize an Objective Function},
  journal = {Journal of the American Statistical Association},
  year = {1963},
  volume = {58},
  pages = {236--244},
  number = {301},
  abstract = {A procedure for forming hierarchical groups of mutually exclusive
	subsets, each of which has members that are maximally similar with
	respect to specified characteristics, is suggested for use in large-scale
	(<latex>$n > 100$</latex>) studies when a precise optimal solution
	for a specified number of groups is not practical. Given n sets,
	this procedure permits their reduction to n - 1 mutually exclusive
	sets by considering the union of all possible n(n - 1)/2 pairs and
	selecting a union having a maximal value for the functional relation,
	or objective function, that reflects the criterion chosen by the
	investigator. By repeating this process until only one group remains,
	the complete hierarchical structure and a quantitative estimate of
	the loss associated with each stage in the grouping can be obtained.
	A general flowchart helpful in computer programming and a numerical
	example are included.},
  copyright = {Copyright © 1963 American Statistical Association},
  file = {paper:References/SOM/som-related/Ward1963.pdf:PDF},
  issn = {01621459},
  publisher = {American Statistical Association},
  url = {http://www.jstor.org/stable/2282967}
}

@ARTICLE{Wilcoxon1945,
  author = {Wilcoxon, Frank},
  title = {Individual Comparisons by Ranking Methods},
  journal = {Biometrics Bulletin},
  year = {1945},
  volume = {1},
  pages = {80--83},
  number = {6},
  copyright = {Copyright © 1945 International Biometric Society},
  file = {Wilcoxon1945.pdf:Statistics/Wilcoxon1945.pdf:PDF},
  issn = {00994987},
  publisher = {International Biometric Society},
  url = {http://www.jstor.org/stable/3001968}
}

@ARTICLE{Xu2005,
  author = {Xu,,R. and Wunsch, D., II},
  title = {Survey of clustering algorithms},
  journal = {Neural Networks, IEEE Transactions on},
  year = {2005},
  volume = {16},
  pages = {645--678},
  number = {3},
  month = {may },
  abstract = {Data analysis plays an indispensable role for understanding various
	phenomena. Cluster analysis, primitive exploration with little or
	no prior knowledge, consists of research developed across a wide
	variety of communities. The diversity, on one hand, equips us with
	many tools. On the other hand, the profusion of options causes confusion.
	We survey clustering algorithms for data sets appearing in statistics,
	computer science, and machine learning, and illustrate their applications
	in some benchmark data sets, the traveling salesman problem, and
	bioinformatics, a new field attracting intensive efforts. Several
	tightly related topics, proximity measure, and cluster validation,
	are also discussed.},
  doi = {10.1109/TNN.2005.845141},
  file = {Xu2005.pdf:Pattern Recognition\\Xu2005.pdf:PDF},
  issn = {1045-9227},
  keywords = {benchmark data sets;bioinformatics;cluster analysis;data analysis;traveling
	salesman problem;data analysis;pattern classification;pattern clustering;Algorithms;Computer
	Simulation;Models, Statistical;Neural Networks (Computer);Numerical
	Analysis, Computer-Assisted;Pattern Recognition, Automated;Signal
	Processing, Computer-Assisted;Stochastic Processes;}
}

@INPROCEEDINGS{Yao1985,
  author = {Yao, A C and Yao, F F},
  title = {A general approach to d-dimensional geometric queries},
  booktitle = {Proceedings of the seventeenth annual ACM symposium on Theory of
	computing},
  year = {1985},
  series = {STOC '85},
  pages = {163--168},
  address = {New York, NY, USA},
  publisher = {ACM},
  acmid = {22163},
  doi = {http://doi.acm.org/10.1145/22145.22163},
  file = {Yao1985.pdf:fast-BMU/Yao1985.pdf:PDF},
  isbn = {0-89791-151-2},
  location = {Providence, Rhode Island, United States},
  numpages = {6},
  url = {http://doi.acm.org/10.1145/22145.22163}
}

@ARTICLE{Yeloglu2007,
  author = {Yeloglu,,O. and Zincir-Heywood,,A.N. and Heywood,,M.I.},
  title = {Growing recurrent self organizing map},
  journal = {Systems, Man and Cybernetics, 2007. ISIC. IEEE International Conference
	on},
  year = {2007},
  pages = {290-295},
  month = {October},
  abstract = {The growing recurrent self-organizing map (GRSOM) is embedded into
	a standard self-organizing map (SOM) hierarchy. To do so, the KDD
	benchmark dataset from the International Knowledge Discovery and
	Data Mining Tools Competition is employed. This dataset consists
	of 500,000 training patterns and 41 features for each pattern. Unlike
	most of the previous methods, only 6 of the basic features are employed.
	The resulting model has a capability of detection (false positive)
	rate of 89.6% (5.66%), where this is as good as the data-mining approaches
	that uses all 41 features and twice as faster than a similar hierarchical
	SOM architecture.},
  doi = {10.1109/ICSMC.2007.4414001},
  keywords = {data mining, self-organising feature mapsData Mining Tools Competition,
	International Knowledge Discovery, growing recurrent self-organizing
	map, hierarchical SOM architecture, standard self-organizing map,
	training patterns}
}

@TECHREPORT{Zhu2005,
  author = {Zhu,,X.},
  title = {Semi-Supervised Learning Literature Survey},
  institution = {Computer Sciences, University of Wisconsin-Madison},
  year = {2005},
  number = {1530},
  file = {Paper:References/SOM/som-related/Zhu2005.pdf:PDF}
}

@BOOK{Zhu2009,
  title = {Introduction to Semi-Supervised Learning},
  publisher = {Morgan \& Claypool Publishers},
  year = {2009},
  author = {Zhu,, X. and Goldberg,, A. B.},
  pages = {1--130},
  month = jan,
  abstract = {Semi-supervised learning is a learning paradigm concerned with the
	study of how computers and natural systems such as humans learn in
	the presence of both labeled and unlabeled data. Traditionally, learning
	has been studied either in the unsupervised paradigm (e.g., clustering,
	outlier detection) where all the data are unlabeled, or in the supervised
	paradigm (e.g., classification, regression) where all the data are
	labeled. The goal of semi-supervised learning is to understand how
	combining labeled and unlabeled data may change the learning behavior,
	and design algorithms that take advantage of such a combination.
	Semi-supervised learning is of great interest in machine learning
	and data mining because it can use readily available unlabeled data
	to improve supervised learning tasks when the labeled data are scarce
	or expensive. Semi-supervised learning also shows potential as a
	quantitative tool to understand human category learning, where most
	of the input is self-evidently unlabeled. In this introductory book,
	we present some popular semi-supervised learning models, including
	self-training, mixture models, co-training and multiview learning,
	graph-based methods, and semi-supervised support vector machines.
	For each model, we discuss its basic mathematical formulation. The
	success of semi-supervised learning depends critically on some underlying
	assumptions. We emphasize the assumptions made by each model and
	give counterexamples when appropriate to demonstrate the limitations
	of the different models. In addition, we discuss semi-supervised
	learning for cognitive psychology. Finally, we give a computational
	learning theoretic perspective on semi-supervised learning, and we
	conclude the book with a brief discussion of open questions in the
	field.
	
	
	Table of Contents: Introduction to Statistical Machine Learning /
	Overview of Semi-Supervised Learning / Mixture Models and EM / Co-Training
	/ Graph-Based Semi-Supervised Learning / Semi-Supervised Support
	Vector Machines / Human Semi-Supervised Learning / Theory and Outlook},
  doi = {10.2200/S00196ED1V01Y200906AIM006},
  issn = {1939-4608},
  journal = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
  owner = {castudillo},
  review = {A short, self-contained introductory book to semi-supervised learning.
	For advanced undergraduates, entry-level graduate students and researchers
	in Computer Science, Electrical Engineering, Statistics, Psychology,
	etc. You may already have access to the book through your institution},
  timestamp = {2011.09.01}
}

@PROCEEDINGS{DBLP:conf/iciar/2006-1,
  title = {Image Analysis and Recognition, Third International Conference, ICIAR
	2006, P{\'o}voa de Varzim, Portugal, September 18-20, 2006, Proceedings,
	Part I},
  year = {2006},
  editor = {Aur{\'e}lio C. Campilho and Mohamed S. Kamel},
  volume = {4141},
  series = {Lecture Notes in Computer Science},
  publisher = {Springer},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  booktitle = {ICIAR (1)},
  isbn = {3-540-44891-8}
}

@PROCEEDINGS{DBLP:conf/iciar/2004-2,
  title = {Image Analysis and Recognition: International Conference, ICIAR 2004,
	Porto, Portugal, September 29-October 1, 2004, Proceedings, Part
	II},
  year = {2004},
  editor = {Aur{\'e}lio C. Campilho and Mohamed S. Kamel},
  volume = {3212},
  series = {Lecture Notes in Computer Science},
  publisher = {Springer},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  booktitle = {ICIAR (2)},
  isbn = {3-540-23240-0}
}

@PROCEEDINGS{DBLP:conf/icann/2001,
  title = {Artificial Neural Networks - ICANN 2001, International Conference
	Vienna, Austria, August 21-25, 2001 Proceedings},
  year = {2001},
  editor = {Georg Dorffner and Horst Bischof and Kurt Hornik},
  volume = {2130},
  series = {Lecture Notes in Computer Science},
  publisher = {Springer},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  booktitle = {ICANN},
  isbn = {3-540-42486-5},
  owner = {castudil},
  timestamp = {2009.08.27}
}

@PROCEEDINGS{DBLP:conf/pci/2001,
  title = {Advances in Informatics, 8th Panhellenic Conference on Informatics,
	PCI 2001. Nicosia, Cyprus, November 8-10, 2001, Revised Selected
	Papers},
  year = {2003},
  editor = {Yannis Manolopoulos and Skevos Evripidou and Antonis C. Kakas},
  volume = {2563},
  series = {Lecture Notes in Computer Science},
  publisher = {Springer},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  booktitle = {Panhellenic Conference on Informatics},
  isbn = {3-540-07544-5}
}

@PROCEEDINGS{DBLP:conf/ausai/2008,
  title = {AI 2008: Advances in Artificial Intelligence, 21st Australasian Joint
	Conference on Artificial Intelligence, Auckland, New Zealand, December
	1-5, 2008. Proceedings},
  year = {2008},
  editor = {Wayne Wobcke and Mengjie Zhang},
  volume = {5360},
  series = {Lecture Notes in Computer Science},
  publisher = {Springer},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  booktitle = {Australasian Conference on Artificial Intelligence},
  isbn = {978-3-540-89377-6}
}

